<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Francisco Requena</title>
    <link>https://franciscorequena.com/post/</link>
      <atom:link href="https://franciscorequena.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 11 Jun 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://franciscorequena.com/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://franciscorequena.com/post/</link>
    </image>
    
    <item>
      <title>Human genetics as a tool for drug discovery</title>
      <link>https://franciscorequena.com/post/2022-07-11-human-genetics-as-a-tool-for-drug-discovery/</link>
      <pubDate>Sat, 11 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/2022-07-11-human-genetics-as-a-tool-for-drug-discovery/</guid>
      <description>&lt;p&gt;For children with a rare disease, an accurate diagnosis is crucial to provide advice, possible therapies and assess the potential risk for family members in future generations. Public initiatives such as the International Rare Diseases Research Consortium (IRDiRC) set the goal for 2017-2027 to &amp;ldquo;enable all people living with a rare disease to receive an accurate diagnosis, care, and available therapy soon after seeking medical care&amp;rdquo; (1).&lt;/p&gt;
&lt;p&gt;Despite significant efforts in the field of drug development, the reality is &lt;strong&gt;discouraging&lt;/strong&gt;. Drug candidates that move from phase 1 clinical trials to approval and launch remain at around 10%. This rate is even lower in some specific fields, such as oncology (3.4%).&lt;/p&gt;
&lt;p&gt;At the same time, large-scale biomedical databases, such as gnomAD and the UK Biobank, offer the possibility to assess the effect of variants in humans and their association with multiple conditions. In addition, the recruitment of population-based cohorts considered disease-agnostic increases the number of hypotheses that can be tested. In particular, the high scientific value of the UK Biobank is worth mentioning, as it not only releases genomic data but also rich phenotype characterization alongside other data sources (273).&lt;/p&gt;
&lt;p&gt;Both mutations and drugs share the same mechanism: they disrupt the normal functioning of the human body. How they do so may differ, but it seems plausible to hypothesize that, in some cases, the phenotypic consequences of a loss-of-function SNVs or deletions are similar to the pharmacological effect of an inhibitor drug.&lt;/p&gt;
&lt;p&gt;There is no doubt that this &lt;strong&gt;view&lt;/strong&gt; poses some problems. As mentioned above, the heterogeneity and particularities in genetics are enormous even for monogenic diseases. Elements such as the genotype of the variant or discrepancies between the predicted effect and the actual consequences can make the use of this approach challenging. For instance, it is already known that LoF mutations may not decrease protein or even mRNA levels.&lt;/p&gt;
&lt;p&gt;In spite of these drawbacks, &lt;strong&gt;human genetics&lt;/strong&gt; seems to be a great tool for the identification of drugs with therapeutic effects.&lt;/p&gt;
&lt;p&gt;There is evidence to support this assumption. For instance, gastrointestinal adverse events observed in clinical trials of DGAT1 inhibitors could have been predicted based on the causal relationship between rare and highly penetrant variants of DGAT1 and congenital diarrheal disorder (3).&lt;/p&gt;
&lt;p&gt;Another well-known example is the association of heterozygous gain-of-function mutations in the PCSK9 gene and familial hypercholesterolemia, which leads to heart attacks or strokes relatively early in life. Strikingly, LoF variants in the PCSK9 gene have been causally associated with low levels of low-density lipoprotein cholesterol (4).&lt;/p&gt;
&lt;p&gt;This &lt;strong&gt;human genetic evidence&lt;/strong&gt; contributed to the technical and regulatory success of PCSK9 inhibitors, leading to the launch of evolocumab (Amgen) and alirocumab (Regeneron), and has also shown value in patient stratification in clinical trials (5).&lt;/p&gt;
&lt;p&gt;Other drug candidates with strong genetic evidence between disease phenotypes and functional genetic variants have been identified, such as HSD17B13 for chronic liver disease (6), TYK2 for multiple autoimmune disorders (7), NRXN1 for neuropsychiatric disease (8), ASGR1 for cardiovascular disease (9),&lt;/p&gt;
&lt;p&gt;These are not isolated examples, but a general trend. Nelson et al. found that pairing genetic target indication with genetic evidence almost doubles the success rate in clinical development. These analyses were re-evaluated three years later by other researchers with data after the original publication and controlling for potential confounding factors and corroborated the same claims (10).&lt;/p&gt;
&lt;p&gt;Harnessing human genomic data can improve the drug discovery process, from target selection to reducing failures due to lack of efficacy or adverse effects.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;.Sanders AD, Falconer E, Hills M, Spierings DCJ, Lansdorp PM. Single-cell template strand sequencing by Strand-seq enables the characterization of individual homologs. Nat Protoc. 2017;12: 1151&amp;ndash;1176.&lt;/li&gt;
&lt;li&gt;Szustakowski JD, Balasubramanian S, Kvikstad E, Khalid S, Bronson PG, Sasson A, et al. Advancing human genetics research and drug discovery through exome sequencing of the UK Biobank. Nat Genet. 2021;53. &lt;a href=&#34;doi:10.1038/s41588-021-00885-0&#34;&gt;doi:10.1038/s41588-021-00885-0&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Haas JT, Winter HS, Lim E, Kirby A, Blumenstiel B, DeFelice M, et al. DGAT1 mutation is linked to a congenital diarrheal disorder. J Clin Invest. 2012;122: 4680&amp;ndash;4684.&lt;/li&gt;
&lt;li&gt;Cohen JC, Boerwinkle E, Mosley TH Jr, Hobbs HH. Sequence variations in PCSK9, low LDL, and protection against coronary heart disease. N Engl J Med. 2006;354: 1264&amp;ndash;1272.&lt;/li&gt;
&lt;li&gt;Sabatine MS, Giugliano RP, Keech AC, Honarpour N, Wiviott SD, Murphy SA, et al. Evolocumab and Clinical Outcomes in Patients with Cardiovascular Disease. N Engl J Med. 2017;376: 1713&amp;ndash;1722.&lt;/li&gt;
&lt;li&gt;Abul-Husn NS, Cheng X, Li AH, Xin Y, Schurmann C, Stevis P, et al. A Protein-Truncating HSD17B13 Variant and Protection from Chronic Liver Disease. N Engl J Med. 2018;378: 1096&amp;ndash;1106.&lt;/li&gt;
&lt;li&gt;Dendrou CA, Cortes A, Shipman L, Evans HG, Attfield KE, Jostins L, et al. Resolving TYK2 locus genotype-to-phenotype differences in autoimmunity. Sci Transl Med. 2016;8: 363ra149.&lt;/li&gt;
&lt;li&gt;Noh HJ, Tang R, Flannick J, O&amp;rsquo;Dushlaine C, Swofford R, Howrigan D, et al. Integrating evolutionary and regulatory information with a multispecies approach implicates genes and pathways in obsessive-compulsive disorder. Nat Commun. 2017;8: 774.&lt;/li&gt;
&lt;li&gt;Nioi P, Sigurdsson A, Thorleifsson G, Helgason H, Agustsdottir AB, Norddahl GL, et al. Variant ASGR1 Associated with a Reduced Risk of Coronary Artery Disease. N Engl J Med. 2016;374: 2131&amp;ndash;2141.&lt;/li&gt;
&lt;li&gt;King EA, Davis JW, Degner JF. Are drug targets with genetic support twice as likely to be approved? Revised estimates of the impact of genetic support for drug mechanisms on the probability of drug approval. PLoS Genet. 2019;15: e1008489.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>How many genes have been associated with cancer in PubMed?</title>
      <link>https://franciscorequena.com/post/genes-associated-cancer_pubmed/</link>
      <pubDate>Sun, 16 May 2021 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/genes-associated-cancer_pubmed/</guid>
      <description>
&lt;script src=&#34;https://franciscorequena.com/post/genes-associated-cancer_pubmed/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In the biomedical literature, it is common to find sentences like:&lt;/p&gt;
&lt;p&gt;“Besides, the gene [gene symbol] has been associated with [type of cancer(s)] [References]”&lt;/p&gt;
&lt;p&gt;The structure of these sentences can change from article to article, but the underlying idea and goal are the same. I will try to summarise it in the following sentence:&lt;/p&gt;
&lt;p&gt;“Hello reader/editor/reviewer, I was studying [any field], and I found this gene. I think it is a relevant/remarkable finding because it has been associated with cancer [references]. Therefore, it supports my hypothesis about the biological relevance of the gene in my field. Please, publish it.”&lt;/p&gt;
&lt;p&gt;This approach is valid and logical as long as the association gene &amp;lt;-&amp;gt; cancer has been well-described and validated by different experiments and research teams. Unfortunately, some of these associations will be just spurious and no well-supported.&lt;/p&gt;
&lt;p&gt;To explore this problem, we will count the number of articles in PubMed associating cancer with each one of the 19,205 protein-coding genes in the human genome.&lt;/p&gt;
&lt;p&gt;To do so, we will write a simple code in R that will make a query for each gene to PubMed using the fantastic rentrez package.&lt;/p&gt;
&lt;p&gt;The script has two simple steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Download the official list of protein-coding gene symbols from HUGO.&lt;/li&gt;
&lt;li&gt;For each gene, make the following query “gene_symbol[Title/Abstract] AND cancer[Title/Abstract]” in PubMed (1-2).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can find the code below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(scales)
library(rentrez)

gene_symbols &amp;lt;- read_tsv(&amp;#39;http://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/tsv/locus_types/gene_with_protein_product.txt&amp;#39;) %&amp;gt;% 
  pull(symbol)

# Careful: it takes long to make all the queries
query_pubmed &amp;lt;- function(input_gene) {
  
  print(input_gene)
  Sys.sleep(0.3)
  query_tmp &amp;lt;- entrez_search(db =&amp;quot;pubmed&amp;quot;, 
                             term = paste(paste0(input_gene, &amp;#39;[Title/Abstract]&amp;#39;),&amp;#39; AND &amp;#39;, &amp;#39;cancer[Title/Abstract]&amp;#39;), 
                             retmax = 600)
  
  tibble(&amp;#39;gene&amp;#39; = input_gene, &amp;#39;n_hits&amp;#39; = length(query_tmp[[&amp;#39;ids&amp;#39;]]))
}

result_genes &amp;lt;- gene_symbols %&amp;gt;% map_dfr(~ query_pubmed(.x))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result_genes %&amp;gt;%
  ggplot(aes(n_hits)) +
  geom_histogram(binwidth = 5) +
  theme_minimal() +
  labs(x = &amp;#39;Nº articles&amp;#39;, y = &amp;#39;Nº genes&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/genes-associated-cancer_pubmed/index.en_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;result_genes %&amp;gt;%
  mutate(category = case_when(
    n_hits == 0 ~ &amp;#39;0 articles&amp;#39;,
    n_hits &amp;gt;= 1 &amp;amp; n_hits &amp;lt;= 5 ~ &amp;#39;1-5 articles&amp;#39;,
    TRUE ~ &amp;#39;&amp;gt;5 articles&amp;#39;
  )) %&amp;gt;% 
  count(category) %&amp;gt;%
  mutate(perc = n / sum(n)) %&amp;gt;%
  ggplot(aes(reorder(category,perc), perc)) +
    geom_col(aes(fill = category), color = &amp;#39;black&amp;#39;) +
  scale_y_continuous(label = percent, limits = c(0, 1)) +
  geom_label(aes(label = paste0(round(perc, 2)*100, &amp;#39;%&amp;#39;))) +
  labs(fill = &amp;#39;Category&amp;#39;, x = &amp;#39;Category&amp;#39;, y = &amp;#39;Percentage&amp;#39;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/genes-associated-cancer_pubmed/index.en_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As you can see in the plot, 41% of the genes have been associated with cancer in more than five articles, 36% in 1-5 articles, and only 23% of the genes with no publications.&lt;/p&gt;
&lt;p&gt;If I choose a random protein-coding from the human genome and do a query in PubMed, it is more likely (77%) to find at least one article than none.&lt;/p&gt;
&lt;p&gt;This data reflects how easy it is to find articles associating cancer with most of the genes. Therefore, when a reader finds this kind of argument [my gene is important -&amp;gt; gene + cancer + references] should take it with a grain of salt.&lt;/p&gt;
&lt;p&gt;An interesting point is the reasons behind these numbers. From a biological perspective, it is difficult to assume the relevance in cancer of most of the human genome even though cancer is a group including many different kinds of diseases with their subgroups.&lt;/p&gt;
&lt;p&gt;In the following points, I describe some of the reasons that might explain these numbers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In most cases, a tumor is produced by the disruption of multiple genes simultaneously. Structural variants or chromosomal aberrations can map a considerable portion of the genome and disrupt many genes. This scenario makes it difficult to identify the driver mutations from those mutations (passengers) with no relevance and, therefore, finding the causal gene(s).
Researchers have an important incentive: publish. Therefore, it makes sense that some researchers “orientate” their studies and results to the cancer field because it is a way to give more “weight” to their research even though, in some cases, the evidence for it is scarce.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The overproduction of scientific studies in specific areas of knowledge has already been described. For instance, in a &lt;a href=&#34;https://www.nature.com/articles/d41586-021-00314-6&#34;&gt;comment&lt;/a&gt; published in 2021, the authors find that only 22% of gene-related publications were related to 1% of genes. Also, they find “new yearly publications focusing on a given gene is linearly proportional to the size of previous literature on it”.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is reasonable to think that a similar scenario happens with many research published trying to link their analysis with any aspect of cancer though the evidence is limit.&lt;/p&gt;
&lt;p&gt;To clarify, this is by no means a way to discredit researchers with work related to cancer. It is a way to make people aware of the problematic aspect of finding articles in PubMed describing the gene A associated with cancer and using them as evidence without further analysis.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Some ideas for a future version&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;There have been multiple naming conventions to identify genes. A paper describing a gene with a synonymous symbol instead of the official gene will not be reported in our script. Luckily, HUGO reports the list of synonymous symbols along with the official one. Therefore, it would be easy to adapt our script with new queries and merge the number of hits.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Some gene symbols can be confounded with composited names. For instance, the query “A1BG[Title/Abstract] AND cancer[Title/Abstract]” retrieved 22 hits, and one of them was describing the A1BG-AS1 lncRNA.
An interesting idea would be to reanalyze the data but change cancer by each of the cancer types.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We can run the queries in parallel, adding the tag “future_” to the function “map_dfr” thanks to the package furrr. Note the API has a limit of simultaneous queries per IP.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;I set a limit of 600 articles retrieved to avoid the exceeded limit error of the API.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Extracting gene panels from the Genomics England Panelapp</title>
      <link>https://franciscorequena.com/post/2021-03-20-extracting-gene-panels-from-the-genomics-england-panelapp/</link>
      <pubDate>Sat, 20 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/2021-03-20-extracting-gene-panels-from-the-genomics-england-panelapp/</guid>
      <description>
&lt;script src=&#34;https://franciscorequena.com/post/2021-03-20-extracting-gene-panels-from-the-genomics-england-panelapp/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The &lt;a href=&#34;https://panelapp.genomicsengland.co.uk/&#34;&gt;Genomics England PanelApp&lt;/a&gt; provides panels of genes related to human disorders manually curated by healthcare experts. From a clinical and research perspective, this is a remarkable resource. At the time of writing this post, over 320 panels have been published.&lt;/p&gt;
&lt;p&gt;Unfortunately, you can only download the panels manually one at a time or through an API that retrieves the information as a JSON file.&lt;/p&gt;
&lt;p&gt;Alternatively, below you can find a script in R to extract all the panels from the website and merge them into a single dataset. Please note the following points before using the script:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I only consider genes labeled as “Expert Review Green” defined as “gene-disease association with a high level of evidence” and exclude STRs and CNVs entities. More information on the criteria used can be found on the main page (heading: Understanding gene classifications in a version 1+ gene panel.&lt;/li&gt;
&lt;li&gt;The script selects only a subset of columns from the total available.
Be careful, the script will download more than 320 files automatically (on my laptop, the execution process is ~7 min).&lt;/li&gt;
&lt;li&gt;The script is ready to run on the Linux system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As the script is based on the current website structure, any changes could break the code. Please let me know if this happens. I will try to code an updated version of the code.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(rvest)
library(purrr)
library(tidyverse)
website &amp;lt;- &amp;quot;https://panelapp.genomicsengland.co.uk/panels/&amp;quot;
page &amp;lt;- read_html(website)

c_ref &amp;lt;- page %&amp;gt;%
  html_nodes(&amp;quot;a&amp;quot;) %&amp;gt;% # find all links
  html_attr(&amp;quot;href&amp;quot;)

df_ref &amp;lt;- tibble(ref = c_ref, id = NA) %&amp;gt;%
  filter(str_detect(ref, &amp;#39;download&amp;#39;)) %&amp;gt;%
  mutate(ref = str_remove(ref, &amp;#39;/panels/&amp;#39;)) %&amp;gt;%
  mutate(id = ref) %&amp;gt;%
  mutate(id = str_remove(id, &amp;#39;/download/01234/&amp;#39;))

# Linux command - if you are using Windows, please make sure that you create a new folder with the name &amp;#39;gene_panel&amp;#39;
# and remove the &amp;quot;system(&amp;#39;mkdir gene_panel&amp;#39;)&amp;quot; line
system(&amp;#39;mkdir gene_panel&amp;#39;)
setwd(&amp;#39;gene_panel&amp;#39;)

walk2(df_ref$ref, df_ref$id, function(a, b)
  download.file(url = paste0(website, a), destfile = paste0(&amp;#39;gene_panel_&amp;#39;, b))
)

files_panel &amp;lt;- list.files()
panel_total &amp;lt;- files_panel %&amp;gt;% map_dfr(~ read_tsv(.x) %&amp;gt;% 
                                         select(`Entity Name`, `Entity type`, `Gene Symbol`, `Sources(; separated)`, 
                                                Level4, Phenotypes) %&amp;gt;% 
                                         mutate(source = .x) )


# Filtering out genes with a evidence level (red - amber)
panel_total &amp;lt;- panel_total %&amp;gt;%
  rename(entity_name = `Entity Name`,
         entity_type = `Entity type`,
         gene = `Gene Symbol`,
         sources = `Sources(; separated)`) %&amp;gt;%
  filter(entity_type == &amp;#39;gene&amp;#39;) %&amp;gt;%  # optional - we can include regions in our analysis
  filter(str_detect(sources, &amp;#39;Expert Review Green&amp;#39;)) %&amp;gt;%
  select(gene, Level4, -sources, source, Phenotypes)

setwd(&amp;#39;..&amp;#39;)

write_tsv(panel_total, &amp;#39;panel_genes.tsv&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to ROC curves with animated examples</title>
      <link>https://franciscorequena.com/post/roc-curves-an-animated-example/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/roc-curves-an-animated-example/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Receiver operating characteristic (ROC) curves is one of the concepts I have struggled most. As a personal view, I do not find it intuitive or clear at first glance. Possibly, because we are used to interpreting information as single values, such as mean, median, accuracy…ROC curves are different because it represents a group of values conforming a curve. Besides, it is the most popular way to represent a model performance for a &lt;em&gt;particular dataset&lt;/em&gt; where the task is a binary classification.&lt;/p&gt;
&lt;p&gt;Before explaining where the ROC curves come from, let’s focus on what is the outcome of most of the classification models. To illustrate this point, let’s train a few logistic regression models with a toy dataset and use the package &lt;code&gt;parsnip&lt;/code&gt; which provides a common interface to train models from many other packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;For this post, we are going to use a dataset that includes 310 patients and six explanatory variables related to biomechanical features of the vertebral column. Besides, it contains a response variable &lt;code&gt;abnormality&lt;/code&gt; that defines if the patient has been diagnosed with a medical condition in the vertebral column (&lt;code&gt;yes&lt;/code&gt; and &lt;code&gt;no&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(gganimate) # animated plots
library(magick) # combine two gif
library(yardstick) # roc_curve helper
library(parsnip) # train logistic regression models


# Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00212/
verterbral &amp;lt;- read.table(&amp;#39;data/column_2C.dat&amp;#39;, header = FALSE, sep = &amp;#39; &amp;#39;)
colnames(verterbral) &amp;lt;- c(&amp;#39;pelvic_incidence&amp;#39;, 
             &amp;#39;pelvic_tilt&amp;#39;, 
             &amp;#39;lumbar_lordosis_angle&amp;#39;, 
             &amp;#39;sacral_slope&amp;#39;, &amp;#39;pelvic_radius&amp;#39;, 
             &amp;#39;degree_spondylolisthesis&amp;#39;, &amp;#39;abnormality&amp;#39;) 

verterbral &amp;lt;- verterbral %&amp;gt;%
 select(abnormality, everything()) %&amp;gt;%
 mutate(id = row_number()) %&amp;gt;%
 mutate(abnormality = factor(if_else(abnormality == &amp;#39;AB&amp;#39;, &amp;#39;yes&amp;#39;, &amp;#39;no&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data&lt;/h2&gt;
&lt;p&gt;…and split it in training (70%) and test set (30%).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(992)

training_ids &amp;lt;- verterbral %&amp;gt;% sample_frac(0.7) %&amp;gt;% pull(id)

vert_training &amp;lt;- verterbral %&amp;gt;% 
 filter(id %in% training_ids)

vert_test &amp;lt;- verterbral %&amp;gt;% 
 filter(!id %in% training_ids)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;train-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Train models&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_model_one &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence, data = vert_training)


logistic_model_two &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt, data = vert_training)

logistic_model_three &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope, data = vert_training)

logistic_model_four &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius, data = vert_training)

logistic_model_five &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius + lumbar_lordosis_angle, data = vert_training)

logistic_model_all &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ ., data = vert_training[,-ncol(vert_training)])

check_pred &amp;lt;- vert_test %&amp;gt;%
 select(id) %&amp;gt;%
 mutate( pred_logistic_one = predict(logistic_model_one, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_two = predict(logistic_model_two, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_three = predict(logistic_model_three, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_four = predict(logistic_model_four, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_five = predict(logistic_model_five, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_all = predict(logistic_model_all, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes
     ) %&amp;gt;%
 left_join(verterbral %&amp;gt;% select(id, abnormality), by = &amp;#39;id&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-raw-outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot raw outcome&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;check_pred %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 93
## Variables: 8
## $ id                  &amp;lt;int&amp;gt; 1, 3, 6, 9, 10, 11, 12, 13, 20, 22, 27, 43, 44,...
## $ pred_logistic_one   &amp;lt;dbl&amp;gt; 0.7499979, 0.8028267, 0.4747471, 0.5213208, 0.4...
## $ pred_logistic_two   &amp;lt;dbl&amp;gt; 0.7940143, 0.8233949, 0.5251698, 0.5516503, 0.3...
## $ pred_logistic_three &amp;lt;dbl&amp;gt; 0.7939271, 0.8233653, 0.5240107, 0.5505372, 0.3...
## $ pred_logistic_four  &amp;lt;dbl&amp;gt; 0.9301525, 0.9029361, 0.4239115, 0.5079236, 0.8...
## $ pred_logistic_five  &amp;lt;dbl&amp;gt; 0.9035941, 0.8837432, 0.3236233, 0.5683134, 0.9...
## $ pred_logistic_all   &amp;lt;dbl&amp;gt; 0.7531384, 0.2486521, 0.4043606, 0.8549908, 0.9...
## $ abnormality         &amp;lt;fct&amp;gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, ye...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each observation of the test set, the models retrieve a probability. This value represents how likely that observation belongs to the label &lt;code&gt;abnormality == yes&lt;/code&gt;[1].&lt;/p&gt;
&lt;p&gt;Probability is not a particular output format of logistic regressions models [2], but a standard way of many models. For instance, models based on tree decisions, such as gradient boosting [3] or random forest, retrieve probabilities as output.&lt;/p&gt;
&lt;p&gt;To make it simple, for now, we will use only the predicted values (&lt;code&gt;pred_logistic_all&lt;/code&gt;) from the trained model that used all the explanatory variables.&lt;/p&gt;
&lt;p&gt;Since we have all the probabilities values retrieved by the model in the variable &lt;code&gt;pred_logistic_all&lt;/code&gt;, we can explore the distribution of the model’s outcome. To do this, there are two common ways: boxplot and density plots. For the scope of this post, we are going to use the latter. Besides, since our observations are defined by two label options (survival == ‘yes’, survival = ‘no’), we are going to plot two different distributions, one for each label:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;check_pred %&amp;gt;% 
 ggplot(aes(pred_logistic_all)) +
  geom_density(aes(fill = abnormality), alpha = 0.4) +
  theme_bw() +
  scale_fill_viridis_d()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can extract some ideas from the above plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since this value represents the probability of an observation to belong to &lt;code&gt;abnormality = &#39;yes&#39;&lt;/code&gt;, it makes sense to find observations whose real label is ‘yes’ with high probability. On the other way around, we expect to find observations whose real label is &lt;code&gt;abnormality = &#39;no&#39;&lt;/code&gt; with low probability. Though this is what we expect, this is not always the case, since we find also observations whose probability of belonging to &lt;code&gt;abnormality = &#39;yes&#39;&lt;/code&gt; is quite low, even though, its real label is &lt;code&gt;yes&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is a twilight zone, where we have observations from both labels levels that have “inaccurate” probabilities.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can somehow see how well a model performed based on the overlapping of these two distributions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A perfect model would retrieve both distributions with no overlapping.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since these models do not retrieve directly the label of the response variable. A threshold to discretize a continuous probability is required to transform the probability into a label. This is a difficult part, because no matter where you define the threshold, we face a trade-off between the percentage of False Positives (FP) and False Negatives (FN). Besides, there is not a clear rule for it, and the results can be pretty arbitrary.&lt;/p&gt;
&lt;p&gt;Another problem arises: if the selection of the threshold is arbitrary, how do we &lt;em&gt;compare different models&lt;/em&gt;? Here it is where the ROC curves come out!&lt;/p&gt;
&lt;p&gt;ROC curves try to overcome this issue, taking into account all the possible scenarios given multiple thresholds. This allows us to estimate the performance of our model independently of the threshold you take.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-create-a-roc-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to create a ROC curve?&lt;/h2&gt;
&lt;p&gt;To create a ROC curve, the starting point is precisely the same information we used to display the density plot: a column with predicted probabilities and another with the real labels. Each row is an observation of the test set.&lt;/p&gt;
&lt;p&gt;Once we have this information, we define as many thresholds [4] as observations found in the test set (plus Inf and -Inf). These values are defined by the probability of each observation.&lt;/p&gt;
&lt;p&gt;Furthermore, for each threshold value, all the probabilities above it will be identified as &lt;code&gt;abnormality = yes&lt;/code&gt; and we count the number of True Positive (TP), True Negative (TN), but also, those observations predicted as abnormality = yes but actually are &lt;code&gt;no&lt;/code&gt; (False Positive (FP)) and those predicted as &lt;code&gt;no&lt;/code&gt; but actually are &lt;code&gt;yes&lt;/code&gt; (False Negative (FN)).&lt;/p&gt;
&lt;p&gt;Finally, we need this information to calculate the values that will make up the ROC curve axis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt; (also known as True positive rate). This metric reflects the number of positives in the test dataset that are correctly identified.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Specificity&lt;/strong&gt; (also known as True negative rate). This metric measures the number of negatives in the test dataset that are correctly identified.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases, a result of 1 is considered perfect.&lt;/p&gt;
&lt;p&gt;To facilitate this, there are multiple packages in R to calculate the ROC curve. For this case, I am going to use the function &lt;code&gt;roc_curve&lt;/code&gt; from the package &lt;code&gt;yardstick&lt;/code&gt; which I recommend.&lt;/p&gt;
&lt;p&gt;Check the output of the function &lt;code&gt;roc_curve&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we just need to specify the column with the labels (abnormality) and the predicted probabilities (pred_logistic_all)
roc_logistic &amp;lt;- check_pred %&amp;gt;% roc_curve(abnormality, pred_logistic_all)
roc_logistic %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   .threshold specificity sensitivity
##        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1  -Inf                1      0     
## 2     0.0179           1      0.0294
## 3     0.0292           1      0.0588
## 4     0.0369           1      0.0882
## 5     0.0372           1      0.118 
## 6     0.0493           1      0.147&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, for the visualization, we only need to modify the specificity variable as &lt;code&gt;1 - specificity&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roc_logistic %&amp;gt;%
 ggplot(aes(x = (1 - specificity), y = sensitivity)) +
 geom_line() +
 geom_abline(linetype = 3) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animated-roc-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Animated ROC curve&lt;/h2&gt;
&lt;p&gt;To build some intuition, we can see how to build the ROC curve while we define thresholds values in the density plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- check_pred %&amp;gt;% 
 ggplot() +
  geom_density(aes(x = pred_logistic_all, fill = abnormality), alpha = 0.5) +
  geom_vline(data = roc_logistic %&amp;gt;% filter( .threshold != Inf) %&amp;gt;% filter(.threshold != -Inf), aes(xintercept = .threshold, group = .threshold)) +
  transition_reveal(.threshold) +
  theme_bw()


b &amp;lt;- roc_logistic %&amp;gt;%
 ggplot(aes(x = (1 - specificity), y = sensitivity)) +
 geom_line() +
  geom_point(colour = &amp;#39;red&amp;#39;, size = 3) +
  transition_reveal(sensitivity) +
 geom_abline(linetype = 3) +
 theme_bw()

# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition
a_gif &amp;lt;- animate(a, width = 440, height = 440)
b_gif &amp;lt;- animate(b, width = 440, height = 440)

a_mgif &amp;lt;- image_read(a_gif)
b_mgif &amp;lt;- image_read(b_gif)

new_gif &amp;lt;- image_append(c(a_mgif[1], b_mgif[1]))
for(i in 2:100){
 combined &amp;lt;- image_append(c(a_mgif[i], b_mgif[i]))
 new_gif &amp;lt;- c(new_gif, combined)
}

new_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-8-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-six-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing six models&lt;/h2&gt;
&lt;p&gt;At the beginning of this post, we trained five models, each one with a different number of explanatory variables: one, two, three, four, five, and six.&lt;/p&gt;
&lt;p&gt;We can easily display their probability distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comparison_six &amp;lt;- check_pred %&amp;gt;%
 pivot_longer(starts_with(&amp;#39;pred&amp;#39;), names_to = &amp;#39;model&amp;#39;, values_to = &amp;#39;prob&amp;#39;) %&amp;gt;%
 mutate(model = fct_inorder(as.factor(model)))


comparison_six %&amp;gt;%
 ggplot(aes(prob)) +
  geom_density(aes(fill = abnormality), alpha = 0.4) +
  theme_bw() +
  scale_fill_viridis_d() +
 facet_wrap(~ model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the example above, we observe that the overlapping between both distributions decreases as we increase the number of explanatory variables. In other words, since we increase the amount of useful information to discriminate between the two labels (&lt;em&gt;yes&lt;/em&gt;, &lt;em&gt;no&lt;/em&gt;), the predictive power of the model improves.&lt;/p&gt;
&lt;p&gt;Besides, we can plot their ROC curves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comparison_six %&amp;gt;%
 group_by(model) %&amp;gt;%
 roc_curve(abnormality, prob) %&amp;gt;%
  ggplot(aes(x = (1 - specificity), y = sensitivity)) +
 geom_line(aes(color = model)) +
 geom_abline(linetype = 3) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animated-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Animated comparison&lt;/h2&gt;
&lt;p&gt;Finally, we can replicate the previous code and compare the six models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roc_comparison &amp;lt;- comparison_six %&amp;gt;%
 group_by(model) %&amp;gt;%
 roc_curve(abnormality, prob) %&amp;gt;% ungroup()

a &amp;lt;- comparison_six %&amp;gt;%
 ggplot(aes(prob)) +
  geom_density(aes(fill = abnormality), alpha = 0.4) +
  geom_vline(data = roc_comparison %&amp;gt;% filter(.threshold != Inf) %&amp;gt;% filter(.threshold != -Inf),
        aes(xintercept = .threshold, group = .threshold)) +
  theme_bw() +
  scale_fill_viridis_d() +
  transition_reveal(.threshold) +
  facet_wrap(~ model)



b &amp;lt;- roc_comparison %&amp;gt;%
 ggplot(aes(x = (1 - specificity), y = sensitivity, group = model)) +
  geom_line(aes(color = model)) +
  geom_point(colour = &amp;#39;red&amp;#39;, size = 3) +
  transition_reveal(sensitivity) +
  geom_abline(linetype = 3) +
  theme_bw()

# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition
a_gif &amp;lt;- animate(a, width = 440, height = 440)
b_gif &amp;lt;- animate(b, width = 440, height = 440)

a_mgif &amp;lt;- image_read(a_gif)
b_mgif &amp;lt;- image_read(b_gif)

new_gif &amp;lt;- image_append(c(a_mgif[1], b_mgif[1]))

for(i in 2:100){
 combined &amp;lt;- image_append(c(a_mgif[i], b_mgif[i]))
 new_gif &amp;lt;- c(new_gif, combined)
}

new_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-11-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[1] The package &lt;code&gt;yardstick&lt;/code&gt; as many other packages use the first level of the response variable factor as the “event”. Therefore, the probability output determines how likely an observation belongs to the &lt;em&gt;first level of the factor&lt;/em&gt; of the response variable. This behavior can be changed in the &lt;code&gt;yardstick&lt;/code&gt; package global options.&lt;/p&gt;
&lt;p&gt;[2] The default output has a logit scale and needs to be transformed first to a probability value. This can be done automatically if we specify in the function &lt;code&gt;predict&lt;/code&gt; the argument &lt;strong&gt;type&lt;/strong&gt; as &lt;em&gt;prob&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] Apart from tree decisions, linear models can be also used in gradient boosting.&lt;/p&gt;
&lt;p&gt;[4] Because of this reason, ROC curves might not be appropriate to evaluate the performance of models on small test sets.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to uncertainty with Bayesian models</title>
      <link>https://franciscorequena.com/post/introduction-uncertainty-bayesian-models/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/introduction-uncertainty-bayesian-models/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;In this post, we will get a first approximation to the “uncertainty” concept. First, we will train two models: logistic regression and its “Bayesian version” and compare their performance. Furthermore, we will explore the advantage of using a Bayesian model when we want to estimate how likely is our prediction. Finally, we will briefly discuss why there are some predicted values more probable than others.&lt;/p&gt;
&lt;div id=&#34;get-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get the data&lt;/h2&gt;
&lt;p&gt;First, we download this data from &lt;a href=&#34;https://www.kaggle.com/gilsousa/habermans-survival-data-set&#34;&gt;Kaggle&lt;/a&gt;. This dataset includes 306 patients from a study of patients that had undergone a surgical operation on breast cancer. The table consists of three explanatory variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age of patient during surgical operation (&lt;code&gt;age&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Year when the operation was made (&lt;code&gt;operation_year&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Number of positive axillary nodes detected (&lt;code&gt;nodes&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, there is a column (&lt;code&gt;survival&lt;/code&gt;) that indicates whether the patient survived at least 5 years after the operation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork) # merge plots
library(ggridges) # ridges plot
library(glue) # paste plot labels
library(yardstick) # helper roc curves and auc
library(rstanarm) # bayesian model
library(bayestestR) # helper for the bayesian model
library(broom) # make tidy

# Source: https://www.kaggle.com/gilsousa/habermans-survival-data-set
haberman &amp;lt;- read_csv(&amp;#39;data/haberman.csv&amp;#39;, col_names = c(&amp;#39;age&amp;#39;, 
                            &amp;#39;operation_year&amp;#39;, 
                            &amp;#39;nodes&amp;#39;, 
                            &amp;#39;survival&amp;#39;))

haberman &amp;lt;- haberman %&amp;gt;%
 mutate(survival = factor(if_else(survival == 1, &amp;#39;Yes&amp;#39;, &amp;#39;No&amp;#39;))) %&amp;gt;%
 mutate(operation_year = factor(operation_year)) %&amp;gt;%
 mutate(id = as.character(row_number())) %&amp;gt;% 
 select(id, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory analysis&lt;/h2&gt;
&lt;p&gt;Since the dataset has 3 explanatory variables, let’s plot the distribution of each one of them with the response variable &lt;em&gt;survival&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- haberman %&amp;gt;%
 ggplot(aes(age)) +
 geom_density(aes(fill = survival), color = &amp;#39;black&amp;#39;, alpha = 0.4) +
 theme_bw()

p2 &amp;lt;- haberman %&amp;gt;%
 ggplot(aes(nodes)) +
 geom_density(aes(fill = survival), color = &amp;#39;black&amp;#39;, alpha = 0.4) +
 theme_bw()

p3 &amp;lt;- haberman %&amp;gt;%
 group_by(operation_year, survival) %&amp;gt;%
 summarise(n = n()) %&amp;gt;%
 mutate(perc = 100*(n / sum(n))) %&amp;gt;%
 ggplot(aes(operation_year, perc)) +
 geom_col(aes(fill = survival), color = &amp;#39;black&amp;#39;) +
 theme_bw() +
 labs(y = &amp;#39;Percentage (%)&amp;#39;)


p1 + p2 + p3 + patchwork::plot_layout(nrow = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Age and number of nodes seem to have a reasonable distribution but surprisingly, patient survival does not increase along the operation year. In theory, the patient survival of most cancer types has increased dramatically over the years. Therefore, it seems reasonable to find a similar pattern in this dataset. The interval of time (1958-1969) seems long enough and happened during a period of major progress in clinical therapies.&lt;/p&gt;
&lt;p&gt;A plausible explanation is an underlying effect of, at least, one remaining variable. Let’s observe the distribution of the variable age of patient over the years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;haberman %&amp;gt;%
 ggplot(aes(age, operation_year)) +
 geom_density_ridges(aes(fill = operation_year), show.legend = FALSE) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;haberman %&amp;gt;%
 ggplot(aes(operation_year, age)) +
 geom_boxplot(aes(fill = operation_year), show.legend = FALSE) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seem to be differences over the years. In this post, further analysis to control for this effect is out of scope, but a more exhaustive analysis of this dataset should be aware of it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classification&lt;/h1&gt;
&lt;p&gt;Once we have explored quickly the dataset, we are going to train a model to try to predict whether the patient survived 5 years after the operation.&lt;/p&gt;
&lt;p&gt;To do so, we are going to test two different approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Logistic regression&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian generalized linear models&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this blog post, we will skip aspects such as cross-validation, feature engineering, precision-recall curve, or unbalanced labels (there is).&lt;/p&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data&lt;/h2&gt;
&lt;p&gt;First, we will split the available dataset &lt;code&gt;haberman&lt;/code&gt; into two sets, a training (70%) and a test (30%).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(991)

training_ids &amp;lt;- haberman %&amp;gt;% sample_frac(0.7) %&amp;gt;% pull(id)

hab_training &amp;lt;- haberman %&amp;gt;% 
 filter(id %in% training_ids) %&amp;gt;% 
 mutate(operation_year = as.integer(operation_year))

hab_test &amp;lt;- haberman %&amp;gt;% 
 filter(!id %in% training_ids) %&amp;gt;% 
 mutate(operation_year = as.integer(operation_year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will train independently both models with the training set and predict the labels of the response variable (“survive”, “no survive”) in the test dataset. These predicted labels will be useful to compare both models in terms of performance and further aspects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training---logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training - logistic regression&lt;/h2&gt;
&lt;p&gt;In R, we just need to use the &lt;code&gt;glm&lt;/code&gt; function and specify the argument &lt;code&gt;family = binomial&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_model &amp;lt;- glm(survival ~ age + nodes + operation_year, family = &amp;#39;binomial&amp;#39;, 
           data = hab_training)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;training---bayesian-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training - Bayesian logistic regression&lt;/h2&gt;
&lt;p&gt;Thanks to the package &lt;code&gt;rstanarm&lt;/code&gt; that provides an elegant interface to &lt;a href=&#34;https://mc-stan.org/&#34;&gt;stan&lt;/a&gt;, we can keep almost the same syntax used before. In this case, we use the function &lt;code&gt;stan_glm&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesian_model &amp;lt;- rstanarm::stan_glm(survival ~ age + nodes + operation_year, 
                   family = &amp;#39;binomial&amp;#39;,
                   data = hab_training,
                   prior = normal())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.137 seconds (Warm-up)
## Chain 1:                0.146 seconds (Sampling)
## Chain 1:                0.283 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.132 seconds (Warm-up)
## Chain 2:                0.284 seconds (Sampling)
## Chain 2:                0.416 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.153 seconds (Warm-up)
## Chain 3:                0.157 seconds (Sampling)
## Chain 3:                0.31 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.144 seconds (Warm-up)
## Chain 4:                0.172 seconds (Sampling)
## Chain 4:                0.316 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance&lt;/h2&gt;
&lt;p&gt;Once we trained both models, we are going to compare their performance with the test set (split at the beginning of the post). To that end, we calculate the ROC curve and the Area Under the Curve (AUC) of each model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_logistic &amp;lt;- predict(logistic_model, hab_test, type = &amp;#39;response&amp;#39;)
pred_bayesian &amp;lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %&amp;gt;% 
 as_tibble() %&amp;gt;%
 map_dbl(~ map_estimate(.x) )

check_pred &amp;lt;- hab_test %&amp;gt;%
 select(id) %&amp;gt;%
 mutate(pred_surv_no_log = pred_logistic,
     pred_surv_no_bay = pred_bayesian) %&amp;gt;%
 left_join(haberman %&amp;gt;% select(id, survival), by = &amp;#39;id&amp;#39;)


roc_logistic &amp;lt;- check_pred %&amp;gt;% roc_curve(survival, pred_surv_no_log) %&amp;gt;% mutate(model = &amp;#39;logistic&amp;#39;)
roc_bayesian &amp;lt;- check_pred %&amp;gt;% roc_curve(survival, pred_surv_no_bay) %&amp;gt;% mutate(model = &amp;#39;bayesian&amp;#39;)

auc_logistic &amp;lt;- check_pred %&amp;gt;% roc_auc(survival, pred_surv_no_log) %&amp;gt;% pull(.estimate) %&amp;gt;% round(3)
auc_bayesian &amp;lt;- check_pred %&amp;gt;% roc_auc(survival, pred_surv_no_bay) %&amp;gt;% pull(.estimate) %&amp;gt;% round(3)

roc_both &amp;lt;- roc_logistic %&amp;gt;% bind_rows(roc_bayesian)


roc_both %&amp;gt;%
 ggplot(aes((1-specificity), sensitivity)) +
  geom_line(aes(color = model), size = 1) +
  theme_bw() +
 geom_abline(linetype = 3) +
  labs(title = &amp;#39;Comparison performance logistic and Bayesian model&amp;#39;,
     subtitle = glue(&amp;#39;AUC (logistic) = {auc_logistic} - AUC (Bayesian) = {auc_bayesian}&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both models demonstrate similar performance. If we would have to decide, at this step of the analysis, one of them (logistic or Bayesian), there would not be a reason to choose one or the other. Probably, the logistic one, since it may sounds more familiar. But this might change when the &lt;strong&gt;uncertainty idea&lt;/strong&gt; comes up!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uncertainty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uncertainty&lt;/h2&gt;
&lt;p&gt;First, we are going to explore the outcomes of the test set provided by the &lt;em&gt;logistic model&lt;/em&gt;. These values represent the probability [2] of each instance of being labeled as “No survive” five years after the operation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_logistic &amp;lt;- predict(logistic_model, hab_test, type = &amp;#39;response&amp;#39;)

p1 &amp;lt;- pred_logistic %&amp;gt;%
 enframe() %&amp;gt;%
 ggplot(aes(value)) +
 geom_density(fill = &amp;#39;steelblue&amp;#39;, alpha = 0.5) +
 theme_bw() +
 labs(x = &amp;#39;Probability&amp;#39;, y = &amp;#39;Density&amp;#39;)


p2 &amp;lt;- pred_logistic %&amp;gt;%
 enframe() %&amp;gt;%
 ggplot(aes(value)) +
 geom_histogram(fill = &amp;#39;yellow&amp;#39;, alpha = 0.5, color = &amp;#39;black&amp;#39;, binwidth = 0.05) +
 theme_bw() +
 labs(x = &amp;#39;Probability&amp;#39;, y = &amp;#39;Density&amp;#39;)

p1 + p2 + plot_layout(ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each probability value represents a single observation. To convert the predicted probability to labels, the user needs to specify a threshold where every value above the threshold is defined as “No survive”, otherwise “survive”. Most of the cases, this creates problematic scenarios where two observations can be equally labeled in spite of having distinct probabilities (e.g. 0.6 and 0.95).&lt;/p&gt;
&lt;p&gt;By contrast, for each one of observations in the test set, the &lt;strong&gt;Bayesian model&lt;/strong&gt; does not provide a single probability value but a posterior distribution. We can represent the posterior distributions from the 92 observations (test set) with a boxplot, for instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_uncertainty &amp;lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %&amp;gt;% 
 as_tibble() %&amp;gt;%
 pivot_longer(everything(), names_to = &amp;#39;rank_obs&amp;#39;, values_to = &amp;#39;pred_surv_n_bay&amp;#39;)

plot_uncertainty %&amp;gt;%
 ggplot(aes(rank_obs, pred_surv_n_bay)) +
 geom_boxplot() +
 theme_bw() +
 labs(x = &amp;#39;Test set - Observation&amp;#39;, y = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to check the dispersion of the predicted outcome is with a ridge plot, that is especially useful when the number of samples is low. So, let’s pick only two observations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_uncertainty %&amp;gt;%
 # if you want to reproduce this code, just change the character &amp;#39;1&amp;#39; or &amp;#39;5&amp;#39; for any other.
 filter(rank_obs %in% c(&amp;#39;1&amp;#39;, &amp;#39;5&amp;#39;)) %&amp;gt;%
 ggplot(aes(pred_surv_n_bay, rank_obs)) +
  geom_density_ridges(aes(fill = rank_obs), alpha = 0.6) +
  theme_bw() +
  labs(y = &amp;#39;Test set - Observation&amp;#39;, x = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.0101&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we observe that both distributions have a “peak” (known as MAP [1]) above 0.5, therefore the predicted label would be, in both cases, ‘no survive’. But, are these two predictions equally certain? Well, we notice, at least, two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both MAPs have different values (sample 1 - 0.9, sample 5 - 7.2).&lt;/li&gt;
&lt;li&gt;Observation number 5 has a flatter curve in comparison with the number 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases, observation 5 reflects a higher uncertainty regarding its predicted label in comparison with observation 1. Should we make the same clinical decisions in both cases? Would this information be valuable in a clinical environment…? Probably yes, but first, we should find a way to measure it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-uncertainty-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measuring uncertainty [3]&lt;/h2&gt;
&lt;p&gt;A handy option is to use the standard deviation (sd) of the distribution, so we can estimate one value for each observation. With this in mind, we can plot the distribution of the sd from the 92 observations of the test dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;std_dev_tbl &amp;lt;- plot_uncertainty %&amp;gt;%
 group_by(rank_obs) %&amp;gt;%
 summarise(std_dev = sd(pred_surv_n_bay)) %&amp;gt;%
 ungroup()


std_dev_tbl %&amp;gt;% 
 ggplot(aes(std_dev)) +
 geom_density(fill = &amp;#39;steelblue&amp;#39;, alpha = 0.5) +
 theme_bw() +
 labs(x = &amp;#39;Standard deviation (sd)&amp;#39;,
    y = &amp;#39;Density&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of observations have a standard deviation of around 0.04. There are a few extreme values in the interval 0.08-0.10. In short, this plot shows that there are some observations whose sd is twice as high as others.&lt;/p&gt;
&lt;p&gt;We can filter and select observations based on the dispersion of its posterior distribution. For instance, we can split the test set of 92 observations in percentiles using the sd and plot the 1st (lowest sd) and 10th percentile (highest sd). In this way, it allows us to compare those observations with the highest and lowest standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_sd &amp;lt;- std_dev_tbl %&amp;gt;% 
 mutate(tile = ntile(std_dev, 10)) %&amp;gt;%
 filter(tile == 1 | tile == 10)

plot_uncertainty %&amp;gt;%
 # left_join(std_dev, by = &amp;#39;rank_obs&amp;#39;) %&amp;gt;%
 filter(rank_obs %in% top_sd$rank_obs) %&amp;gt;%
 ggplot(aes(pred_surv_n_bay, rank_obs)) +
 geom_density_ridges(aes(fill = rank_obs), alpha = 0.6, show.legend = FALSE) +
 theme_bw() +
 labs(y = &amp;#39;Test set - Observation&amp;#39;, x = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we easily identify to which group each observation belongs to. Independently of the predicted labels, should their predictions be considered equally likely? If the final user of the model just receives a categorical outcome, he/she is definitely skipping some valuable information since some predictions look more unlikely than others. As an alternative, predictions could be grouped into categories and neglect those with a high dispersion or make it clear than further support should be required.&lt;/p&gt;
&lt;p&gt;In this post, we have measured the uncertainty of observations and identifying those samples with high uncertainty. But, we have not talked yet about what is the origin of it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-some-predictions-are-more-unlikely-than-others&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why some predictions are more unlikely than others?&lt;/h2&gt;
&lt;p&gt;In other words, why our model has more doubts about a sample than others? I find two possible explanations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The sample is mislabeled.&lt;/li&gt;
&lt;li&gt;Group variability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first one is difficult to address but we can explore the group variability. Since we have three continuous explanatory variable, we can easily do a PCA with the function &lt;code&gt;prcomp&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl &amp;lt;- hab_test %&amp;gt;%
 # take only numeric columns
 select_if(is.numeric) %&amp;gt;%
 # Important - we need to scale and center each variable before PCA
 prcomp(scale = TRUE, center = TRUE) %&amp;gt;%
 tidy() %&amp;gt;%
 mutate(row = as.character(row)) %&amp;gt;%
 pivot_wider(id_cols = row, values_from = value, names_from = PC, names_prefix = &amp;#39;PC&amp;#39;) %&amp;gt;%
 left_join(top_sd %&amp;gt;% select(-std_dev), by = c(&amp;#39;row&amp;#39; = &amp;#39;rank_obs&amp;#39;)) %&amp;gt;%
 mutate(tile = ifelse(is.na(tile), &amp;#39;ok&amp;#39;, tile)) %&amp;gt;%
 left_join(hab_test %&amp;gt;% select(survival) %&amp;gt;% mutate(row = as.character(row_number())), 
      by = &amp;#39;row&amp;#39;)


pca_tbl %&amp;gt;%
 ggplot(aes(PC1, PC2)) +
  geom_point() +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we have observations in two categories, let’s split them into two plots:&lt;/p&gt;
&lt;p&gt;Since we have observations defined into two categories (survival = Yes, survival = No), let’s lay out the plot into two different:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl %&amp;gt;%
 ggplot(aes(PC1, PC2)) +
  geom_point() +
  theme_bw() +
  facet_grid(~ survival)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, we are going to highlight those observations that belong to the highest and lowest sd groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl %&amp;gt;%
 mutate(tile = case_when(
  tile == &amp;#39;1&amp;#39; ~ &amp;#39;lowest sd&amp;#39;,
  tile == &amp;#39;10&amp;#39; ~ &amp;#39;highest sd&amp;#39;,
  tile == &amp;#39;ok&amp;#39; ~ &amp;#39;ok&amp;#39;
 )) %&amp;gt;%
 ggplot(aes(PC1, PC2)) +
  geom_point(aes(fill = tile), color = &amp;#39;black&amp;#39;, shape = 21, size = 2) +
  theme_bw() +
  facet_grid(~ survival) +
  labs(fill = &amp;#39;Category&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the one hand, “lowest sd” group observations are centrally located in the plot. This reflects a tendency of these samples to have similar features values with observations belonging to their own label. On the other hand, “highest sd” group points tend to be dispersed from the rest, all over the components. It makes sense since the uncertainty to predict these points come from the fact that their own feature values are different from points on the same category.&lt;/p&gt;
&lt;p&gt;Surprisingly, there is a red point on the left panel whose location is centric respect to the rest of the values. This perhaps arises the disadvantage of reducing a probability distribution to a point-estimate (standard deviation). The dispersion estimation might have not be accurate enough and further ways of measuring might be needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As humans, we make decisions based on uncertainty, even though we are not aware of it. If the weather forecasters show 10% of raining on the weekend, we will probably make a plan to go to the mountain. With 90% we may rethink about it…When we are talking with someone about a delicate topic, we pick the words based on the uncertainty of his/her predicted response: words with a broad meaning and therefore ambiguous might not be chosen, due to the high uncertainty. Therefore, if we constantly map our reality and &lt;em&gt;act&lt;/em&gt; through the constant evaluation of uncertainty, &lt;em&gt;why should we believe in predictions from machines without a shadow of doubt?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;My personal view is that the measurement of uncertainty will end up being essential. Especially, for every decision process supported by a machine in a clinical environment. In that way, I find &lt;em&gt;Bayesian models&lt;/em&gt; a nice fit for many of the challenges of tomorrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[1] To calculate the roc curves of the Bayesian model’s predictions, a single probability value for each observation is required. There are multiple ways to estimate it, such as mean, median, and MAP (Highest Maximum A Posteriori). In this case, I chose the latest because it provided the highest performance.&lt;/p&gt;
&lt;p&gt;[2] The function &lt;code&gt;predict&lt;/code&gt; retrieves outcomes as probabilities because we specified &lt;code&gt;type = response&lt;/code&gt; as argument. Otherwise, the default output would be as logit.&lt;/p&gt;
&lt;p&gt;[3] I am interested to know more ways to estimate the “uncertainty” of a prediction. Please if you have any reference or idea, let me know! ;P&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Poisson distribution applied in genomics</title>
      <link>https://franciscorequena.com/post/poisson-distribution-applied-in-genomics/</link>
      <pubDate>Thu, 14 May 2020 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/poisson-distribution-applied-in-genomics/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;In this post, I will discuss briefly what is the Poisson distribution and describe two examples extracted from research articles in the genomics field. One of them based on the distribution of structural variants across the genome and other about &lt;em&gt;de novo&lt;/em&gt; variants in a patient cohort.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-distribution&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson distribution&lt;/h2&gt;
&lt;p&gt;In genomics, many of the events we observe correspond to countable values. For instance, the number of mutations found in a specific type of genomic regions or a patient cohort, sequence reads…The Poisson distribution is a &lt;em&gt;discrete probability model&lt;/em&gt; that takes countable numbers as the mentioned before and will be defined as &lt;em&gt;events&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;To calculate a Poisson distribution, we need to specify a single parameter which is called lambda (&lt;span class=&#34;math display&#34;&gt;\[\lambda\]&lt;/span&gt;). This value is known as the &lt;em&gt;rate parameter&lt;/em&gt; and defines the mean number of events in a given interval. In other words, if we know the total number of events of our system, we just need to divide it by the number of intervals. We will see further some examples of this.&lt;/p&gt;
&lt;p&gt;Once we know lambda, we can calculate the probability of seeing &lt;span class=&#34;math display&#34;&gt;\[/x\]&lt;/span&gt; number of events on a given interval, following this formula:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[P\left( x \right) = \frac{{e^{ - \lambda } \lambda ^x }}{{x!}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the next example, we are going to generate a Poisson distribution of 100 samples whose lambda value is equal to 2 with the &lt;code&gt;rpois&lt;/code&gt; function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries
library(dplyr)
library(ggplot2)
library(gganimate)
library(tidyr)


tibble(x = rpois(n = 100, lambda = 2)) %&amp;gt;%
 ggplot(aes(x)) +
 geom_histogram(binwidth = 1, fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) + 
 theme_bw() +
 labs(title = &amp;#39;Poisson distribution&amp;#39;, x = &amp;#39;Events&amp;#39;, y = &amp;#39;Count&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-14-poisson-genomics_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;An interesting aspect about the Poisson distribution: the mean and variance of the distribution are equal to the value of lambda. Therefore, the probability of finding an interval with 3 events (for instance) is higher as long as we increase the value of lambda.&lt;/p&gt;
&lt;p&gt;Confusing? Check out this example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;poisson_tbl &amp;lt;- tibble(lambda = seq(1.2, 7, 0.2)) %&amp;gt;% 
 rowwise() %&amp;gt;%
 mutate(value = paste(rpois(1000, lambda), collapse = &amp;#39;,&amp;#39;)) %&amp;gt;%
 separate_rows(value, sep = &amp;#39;,&amp;#39;) %&amp;gt;%
 mutate(value = as.integer(value)) %&amp;gt;%
 mutate(prob_7 = round(dpois(7, lambda), 5))

poisson_tbl %&amp;gt;%
 ggplot(aes(value)) +
 geom_histogram(binwidth = 1, fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) +
 transition_states(lambda) +
 geom_vline(xintercept = 7, color = &amp;#39;red&amp;#39;, linetype = 4 ) +
 labs(title = &amp;#39;Poisson distribution&amp;#39;, 
  subtitle = &amp;#39;lambda: {closest_state}, P(X = 7) : {poisson_tbl[poisson_tbl$lambda == {closest_state},] %&amp;gt;% pull(prob_7) %&amp;gt;% unique()}&amp;#39;,
  x = &amp;#39;Events&amp;#39;,
  y = &amp;#39;Count&amp;#39;) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-14-poisson-genomics_files/figure-html/unnamed-chunk-2-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;p&gt;In the example above, we are generating 30 times a set of 1000 random values following a Poisson distribution, increasing each time the value of lambda (from 1.2 to 7). The probability of finding an interval with 7 events (red line) is higher as long as we increase lambda.&lt;/p&gt;
&lt;p&gt;As we said before, we only need lambda to generate a Poisson distribution. Generally, we calculate this value if we know beforehand the number of events and intervals (as we will see in the second example, this is not always the case).&lt;/p&gt;
&lt;p&gt;Let’s put some examples of what an interval or event can be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Intervals&lt;/em&gt;. We can define intervals as &lt;em&gt;fixed&lt;/em&gt; time units, such as days, months, years…or also delimited areas of a geographical region (see this nice &lt;a href=&#34;https://sciprincess.wordpress.com/2019/03/05/cancer-clusters-and-the-poisson-distributions/&#34;&gt;post&lt;/a&gt; of cancer clusters or &lt;a href=&#34;https://www.wired.com/2012/12/what-does-randomness-look-like/&#34;&gt;this one&lt;/a&gt; about the distribution of impacts of V-1 and V-2 missiles during WWII).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;Events&lt;/em&gt;. The amount of clicks on a banner or the number of homicides or blackouts every year…An important condition is that each event is independent of each other (events occur independently).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every time, we perform a Poisson distribution, we always ask ourselves the same question: are these events distributed randomly across the intervals?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;poisson-distribution-in-genomics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poisson distribution in genomics&lt;/h2&gt;
&lt;p&gt;In genomics, as many other fields, there are different ways to define intervals and events. In the next examples, we will explore two completely different approaches:&lt;/p&gt;
&lt;div id=&#34;example-1-structural-variants-in-the-human-genome&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;Example 1&lt;/em&gt; : Structural variants in the human genome&lt;/h3&gt;
&lt;p&gt;Structural Variants (SVs) are mutations of more than 50bp and include deletions, duplications, inversion, translocations…These types of variants are important causes of multiple disorders, such as autism, schizoprenia, autoimmune diseases or developmental disorders.&lt;/p&gt;
&lt;p&gt;In this article &lt;a href=&#34;https://academic.oup.com/gbe/article/11/4/1136/5384496&#34;&gt;Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots&lt;/a&gt; [1], the authors explore whether the distribution of this type of mutations is random or follow any pattern across the genome.&lt;/p&gt;
&lt;p&gt;To do this, the researchers defined each structural variant as an event. Next, they divided the human genome into 100 kb intervals and after discarding incomplete intervals (the intervals need to be &lt;em&gt;fixed&lt;/em&gt;), they got a total of 28,103 intervals.&lt;/p&gt;
&lt;p&gt;The number of structural variants is 42,758 SVs. Therefore, to calculate lambda, they just had to divide this number by the total number of intervals. Finally, they generated a Poisson distribution and defined as “hotspot regions” all the intervals that exceeded the 99th percentile (6 SVs per 100 kb interval) concluding that these intervals had more SVs than expected by chance. Furthermore, they were able to identify “desert regions” as those intervals with a lower nº of SVs as compared with the number expected by chance.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;example-2-de-novo-variants-in-neurodevelopmental-disorders&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;em&gt;Example 2&lt;/em&gt; : De novo variants in neurodevelopmental disorders&lt;/h3&gt;
&lt;p&gt;In this article &lt;a href=&#34;https://www.nature.com/articles/nature25983&#34;&gt;De novo mutations in regulatory elements in neurodevelopmental disorders&lt;/a&gt; [2], the researchers explore the impact
of de novo variants (those present on children but not their parents) on regulatory regions of the genome in a cohort of patients with neurodevelopmental disorders. The majority of patients in this cohort did not present any de novo mutations (DNMs) in protein-coding genes. Therefore, a plausible hypothesis is to find some of these DNMs in those regions of the DNA yet unexplored: regulatory regions.&lt;/p&gt;
&lt;p&gt;Most of the human genome (98%) do not encode for protein regions. Therefore, the researchers decided to narrow down the search and focus only on those regulatory regions based on two features: regions highly conserved or experimentally validated.&lt;/p&gt;
&lt;p&gt;Finally, they found DNMs mapping this set of regulatory regions, which is great since it allows us to identify the causal mutation and find a diagnos….but wait a minute: Each person’s genome harbors many variants and most of the time, these variants are not harmful. So, we expect to find variants &lt;em&gt;randomly&lt;/em&gt; in these regulatory regions just &lt;em&gt;by chance&lt;/em&gt;. Yes, as you can guess…here it comes the &lt;em&gt;Poisson distribution&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The researchers knew this fact, therefore, to validate their results, they performed (surprise…) a Poisson distribution. First, they calculated the lambda parameter following the next approach:&lt;/p&gt;
&lt;p&gt;They focused on 6,239 individuals and counted the number of mutations found in regulatory regions. Furthermore, for each region, they calculated the expected number of mutations given the nucleotide context.&lt;/p&gt;
&lt;p&gt;Once they got the expected number of mutations for each regulatory region, they summed the values and multiplied by the total number of individuals (6,239) to obtain lambda. This value represents the expected number of mutations. Finally, they generated a Poisson distribution with lambda and compared it with the number of observed mutations. This allowed them to demonstrate, first, there were some subgroups of regulatory regions with an excess of the novo variants and second, this excess could be considered as statistically significative. These significant regions were mostly featured by fetal brain DNase signal.&lt;/p&gt;
&lt;p&gt;Contrary to what we saw at the beginning of the post, the approach to calculate lambda has been completely different in the second example. Precisely, this versatility makes the Poisson distribution one of the most popular ways to model counted data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;We discussed here about two different scenarios whose events were defined as mutations. But the Poisson distribution can help us to modelate other kind of events, for instance, sequence data. One of the most used techniques for the identification of peaks in Chip-seq analysis is called Model-based Analysis of ChIP-Seq data ( &lt;a href=&#34;https://genomebiology.biomedcentral.com/articles/10.1186/gb-2008-9-9-r137&#34;&gt;MACS&lt;/a&gt;). This program generates a Poisson distribution to identify regions with a higher number of reads than just by chance.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;When we use the genome to produce fixed size intervals to generate a Poisson distribution, an important aspect, it is the genome size. In principle, we already know this value: ~3,100 milions b.p (hg19) and ~ 3,200 millions b.p (hg38). Unfortunately, there are many inaccesible regions (gap regions) represented by Ns. Therefore, the use of the total size would artificially decrease the value of lambda and increase the number of false findings. As a consequence, we need to provide a &lt;em&gt;effective genome size&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Lin, Yen-Lung, and Omer Gokcumen. “Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots” Genome biology and evolution 11.4 (2019): 1136-1151.&lt;/p&gt;
&lt;p&gt;[2] Short, Patrick J., et al. “De novo mutations in regulatory elements in neurodevelopmental disorders.” Nature 555.7698 (2018): 611-616.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating pi value with Monte Carlo simulation</title>
      <link>https://franciscorequena.com/post/estimating-pi-value-with-monte-carlo-simulation/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/estimating-pi-value-with-monte-carlo-simulation/</guid>
      <description> 



&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load of libraries

library(tidyverse)
library(sp)
library(gganimate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;n_simulations &amp;lt;- 3000

df &amp;lt;- tibble(
  
  values_x = runif(n_simulations,0,1),
  values_y = runif(n_simulations,0,1)
)


circleFun &amp;lt;- function(center=c(0,0), diameter=1, npoints=100, start=0, end=2)
{
  tt &amp;lt;- seq(start*pi, end*pi, length.out=npoints)
  data.frame(x = center[1] + diameter / 2 * cos(tt), 
             y = center[2] + diameter / 2 * sin(tt))
}

dat &amp;lt;- circleFun(c(0,0), 2, start=1.5, end=2.5)


df &amp;lt;- df %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(label = point.in.polygon(values_x, values_y, dat$x, dat$y, mode.checked=FALSE)) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(count_in = cumsum(label),
         id = row_number(),
         pi_value = 4*(count_in / id))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df) +
  geom_rect(aes(xmin = -1, xmax = 1, ymin = -1, ymax = 1), 
            colour = &amp;quot;black&amp;quot;, show.legend = FALSE) +
  geom_polygon(aes(x, y), data = dat, alpha = 0.4) +
  geom_point(aes(x = values_x, y = values_y, fill = factor(label), group=id), 
             color = &amp;#39;black&amp;#39;, shape = 21, show.legend = FALSE) +
  theme_minimal() +
  coord_cartesian(ylim=c(0, 1), xlim = c(0,1)) +
  transition_reveal(id)  +
  labs(title = &amp;#39;Nº of observations: {frame_along} of {n_simulations}&amp;#39;,
       subtitle = &amp;#39;Estimated value of pi: {df$pi_value[as.integer(frame_along)]}&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-03-estimating-pi-value-with-monte-carlo-simulation_files/figure-html/unnamed-chunk-2-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df, aes(id, pi_value)) +
  geom_line() +
  geom_point(colour = &amp;#39;red&amp;#39;, size = 3) +
  transition_reveal(id) +
  geom_hline(yintercept = pi, color = &amp;#39;red&amp;#39;, linetype= &amp;#39;dashed&amp;#39;) +
  theme_bw() +
  labs(title = &amp;#39;Nº of observations: {frame_along} of {n_simulations}&amp;#39;,
       subtitle = &amp;#39;Estimated value of pi: {df$pi_value[as.integer(frame_along)]}&amp;#39;,
       x = &amp;#39;Sample size&amp;#39;,
       y = &amp;#39;Pi value&amp;#39;) +
    coord_cartesian(ylim=c(0, 4)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2020-05-03-estimating-pi-value-with-monte-carlo-simulation_files/figure-html/unnamed-chunk-3-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Exploring world flights using a network approach</title>
      <link>https://franciscorequena.com/post/exploring-world-airline-network/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/exploring-world-airline-network/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, I started to read &lt;a href=&#34;http://networksciencebook.com&#34;&gt;this free accessible book&lt;/a&gt; written by Albert-László Barabási. In the Chapter 4 of his book, it depicts the USA airport networks to &lt;strong&gt;represent scale-free networks&lt;/strong&gt;. I was wondering if we can get a &lt;em&gt;world picture&lt;/em&gt;, creating the same network but including the global routes using open data from internet.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-a-scale-free-network&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;1. What is a scale-free network?&lt;/h2&gt;
&lt;p&gt;Scale-free networks are characterized by a large number of nodes with low degree (number of links) and very few hubs with a high degree. If we represent the distribution of degrees of these nodes, it follows a power-law distribution. To illustrate this idea, let’s create a quick example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load libraries
library(tidygraph) 
library(ggraph)
library(igraph)
library(stringr)
library(tidyverse)
library(patchwork)
library(ggthemes)

# 1. Example showing a scale-free network

scale_free_net &amp;lt;- play_barabasi_albert(n = 1000, power = 1)

# 1.1 Scale-free network
p1 &amp;lt;- ggraph(scale_free_net, layout = &amp;#39;kk&amp;#39;) + 
  geom_edge_link(alpha = 0.3) + 
  geom_node_point(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;, shape = 21) +
  ggtitle(&amp;#39;Scale-free network&amp;#39;) + 
  theme_graph()

vector_values &amp;lt;- degree_distribution(scale_free_net)[-1] # Eliminate first element, it represents zero degree vertices
  
df &amp;lt;- data.frame(frequency = vector_values,
                 degrees = seq(1, length(vector_values),1))

# 1.2 Degree distribution
p2 &amp;lt;- ggplot(df, aes(degrees, frequency)) +
  geom_col(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) +
  ggtitle(&amp;#39;Degree Distribution of a scale-free network&amp;#39;) +
  ylab(&amp;#39;Relative frequency&amp;#39;) +
  xlab(&amp;#39;Number of links&amp;#39;) +
  theme_bw()

p1 + p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Many real networks share this feature. For instance, if we take a look how internet is organized and calculate the number of links that every site has, we find that the most of websites (nodes) have a low number of links (edges) and very few will have a large number of links (e.g. Google, Facebook…). Other examples are social, co-authorship or protein-protein network.&lt;/p&gt;
&lt;p&gt;We hope to see the same pattern through our airport’s network: &lt;em&gt;very few airports have a large number of routes while the most will have few routes&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-airports-and-routes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;2. Data (airports and routes)&lt;/h2&gt;
&lt;p&gt;At the beginning, we talked about creating our own network of airlines routes. To achieve this, we download our data from &lt;a href=&#34;https://openflights.org&#34;&gt;Openflights&lt;/a&gt; whose have a lot of information about flights. We will just download data about &lt;strong&gt;airports&lt;/strong&gt; (selecting: code, longitude and latitude) and &lt;strong&gt;routes&lt;/strong&gt; (selecting: name, code source, code destination and continent location). Besides, we will clean those observations with NA’s values or wrong strings.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: Aiports will be the nodes of our network and the routes will conform the edges between the nodes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Data with routes

# https://openflights.org/data.html#route

df &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat&amp;#39;,header = FALSE,
               stringsAsFactors = FALSE,
               col.names = c(&amp;#39;airline&amp;#39;, &amp;#39;airline_id&amp;#39;, &amp;#39;src&amp;#39;, &amp;#39;src_id&amp;#39;, &amp;#39;dest&amp;#39;, &amp;#39;dest_id&amp;#39;, &amp;#39;codeshare&amp;#39;,&amp;#39;stops&amp;#39;,  &amp;#39;equip&amp;#39;))[,c(3,5)]

# Data with airport information

# https://openflights.org/data.html#airport
df2 &amp;lt;- read.csv(&amp;#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat&amp;#39;,header = FALSE,
                   stringsAsFactors = FALSE)[,c(2,5,7,8,12)]
colnames(df2) &amp;lt;- c(&amp;#39;name&amp;#39;,&amp;#39;code&amp;#39;, &amp;#39;lat&amp;#39;, &amp;#39;long&amp;#39;, &amp;#39;location&amp;#39;)


# Clean data

df_airport &amp;lt;- df2 %&amp;gt;% 
  filter(!str_detect(code, fixed(&amp;quot;\\N&amp;quot;))) %&amp;gt;%
  filter(!str_detect(location, fixed(&amp;quot;\\N&amp;quot;))) %&amp;gt;%
  as_tibble()

tmp_loc &amp;lt;- str_split(df_airport$location, &amp;#39;/&amp;#39;)
df_airport$location &amp;lt;- map_chr(tmp_loc, function(x) x[[1]])
df_airport &amp;lt;- df_airport %&amp;gt;% mutate(location = as.factor(location))

df_routes &amp;lt;- df %&amp;gt;% 
  filter(!str_detect(src, fixed(&amp;quot;\\N&amp;quot;)) &amp;amp; !str_detect(dest, fixed(&amp;quot;\\N&amp;quot;))) %&amp;gt;%
  filter(!src == dest) %&amp;gt;%
  group_by(src, dest) %&amp;gt;%
  count() %&amp;gt;%
  arrange(desc(n)) %&amp;gt;%
  ungroup() %&amp;gt;%
  as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;airport-network-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;3. Airport Network visualization&lt;/h2&gt;
&lt;p&gt;To make possible the downstream analysis, we have to transform the observations of our dataframe into nodes and edges (tbl_graph object). We can do this thanks to the &lt;strong&gt;package ggraph&lt;/strong&gt;. Once we do this, we will be able to visualise the network applying different algorithms layers and calculate topological parameters of the nodes that otherwise would not be possible.&lt;/p&gt;
&lt;p&gt;For instance, we can choose the ‘mds’ layout (you can find many other layouts described &lt;a href=&#34;https://www.data-imaginist.com/2017/ggraph-introduction-layouts/&#34;&gt;here&lt;/a&gt;). This algorithm layout measures the shortest path between each node and display together those nodes which are closer in the network. Besides, we are going to calculate some scores per node and to make faster the algorithm, I will eliminate those airports whose number of routes are low.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Convert dataframe (df_routes) to tbl_graph object (df_graph)
df_graph &amp;lt;- as_tbl_graph(df_routes,directed = FALSE) %&amp;gt;% activate(edges) %&amp;gt;%
  filter(!edge_is_multiple()) %&amp;gt;% activate(nodes) %&amp;gt;%
  mutate(n_degree = centrality_degree(),
         betweenness = centrality_betweenness(),
         community = group_walktrap(),
         n_triangles = local_triangles(),
         clust = local_transitivity()) %&amp;gt;%
  left_join(df_airport, by = c(&amp;#39;name&amp;#39; = &amp;#39;code&amp;#39;)) %&amp;gt;%
  filter(!is.na(lat) &amp;amp; !is.na(long))

# ggraph(df_graph %&amp;gt;% activate(nodes) %&amp;gt;% filter(n_degree &amp;gt;= 10), layout = &amp;quot;mds&amp;quot;) + 
#   geom_edge_link(aes(edge_width = n), alpha = 0.1, edge_colour = &amp;#39;gray&amp;#39;) + 
#   geom_node_point(aes(size = n_degree, fill = location), shape = 21) +
#   scale_fill_brewer(palette = &amp;#39;Set1&amp;#39;) +
#   scale_size(range = c(0, 14)) +
#   theme_graph() +
#   guides(size=FALSE, edge_width = FALSE, fill = guide_legend(override.aes = list(size = 7))) +
#   ggtitle(&amp;#39;Airports network&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Besides, we plot the degree distribution of our network using ggplot2. For that, we convert our tbl_graph to a dataframe (the reverse step we did before) applying the function &lt;em&gt;activation(nodes)&lt;/em&gt; and then &lt;em&gt;as_tibble()&lt;/em&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Degree distribution
df_nodes &amp;lt;- df_graph %&amp;gt;% activate(nodes) %&amp;gt;% as_tibble()
ggplot(df_nodes, aes(n_degree)) +
  geom_histogram(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;, binwidth = 1) +
  ggtitle(&amp;#39;Degree Distribution of airports network&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Number of links&amp;#39;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we saw at the beginning, both networks follow a power-law distribution.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-is-my-airport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;4. Where is my airport?&lt;/h2&gt;
&lt;p&gt;At first glance, let’s take a look at the distribution of the airports around the world based on their region:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;worldmap &amp;lt;- borders(&amp;quot;world&amp;quot;, colour=&amp;quot;#efede1&amp;quot;, fill=&amp;quot;#efede1&amp;quot;) 

# Get airports by degree
ggplot(df_airport, aes(long, lat)) + worldmap + 
  geom_point(aes(fill = location), color = &amp;#39;black&amp;#39;, shape = 21) +
  theme_void() +
  guides(fill = guide_legend(override.aes = list(size = 7))) +
  ggtitle(&amp;#39; Aiports across the world by region&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see the biggest hubs are influenced by the economical situation and the population density of the region.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-are-the-best-connected-airports&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;What are the best connected airports?&lt;/h1&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(df_nodes %&amp;gt;% filter(n_degree &amp;gt;= 50), aes(long, lat)) + worldmap + 
  geom_point(aes(size = n_degree, fill = n_degree), pch = 21) +
  scale_fill_viridis_c() +
  theme_void() +
  scale_size_continuous(range = c(1,10))

p2 &amp;lt;- ggplot(df_nodes %&amp;gt;% top_n(20, n_degree), aes(reorder(name, -n_degree), n_degree)) +
  geom_col(aes(fill = n_degree), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 20 airport by number of routes&amp;#39;) +
  ylab(&amp;#39;Nº of routes&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))



p3 &amp;lt;- ggplot(df_nodes %&amp;gt;% group_by(location) %&amp;gt;% top_n(10, n_degree), aes(reorder(name, -n_degree), n_degree)) +
  geom_col(aes(fill = n_degree), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 airport by number of routes and region&amp;#39;) +
  ylab(&amp;#39;Nº of routes&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  facet_wrap(~ location, scales = &amp;#39;free_x&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))

p1 + p2  + plot_layout(ncol = 1, heights = c(3, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;what-is-the-longest-path-possible&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the longest path possible?&lt;/h2&gt;
&lt;p&gt;Can you guess how many steps would be required to travel the longest path possible between two airports? This number is called diameter and can be calculated easily:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_graph %&amp;gt;% activate(nodes) %&amp;gt;% 
  mutate(diam = graph_diameter()) %&amp;gt;% 
  distinct(diam) %&amp;gt;% 
  as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##    diam
##   &amp;lt;dbl&amp;gt;
## 1    12&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The longest path is 12 steps. Not so long if we take into account the remote distance of some of the airports (Siberia, Greenland, Pacific regions…).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-shortest-path-between-two-airports&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the shortest path between two airports?&lt;/h2&gt;
&lt;p&gt;We can select an airport and calculate the shortest path needed to reach another one. For instance, the Charles de Gaulle Airport (Paris) is one step from Adolfo Suárez Madrid–Barajas (Madrid), but what is the number of steps needed to reach the Hawai’s airport from Paris? Let’s calculate it:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;shortest_paths(df_graph, &amp;#39;CDG&amp;#39;, &amp;#39;HNL&amp;#39;)$vpath[[1]]&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## + 3/3209 vertices, named, from 9a73413:
## [1] CDG ORD HNL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;strong&gt;shortest path&lt;/strong&gt; from Paris to Honolulu is: Paris -&amp;gt; Chicago -&amp;gt; Honolulu.&lt;/p&gt;
&lt;p&gt;Now, imagine that we calculate all the shortest paths between Paris and the rest of airports and we repeat it with every airport and calculate the average. This value is called: &lt;strong&gt;average shortest path&lt;/strong&gt; and is average number of minimum connections required from any airport to any other airport.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_graph %&amp;gt;% activate(nodes) %&amp;gt;% 
  mutate(dist = graph_mean_dist()) %&amp;gt;% 
  distinct(dist) %&amp;gt;% 
  as_tibble()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 1
##    dist
##   &amp;lt;dbl&amp;gt;
## 1  3.97&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average shortesth path is 3.94, almost 4 steps on average to go from an airport to any other.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-is-the-longest-distance-possible-from-a-specific-airport&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What is the longest distance possible from a specific airport?&lt;/h2&gt;
&lt;p&gt;We are in Paris again, and we want to go to the most distant airport possible (in steps). This value is called &lt;strong&gt;eccentricity&lt;/strong&gt; and is specific for each airport. Let’s take a look at three of the most connected airports:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_graph_eccen &amp;lt;- df_graph %&amp;gt;% activate(nodes) %&amp;gt;% 
  mutate(eccentricity = node_eccentricity()) %&amp;gt;% as_tibble()
  
df_graph_eccen %&amp;gt;% 
  filter(name == &amp;#39;ATL&amp;#39; | name == &amp;#39;CDG&amp;#39; | name == &amp;#39;AMS&amp;#39;) %&amp;gt;% 
  select(name.y, eccentricity )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3 x 2
##   name.y                                           eccentricity
##   &amp;lt;chr&amp;gt;                                                   &amp;lt;dbl&amp;gt;
## 1 Hartsfield Jackson Atlanta International Airport            7
## 2 Charles de Gaulle International Airport                     7
## 3 Amsterdam Airport Schiphol                                  7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We would need 7 steps to go from Paris to the most distant airport, the same value obtained with Atlanta and Amsterdam airports. This make sense as we have selected nodes with the highest nº of routes. But the value 7 is the lowest that we can get?&lt;/p&gt;
&lt;p&gt;Let’s see the distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# The filter(eccentricity &amp;gt; 2) eliminate those airports that are disconnected from the main network and have a eccentricity from 0 to 2
ggplot(df_graph_eccen %&amp;gt;% filter(eccentricity &amp;gt; 2), aes(eccentricity)) +
  geom_histogram(fill = &amp;#39;steelblue&amp;#39;, color = &amp;#39;black&amp;#39;) +
  ylab(&amp;#39;Nº of airports&amp;#39;) +
  theme(text = element_text(size=20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we see above, most of the airports are located between 8 and 9. Those airports with the highest number of routes have a value of 7. But there is an airport whose value is 6.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; df_graph_eccen %&amp;gt;% filter(eccentricity == 6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 11
##   name  n_degree betweenness community n_triangles clust name.y   lat  long
##   &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;     &amp;lt;int&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
## 1 YYZ        147     249941.         1        2061 0.192 Leste~  43.7 -79.6
## # ... with 2 more variables: location &amp;lt;fct&amp;gt;, eccentricity &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Well, it is interesting that the airport with the lowest eccentricity is &lt;strong&gt;Lester B. Pearson International Airport&lt;/strong&gt; located at Toronto. Its number of routes (n_degree) is not very high but has an important particularity. If we see the map, Canada is a country with a large number of airports sparse along the territory. While the majority of airports have to “spend” steps to reach those distant airport (mainly at the north of the territory), this airport is very close to them and at the same time is close to the rest of airports across the world (USA, Europe, China…)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;where-are-the-hubs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Where are the hubs?&lt;/h2&gt;
&lt;p&gt;We can detect also the most relevant hubs (densely connected subgraphs) and display those airports that belongs to one of the top 10 hubs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_nodes %&amp;gt;% filter(community &amp;lt;= 10), aes(long, lat)) + worldmap + 
  geom_point(aes(fill = as.factor(community)), color = &amp;#39;black&amp;#39;, shape = 21) +
  theme_void() +
  scale_fill_brewer(palette = &amp;#39;Paired&amp;#39;) +
  guides(fill = guide_legend(override.aes = list(size = 12))) +
  ggtitle(&amp;#39; Aiports across the world by region&amp;#39;) +
  labs(fill=&amp;quot;List of Hubs&amp;quot;) +
  theme_map()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We have applied a walktrap community finding algorithm that uses random walks between the nodes and group those airports that are connected by short random walks.&lt;/p&gt;
&lt;p&gt;If you take a look at the map, these hubs represent not only a group of airports densely connected but also political and economical hubs. For instance, a hub includes Ex-soviets states, another Europe, Canary Islands and some cities from Magreb.&lt;/p&gt;
&lt;p&gt;In addition, we can classify the airports in 3 categories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Core&lt;/strong&gt;: Those aiports whose have the highest number of triangles (subgraph of 3 nodes and 3 edges). If an airport is located in many triangles, we consider it as a well connected airport.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Peryphery&lt;/strong&gt;: Airports that are located in distant regions with few routes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bridge&lt;/strong&gt;: Those airports that allow the communication between the airports that form the core and the periphery.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_nodes &amp;lt;- df_nodes %&amp;gt;% mutate(category = &amp;#39;Bridge&amp;#39;)
df_nodes$category &amp;lt;- ifelse(df_nodes$n_triangles &amp;gt; 400, &amp;#39;Core&amp;#39;, df_nodes$category)
df_nodes$category &amp;lt;- ifelse(df_nodes$clust == 0, &amp;#39;Periphery&amp;#39;, df_nodes$category)

ggplot(df_nodes, aes(long, lat)) + worldmap +
  geom_point(aes(fill = category), color = &amp;#39;black&amp;#39;, shape = 21) +
  facet_grid(category ~.) +
  theme_map() +
  theme(strip.text = element_text(size=25)) +
  guides(fill = guide_legend(override.aes = list(size = 20)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-are-the-best-connected-airports&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;5. Which are the best connected airports?&lt;/h1&gt;
&lt;p&gt;There are different ways to measure the connectivity of a node in a network. One of the most used is the betweenness centrality which is the sum of the shortest paths that pass through a node:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- ggplot(df_nodes %&amp;gt;% filter(n_degree &amp;gt;= 20), aes(long, lat)) + worldmap + 
  geom_point(aes(size = betweenness, fill = betweenness), pch = 21) +
  scale_fill_viridis_c() +
  theme_void() +
  scale_size_continuous(range = c(1,10))

p2 &amp;lt;- ggplot(df_nodes %&amp;gt;% top_n(20, betweenness), aes(reorder(name, -betweenness), betweenness)) +
  geom_col(aes(fill = betweenness), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 20 airport by number of betweenness&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))

p3 &amp;lt;- ggplot(df_nodes %&amp;gt;% group_by(location) %&amp;gt;% top_n(10, betweenness), aes(reorder(name, -betweenness), betweenness)) +
  geom_col(aes(fill = betweenness), color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 airport by number of betweenness and region&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Code Airport&amp;#39;) +
  facet_wrap(~ location, scales = &amp;#39;free_x&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))

p1 + p2 + plot_layout(ncol = 1, heights = c(3, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-15-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we see above, airports with with a high number of routes usually have a high betweenness. But we find an exception: the &lt;strong&gt;Ted Stevens Anchorage International Airport (ANL)&lt;/strong&gt;. Honestly, I did not expect this airport with the highest betweenness but if we take a look at the organization of the Alaska’s airports:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_routes_def &amp;lt;- df_routes %&amp;gt;% 
  left_join(df_airport, by = c(&amp;#39;src&amp;#39; = &amp;#39;code&amp;#39;)) %&amp;gt;%
  rename(long_src = long, lat_src = lat) %&amp;gt;%
  left_join(df_airport, by = c(&amp;#39;dest&amp;#39; = &amp;#39;code&amp;#39;)) %&amp;gt;%
  rename(long_dest = long, lat_dest = lat) %&amp;gt;%
  left_join(df_nodes, by = c(&amp;#39;src&amp;#39; = &amp;#39;name&amp;#39;)) %&amp;gt;%
  select(-lat, -long)

df_routes_anc &amp;lt;- df_routes_def %&amp;gt;% 
  filter( dest == &amp;#39;ANC&amp;#39;)
        
ggplot(df_routes_anc, aes(long_src, lat_src)) + worldmap +
  coord_map(xlim=c(-180,180)) +
            geom_segment(aes(x = long_src, y = lat_src,
                  xend = long_dest, yend = lat_dest),
               alpha = 0.7, color = &amp;#39;steelblue&amp;#39;) +
  scale_fill_viridis_c() +
  theme_map() +
  ggtitle(&amp;#39;Ted Stevens Anchorage International Airport&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-16-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Crossing this airport is required to reach the rest of airports in Alaska. Therefore, this create a bottleneck where most of nodes have to cross this airport before reach the rest.&lt;/p&gt;
&lt;div id=&#34;routes-by-number-of-airlines&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;7. Routes by number of airlines&lt;/h2&gt;
&lt;p&gt;We can take a look at those routes whose have the largest number of airlines:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_routes %&amp;gt;% top_n(20, n), aes(reorder(paste(src, dest, sep =&amp;#39; - &amp;#39;), -n), n)) +
  geom_col(aes(fill = n),  color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 20 routes by number of airlines&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Route&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-17-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;connections-between-madrid-and-dubai&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;8. Connections between Madrid and Dubai&lt;/h1&gt;
&lt;p&gt;We can display all the connections between Madrid and Dubai.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_routes_dubai &amp;lt;- df_routes_def %&amp;gt;% 
  filter(  src == &amp;#39;DXB&amp;#39; | dest == &amp;#39;DXB&amp;#39;)
        
p1 &amp;lt;- ggplot(df_routes_dubai, aes(long_src, lat_src)) + worldmap +
  coord_map(&amp;quot;gilbert&amp;quot;, xlim=c(-180,180)) +
  geom_segment(aes(x = long_src, y = lat_src,
                  xend = long_dest, yend = lat_dest),
               alpha = 0.3, color = &amp;#39;steelblue&amp;#39;) +
  scale_fill_viridis_c() +
  theme_map() +
  ggtitle(&amp;#39;Dubai International Airport connections&amp;#39;) 

p2 &amp;lt;- ggplot(df_routes_dubai %&amp;gt;% filter(src == &amp;#39;DXB&amp;#39;) %&amp;gt;% top_n(10, n) , aes(reorder(paste(src, dest, sep =&amp;#39; - &amp;#39;), -n), n)) +
  geom_col(aes(fill = n),  color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 routes by number of airlines&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Route&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))


p1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;FALSE Warning: Removed 10 rows containing missing values (geom_segment).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-18-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;…or the routes between Madrid and the rest of the world:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_routes_madrid &amp;lt;- df_routes_def %&amp;gt;% 
  filter(  src == &amp;#39;MAD&amp;#39; | dest == &amp;#39;MAD&amp;#39;)
        
p1 &amp;lt;- ggplot(df_routes_madrid, aes(long_src, lat_src)) + worldmap +
  coord_map(&amp;quot;gilbert&amp;quot;, xlim=c(-180,180)) +
  geom_segment(aes(x = long_src, y = lat_src,
                  xend = long_dest, yend = lat_dest),
               alpha = 0.3, color = &amp;#39;orange&amp;#39;) +
  scale_fill_viridis_c() +
  theme_map() +
  ggtitle(&amp;#39;Madrid Barajas International Airport connections&amp;#39;)

p2 &amp;lt;- ggplot(df_routes_madrid %&amp;gt;% filter(src == &amp;#39;MAD&amp;#39;) %&amp;gt;% top_n(10, n) , aes(reorder(paste(src, dest, sep =&amp;#39; - &amp;#39;), -n), n)) +
  geom_col(aes(fill = n),  color = &amp;#39;black&amp;#39;) +
  scale_fill_viridis() +
  ggtitle(&amp;#39;Top 10 routes by number of airlines&amp;#39;) +
  ylab(&amp;#39;Frequency&amp;#39;) +
  xlab(&amp;#39;Route&amp;#39;) +
  theme_bw() +
  guides(fill = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        text = element_text(size=20))


p1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;FALSE Warning: Removed 4 rows containing missing values (geom_segment).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-19-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/exploring-world-airline-network_files/figure-html/unnamed-chunk-19-2.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;relevance of an airport&lt;/strong&gt; in the network can be assessed through different metrics: nº of routes, nº of triangles, clustering, betweenness, eccentricity or shortest path. At the same time, the &lt;strong&gt;identification of groups of airports&lt;/strong&gt;, we have clustered airports by continent, random walks algorithm, or using a blend of centrality measures filtering the nodes in three groups (core, bridge, peripherial).&lt;/p&gt;
&lt;p&gt;In conclusion, network science allows us to improve our knowledge about data that can be converted into a network, through the use of multiple approaches.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final notes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To simplify this post, I have not included the direction of the edges neither the real distance between airports.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A very interesting point is the analysis of the resilence: what would happen if we delete a specific airport from the network? Would the impact be equal across the aiports?&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Can we predict cases of dengue with climate variables?</title>
      <link>https://franciscorequena.com/post/can-we-predict-cases-of-dengue-with-climate-variables/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://franciscorequena.com/post/can-we-predict-cases-of-dengue-with-climate-variables/</guid>
      <description>


&lt;p&gt;Recently, I discovered a new website about competitions that it is not called Kaggle! Its name is Drivendata.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DrivenData&lt;/strong&gt; offers different competitions related with multiple types of field, such as health (oh yes!), ecology, society… with a common element: to face the world’s biggest social challenges.&lt;/p&gt;
&lt;p&gt;I decided to join my first competition called &lt;em&gt;‘DengAI: Predicting Disease Spread‘&lt;/em&gt;. In this case, the user receives a set of weather information (temperatures, precipitations, vegetations) from two cities: &lt;strong&gt;San Juan&lt;/strong&gt; (Puerto Rico) and &lt;strong&gt;Iquitos&lt;/strong&gt; (Peru) with total cases of dengue by year and week of every year.&lt;/p&gt;
&lt;p&gt;The goal of the competition is to develop a prediction model that would be able to anticipate the cases of dengue in every city depending on a set of climate variables.&lt;/p&gt;
&lt;p&gt;The DrivenData’s blog wrote some days ago, a post about a fast approach with this dataset. It was written in Python. So, I decided to “translate” to R language.&lt;/p&gt;
&lt;p&gt;The next code is divided into three main points:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Code with clean tasks (transform NA values, remove of columns…) and exploratory analyses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Function with every step during cleaning of data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Development of model, prediction and comparison of predicted vs real total cases detected.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load of libraries

library(tidyverse)
library(zoo)
library(corrplot)
library(MASS)
library(reshape2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load data
train_features &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_features_train.csv&amp;#39;)
  
train_labels &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_labels_train.csv&amp;#39;)

test_features &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_features_test.csv&amp;#39;)

submission_format &amp;lt;- read.csv(&amp;#39;data/dengue_post/submission_format.csv&amp;#39;)
  
# Filter of data by city  

sj_train_labels &amp;lt;- filter(train_labels, city == &amp;#39;sj&amp;#39;)
sj_train_features &amp;lt;- filter(train_features, city == &amp;#39;sj&amp;#39;)

iq_train_labels &amp;lt;- filter(train_labels, city == &amp;#39;iq&amp;#39;)
iq_train_features &amp;lt;- filter(train_features, city == &amp;#39;iq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Is there NA values?

df_na_sj &amp;lt;- as.data.frame(apply(sj_train_features,2, function(x) any(is.na(x))))
colnames(df_na_sj) &amp;lt;- &amp;#39;is_there_NA&amp;#39;
df_na_sj$number_NA &amp;lt;- apply(sj_train_features,2, function(x) sum(is.na(x)))
df_na_sj$mean_NA &amp;lt;- apply(sj_train_features, 2, function(x) mean(is.na(x)))

df_na_iq &amp;lt;- as.data.frame(apply(iq_train_features, 2, function(x) any(is.na(x))))
colnames(df_na_iq) &amp;lt;- &amp;#39;is_there_NA&amp;#39;
df_na_iq$number_NA &amp;lt;- apply(iq_train_features, 2, function(x) sum(is.na(x)))
df_na_iq$mean_NA &amp;lt;- apply(iq_train_features, 2, function(x) mean(is.na(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Vegetation Index over Time Plot with NAs

ggplot(sj_train_features, aes(x = as.Date(week_start_date), y = ndvi_ne )) +
  ggtitle(&amp;#39;Vegetation Index over Time&amp;#39;) +
  theme_bw() +
  xlab(&amp;#39;Title&amp;#39;) +
  geom_line(na.rm = FALSE, color = &amp;#39;blue&amp;#39;) +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove &amp;#39;weekofyear&amp;#39; column

sj_train_features &amp;lt;- dplyr::select(sj_train_features, -week_start_date)

iq_train_features &amp;lt;- dplyr::select(iq_train_features, -week_start_date)

# Fill the NA values with the previous value

sj_train_features &amp;lt;- sj_train_features %&amp;gt;%
            do(na.locf(.))

iq_train_features &amp;lt;- iq_train_features %&amp;gt;%
            do(na.locf(.))

# Distribution of labels

# print(mean(sj_train_labels$total_cases))
# print(var(sj_train_labels$total_cases))
# 
# print(mean(iq_train_labels$total_cases))
# print(var(iq_train_labels$total_cases))


ggplot(sj_train_labels, aes(x = total_cases)) +
  theme_bw() +
  ggtitle(&amp;#39;Cases of dengue in San Juan&amp;#39;) +
  geom_histogram() +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iq_train_labels, aes(x = total_cases)) +
  theme_bw() +
  ggtitle(&amp;#39;Cases of dengue in Iquitos&amp;#39;) +
  geom_histogram() +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add total_cases column to *_train_features dataframes


# sj_train_features &amp;lt;- left_join(sj_train_features, sj_train_labels, by = c(&amp;#39;city&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;weekofyear&amp;#39;))
 sj_train_features$total_cases &amp;lt;- sj_train_labels$total_cases

# iq_train_features &amp;lt;- left_join(iq_train_features, iq_train_labels, by = c(&amp;#39;city&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;weekofyear&amp;#39;))
iq_train_features$total_cases &amp;lt;- iq_train_labels$total_cases

# Correlation matrix

m_sj_train_features &amp;lt;- data.matrix(sj_train_features)
m_sj_train_features &amp;lt;- cor(x = m_sj_train_features[,3:24], use = &amp;#39;complete.obs&amp;#39;, method = &amp;#39;pearson&amp;#39;)

m_iq_train_features &amp;lt;- data.matrix(iq_train_features)
m_iq_train_features &amp;lt;- cor(x = m_iq_train_features[,3:24], use = &amp;#39;everything&amp;#39;, method = &amp;#39;pearson&amp;#39;)

# Correlation Heatmap

corrplot(m_sj_train_features, type = &amp;#39;full&amp;#39;, tl.col = &amp;#39;black&amp;#39;, method=&amp;quot;shade&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrplot(m_iq_train_features, type = &amp;#39;full&amp;#39;, tl.col = &amp;#39;black&amp;#39;, method = &amp;#39;shade&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation Bar plot

df_sj_train_features &amp;lt;- data.frame(m_sj_train_features)[2:21,] 
df_sj_train_features &amp;lt;- dplyr::select(df_sj_train_features, total_cases) 
                                    
df_iq_train_features &amp;lt;- data.frame(m_iq_train_features)[2:21,]
df_iq_train_features &amp;lt;- dplyr::select(df_iq_train_features, total_cases) 

ggplot(df_sj_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;) +
  theme_bw() +
  ggtitle(&amp;#39;Correlation of variables in San Juan&amp;#39;) +
  ylab(&amp;#39;Correlation&amp;#39;) +
  xlab(&amp;#39;Variables&amp;#39;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_iq_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;) +
  theme_bw() +
  ggtitle(&amp;#39;Correlation of variables in Iquitos&amp;#39;) +
  ylab(&amp;#39;Correlation&amp;#39;) +
  xlab(&amp;#39;Variables&amp;#39;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function data cleaning

data_clean &amp;lt;- function(df_dengue_features, df_dengue_labels = NULL, add_cases = TRUE) {
  
  # Filter by city
  sj_df_dengue_features &amp;lt;- filter(df_dengue_features, city == &amp;#39;sj&amp;#39;)
  iq_df_dengue_features &amp;lt;- filter(df_dengue_features, city == &amp;#39;iq&amp;#39;)
  
  if (add_cases == TRUE) {
  sj_df_dengue_labels &amp;lt;- filter(df_dengue_labels, city == &amp;#39;sj&amp;#39;)
  iq_df_dengue_labels &amp;lt;- filter(df_dengue_labels, city == &amp;#39;iq&amp;#39;)
  }
  # Removing week_start_date column
  sj_df_dengue_features &amp;lt;- dplyr::select(sj_df_dengue_features, -week_start_date)
  iq_df_dengue_features &amp;lt;- dplyr::select(iq_df_dengue_features, -week_start_date)

  # Fill of NA values with the previous value
  sj_df_dengue_features &amp;lt;- sj_df_dengue_features %&amp;gt;%
    do(na.locf(.))
  
  iq_df_dengue_features &amp;lt;- iq_df_dengue_features %&amp;gt;%
    do(na.locf(.))
  
  # Add total_cases to dataframe with features
  if (add_cases == TRUE) {
  sj_df_dengue_features$total_cases &amp;lt;- sj_df_dengue_labels$total_cases
  iq_df_dengue_features$total_cases &amp;lt;- iq_df_dengue_labels$total_cases
  }
  
  # Converting character columns into numbers
  sj_df_dengue_features &amp;lt;- as.data.frame(apply(sj_df_dengue_features,2,as.numeric))
  sj_df_dengue_features$city &amp;lt;- rep(&amp;#39;sj&amp;#39;, nrow(sj_df_dengue_features))
  iq_df_dengue_features &amp;lt;- as.data.frame(apply(iq_df_dengue_features,2,as.numeric))
  iq_df_dengue_features$city &amp;lt;- rep(&amp;#39;iq&amp;#39;, nrow(iq_df_dengue_features))
  
  result &amp;lt;- list(sj_df_dengue_features, iq_df_dengue_features )
  
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting data_training clean

data_train &amp;lt;- data_clean(train_features, train_labels, TRUE)

# Getting negative binomials models by city

training_sj &amp;lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +
                     reanalysis_dew_point_temp_k +
                     station_min_temp_c +
                     station_avg_temp_c, data = data_train[[1]])

training_iq &amp;lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +
                        reanalysis_dew_point_temp_k +
                        station_min_temp_c +
                        station_avg_temp_c, data = data_train[[2]])

# Getting data_test clean

data_test &amp;lt;- data_clean(test_features, add_cases = FALSE)


# Testing model with training data

prediction_train_sj &amp;lt;-  predict(training_sj, data_train[[1]], type = &amp;#39;response&amp;#39;)
prediction_train_iq &amp;lt;-  predict(training_iq, data_train[[2]], type = &amp;#39;response&amp;#39;)

df_prediction_train_sj &amp;lt;- data.frame(&amp;#39;prediction&amp;#39; = prediction_train_sj, &amp;#39;actual&amp;#39; = data_train[[1]]$total_cases,
                                     &amp;#39;time&amp;#39; = as.Date(train_features$week_start_date[1:936]))
df_prediction_train_sj &amp;lt;- melt(df_prediction_train_sj, id.vars = &amp;#39;time&amp;#39;)
ggplot(df_prediction_train_sj, aes(x = time, y = value, color = variable)) +
  geom_line() +
  ggtitle(&amp;#39;Dengue predicted Cases vs. Actual Cases (City-San Juan) &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_prediction_train_iq &amp;lt;- data.frame(&amp;#39;prediction&amp;#39; = prediction_train_iq, &amp;#39;actual&amp;#39; = data_train[[2]]$total_cases,
                                     &amp;#39;time&amp;#39; = as.Date(train_features$week_start_date[937:1456]))
df_prediction_train_iq &amp;lt;- melt(df_prediction_train_iq, id.vars = &amp;#39;time&amp;#39;)
ggplot(df_prediction_train_iq, aes(x = time, y = value, color = variable)) +
  geom_line() +
  ggtitle(&amp;#39;Dengue predicted Cases vs. Actual Cases (City-Iquitos) &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://franciscorequena.com/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prediction of total_cases in the data set

prediction_sj &amp;lt;-  predict(training_sj, data_test[[1]], type = &amp;#39;response&amp;#39;)
prediction_iq &amp;lt;-  predict(training_iq, data_test[[2]], type = &amp;#39;response&amp;#39;)
 
data_prediction_sj &amp;lt;- data.frame(&amp;#39;city&amp;#39; = rep(&amp;#39;sj&amp;#39;, length(prediction_sj) ), 
                                 &amp;#39;total_cases&amp;#39; = prediction_sj, 
                                 &amp;#39;weekofyear&amp;#39; = data_test[[1]]$weekofyear,
                                 &amp;#39;year&amp;#39; = data_test[[1]]$year )

data_prediction_iq &amp;lt;- data.frame(&amp;#39;city&amp;#39; = rep(&amp;#39;iq&amp;#39;, length(prediction_iq) ), 
                                 &amp;#39;total_cases&amp;#39; = prediction_iq,
                                 &amp;#39;weekofyear&amp;#39; = data_test[[2]]$weekofyear,
                                 &amp;#39;year&amp;#39; = data_test[[2]]$year)


  
submission_format$total_cases &amp;lt;- as.numeric(c(data_prediction_sj$total_cases, 
                                                   data_prediction_iq$total_cases))

submission_format$total_cases &amp;lt;- round(submission_format$total_cases, 0)
  
write.csv(submission_format,
          file = &amp;#39;submission_format_submit.csv&amp;#39;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
