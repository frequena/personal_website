<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>machine learning | Francisco Requena</title>
    <link>/categories/machine-learning/</link>
      <atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>machine learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 12 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>machine learning</title>
      <link>/categories/machine-learning/</link>
    </image>
    
    <item>
      <title>An introduction to ROC curves with animated examples</title>
      <link>/post/roc-curves-an-animated-example/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/roc-curves-an-animated-example/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Receiver operating characteristic (ROC) curves is one of the concepts I have struggled most. As a personal view, I do not find it intuitive or clear at first glance. Possibly, because we are used to interpreting information as single values, such as mean, median, accuracy…ROC curves are different because it represents a group of values conforming a curve. Besides, it is the most popular way to represent a model performance for a &lt;em&gt;particular dataset&lt;/em&gt; where the task is a binary classification.&lt;/p&gt;
&lt;p&gt;Before explaining where the ROC curves come from, let’s focus on what is the outcome of most of the classification models. To illustrate this point, let’s train a few logistic regression models with a toy dataset and use the package &lt;code&gt;parsnip&lt;/code&gt; which provides a common interface to train models from many other packages.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;For this post, we are going to use a dataset that includes 310 patients and six explanatory variables related to biomechanical features of the vertebral column. Besides, it contains a response variable &lt;code&gt;abnormality&lt;/code&gt; that defines if the patient has been diagnosed with a medical condition in the vertebral column (&lt;code&gt;yes&lt;/code&gt; and &lt;code&gt;no&lt;/code&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(gganimate) # animated plots
library(magick) # combine two gif
library(yardstick) # roc_curve helper
library(parsnip) # train logistic regression models


# Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00212/
verterbral &amp;lt;- read.table(&amp;#39;data/column_2C.dat&amp;#39;, header = FALSE, sep = &amp;#39; &amp;#39;)
colnames(verterbral) &amp;lt;- c(&amp;#39;pelvic_incidence&amp;#39;, 
             &amp;#39;pelvic_tilt&amp;#39;, 
             &amp;#39;lumbar_lordosis_angle&amp;#39;, 
             &amp;#39;sacral_slope&amp;#39;, &amp;#39;pelvic_radius&amp;#39;, 
             &amp;#39;degree_spondylolisthesis&amp;#39;, &amp;#39;abnormality&amp;#39;) 

verterbral &amp;lt;- verterbral %&amp;gt;%
 select(abnormality, everything()) %&amp;gt;%
 mutate(id = row_number()) %&amp;gt;%
 mutate(abnormality = factor(if_else(abnormality == &amp;#39;AB&amp;#39;, &amp;#39;yes&amp;#39;, &amp;#39;no&amp;#39;)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data&lt;/h2&gt;
&lt;p&gt;…and split it in training (70%) and test set (30%).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(992)

training_ids &amp;lt;- verterbral %&amp;gt;% sample_frac(0.7) %&amp;gt;% pull(id)

vert_training &amp;lt;- verterbral %&amp;gt;% 
 filter(id %in% training_ids)

vert_test &amp;lt;- verterbral %&amp;gt;% 
 filter(!id %in% training_ids)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;train-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Train models&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_model_one &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence, data = vert_training)


logistic_model_two &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt, data = vert_training)

logistic_model_three &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope, data = vert_training)

logistic_model_four &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius, data = vert_training)

logistic_model_five &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius + lumbar_lordosis_angle, data = vert_training)

logistic_model_all &amp;lt;- logistic_reg() %&amp;gt;%
 set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;%
 set_mode(&amp;quot;classification&amp;quot;) %&amp;gt;%
 fit(abnormality ~ ., data = vert_training[,-ncol(vert_training)])

check_pred &amp;lt;- vert_test %&amp;gt;%
 select(id) %&amp;gt;%
 mutate( pred_logistic_one = predict(logistic_model_one, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_two = predict(logistic_model_two, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_three = predict(logistic_model_three, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_four = predict(logistic_model_four, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_five = predict(logistic_model_five, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes,
     pred_logistic_all = predict(logistic_model_all, vert_test, type = &amp;#39;prob&amp;#39;)$.pred_yes
     ) %&amp;gt;%
 left_join(verterbral %&amp;gt;% select(id, abnormality), by = &amp;#39;id&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;plot-raw-outcome&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plot raw outcome&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;check_pred %&amp;gt;% glimpse()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 93
## Variables: 8
## $ id                  &amp;lt;int&amp;gt; 1, 3, 6, 9, 10, 11, 12, 13, 20, 22, 27, 43, 44,...
## $ pred_logistic_one   &amp;lt;dbl&amp;gt; 0.7499979, 0.8028267, 0.4747471, 0.5213208, 0.4...
## $ pred_logistic_two   &amp;lt;dbl&amp;gt; 0.7940143, 0.8233949, 0.5251698, 0.5516503, 0.3...
## $ pred_logistic_three &amp;lt;dbl&amp;gt; 0.7939271, 0.8233653, 0.5240107, 0.5505372, 0.3...
## $ pred_logistic_four  &amp;lt;dbl&amp;gt; 0.9301525, 0.9029361, 0.4239115, 0.5079236, 0.8...
## $ pred_logistic_five  &amp;lt;dbl&amp;gt; 0.9035941, 0.8837432, 0.3236233, 0.5683134, 0.9...
## $ pred_logistic_all   &amp;lt;dbl&amp;gt; 0.7531384, 0.2486521, 0.4043606, 0.8549908, 0.9...
## $ abnormality         &amp;lt;fct&amp;gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, ye...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For each observation of the test set, the models retrieve a probability. This value represents how likely that observation belongs to the label &lt;code&gt;abnormality == yes&lt;/code&gt;[1].&lt;/p&gt;
&lt;p&gt;Probability is not a particular output format of logistic regressions models [2], but a standard way of many models. For instance, models based on tree decisions, such as gradient boosting [3] or random forest, retrieve probabilities as output.&lt;/p&gt;
&lt;p&gt;To make it simple, for now, we will use only the predicted values (&lt;code&gt;pred_logistic_all&lt;/code&gt;) from the trained model that used all the explanatory variables.&lt;/p&gt;
&lt;p&gt;Since we have all the probabilities values retrieved by the model in the variable &lt;code&gt;pred_logistic_all&lt;/code&gt;, we can explore the distribution of the model’s outcome. To do this, there are two common ways: boxplot and density plots. For the scope of this post, we are going to use the latter. Besides, since our observations are defined by two label options (survival == ‘yes’, survival = ‘no’), we are going to plot two different distributions, one for each label:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;check_pred %&amp;gt;% 
 ggplot(aes(pred_logistic_all)) +
  geom_density(aes(fill = abnormality), alpha = 0.4) +
  theme_bw() +
  scale_fill_viridis_d()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can extract some ideas from the above plot:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Since this value represents the probability of an observation to belong to &lt;code&gt;abnormality = &#39;yes&#39;&lt;/code&gt;, it makes sense to find observations whose real label is ‘yes’ with high probability. On the other way around, we expect to find observations whose real label is &lt;code&gt;abnormality = &#39;no&#39;&lt;/code&gt; with low probability. Though this is what we expect, this is not always the case, since we find also observations whose probability of belonging to &lt;code&gt;abnormality = &#39;yes&#39;&lt;/code&gt; is quite low, even though, its real label is &lt;code&gt;yes&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There is a twilight zone, where we have observations from both labels levels that have “inaccurate” probabilities.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;We can somehow see how well a model performed based on the overlapping of these two distributions.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A perfect model would retrieve both distributions with no overlapping.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since these models do not retrieve directly the label of the response variable. A threshold to discretize a continuous probability is required to transform the probability into a label. This is a difficult part, because no matter where you define the threshold, we face a trade-off between the percentage of False Positives (FP) and False Negatives (FN). Besides, there is not a clear rule for it, and the results can be pretty arbitrary.&lt;/p&gt;
&lt;p&gt;Another problem arises: if the selection of the threshold is arbitrary, how do we &lt;em&gt;compare different models&lt;/em&gt;? Here it is where the ROC curves come out!&lt;/p&gt;
&lt;p&gt;ROC curves try to overcome this issue, taking into account all the possible scenarios given multiple thresholds. This allows us to estimate the performance of our model independently of the threshold you take.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-create-a-roc-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to create a ROC curve?&lt;/h2&gt;
&lt;p&gt;To create a ROC curve, the starting point is precisely the same information we used to display the density plot: a column with predicted probabilities and another with the real labels. Each row is an observation of the test set.&lt;/p&gt;
&lt;p&gt;Once we have this information, we define as many thresholds [4] as observations found in the test set (plus Inf and -Inf). These values are defined by the probability of each observation.&lt;/p&gt;
&lt;p&gt;Furthermore, for each threshold value, all the probabilities above it will be identified as &lt;code&gt;abnormality = yes&lt;/code&gt; and we count the number of True Positive (TP), True Negative (TN), but also, those observations predicted as abnormality = yes but actually are &lt;code&gt;no&lt;/code&gt; (False Positive (FP)) and those predicted as &lt;code&gt;no&lt;/code&gt; but actually are &lt;code&gt;yes&lt;/code&gt; (False Negative (FN)).&lt;/p&gt;
&lt;p&gt;Finally, we need this information to calculate the values that will make up the ROC curve axis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sensitivity&lt;/strong&gt; (also known as True positive rate). This metric reflects the number of positives in the test dataset that are correctly identified.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Specificity&lt;/strong&gt; (also known as True negative rate). This metric measures the number of negatives in the test dataset that are correctly identified.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases, a result of 1 is considered perfect.&lt;/p&gt;
&lt;p&gt;To facilitate this, there are multiple packages in R to calculate the ROC curve. For this case, I am going to use the function &lt;code&gt;roc_curve&lt;/code&gt; from the package &lt;code&gt;yardstick&lt;/code&gt; which I recommend.&lt;/p&gt;
&lt;p&gt;Check the output of the function &lt;code&gt;roc_curve&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we just need to specify the column with the labels (abnormality) and the predicted probabilities (pred_logistic_all)
roc_logistic &amp;lt;- check_pred %&amp;gt;% roc_curve(abnormality, pred_logistic_all)
roc_logistic %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   .threshold specificity sensitivity
##        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt;
## 1  -Inf                1      0     
## 2     0.0179           1      0.0294
## 3     0.0292           1      0.0588
## 4     0.0369           1      0.0882
## 5     0.0372           1      0.118 
## 6     0.0493           1      0.147&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, for the visualization, we only need to modify the specificity variable as &lt;code&gt;1 - specificity&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roc_logistic %&amp;gt;%
 ggplot(aes(x = (1 - specificity), y = sensitivity)) +
 geom_line() +
 geom_abline(linetype = 3) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animated-roc-curve&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Animated ROC curve&lt;/h2&gt;
&lt;p&gt;To build some intuition, we can see how to build the ROC curve while we define thresholds values in the density plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;a &amp;lt;- check_pred %&amp;gt;% 
 ggplot() +
  geom_density(aes(x = pred_logistic_all, fill = abnormality), alpha = 0.5) +
  geom_vline(data = roc_logistic %&amp;gt;% filter( .threshold != Inf) %&amp;gt;% filter(.threshold != -Inf), aes(xintercept = .threshold, group = .threshold)) +
  transition_reveal(.threshold) +
  theme_bw()


b &amp;lt;- roc_logistic %&amp;gt;%
 ggplot(aes(x = (1 - specificity), y = sensitivity)) +
 geom_line() +
  geom_point(colour = &amp;#39;red&amp;#39;, size = 3) +
  transition_reveal(sensitivity) +
 geom_abline(linetype = 3) +
 theme_bw()

# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition
a_gif &amp;lt;- animate(a, width = 440, height = 440)
b_gif &amp;lt;- animate(b, width = 440, height = 440)

a_mgif &amp;lt;- image_read(a_gif)
b_mgif &amp;lt;- image_read(b_gif)

new_gif &amp;lt;- image_append(c(a_mgif[1], b_mgif[1]))
for(i in 2:100){
 combined &amp;lt;- image_append(c(a_mgif[i], b_mgif[i]))
 new_gif &amp;lt;- c(new_gif, combined)
}

new_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-8-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-six-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparing six models&lt;/h2&gt;
&lt;p&gt;At the beginning of this post, we trained five models, each one with a different number of explanatory variables: one, two, three, four, five, and six.&lt;/p&gt;
&lt;p&gt;We can easily display their probability distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comparison_six &amp;lt;- check_pred %&amp;gt;%
 pivot_longer(starts_with(&amp;#39;pred&amp;#39;), names_to = &amp;#39;model&amp;#39;, values_to = &amp;#39;prob&amp;#39;) %&amp;gt;%
 mutate(model = fct_inorder(as.factor(model)))


comparison_six %&amp;gt;%
 ggplot(aes(prob)) +
  geom_density(aes(fill = abnormality), alpha = 0.4) +
  theme_bw() +
  scale_fill_viridis_d() +
 facet_wrap(~ model)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the example above, we observe that the overlapping between both distributions decreases as we increase the number of explanatory variables. In other words, since we increase the amount of useful information to discriminate between the two labels (&lt;em&gt;yes&lt;/em&gt;, &lt;em&gt;no&lt;/em&gt;), the predictive power of the model improves.&lt;/p&gt;
&lt;p&gt;Besides, we can plot their ROC curves:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;comparison_six %&amp;gt;%
 group_by(model) %&amp;gt;%
 roc_curve(abnormality, prob) %&amp;gt;%
  ggplot(aes(x = (1 - specificity), y = sensitivity)) +
 geom_line(aes(color = model)) +
 geom_abline(linetype = 3) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;animated-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Animated comparison&lt;/h2&gt;
&lt;p&gt;Finally, we can replicate the previous code and compare the six models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;roc_comparison &amp;lt;- comparison_six %&amp;gt;%
 group_by(model) %&amp;gt;%
 roc_curve(abnormality, prob) %&amp;gt;% ungroup()

a &amp;lt;- comparison_six %&amp;gt;%
 ggplot(aes(prob)) +
  geom_density(aes(fill = abnormality), alpha = 0.4) +
  geom_vline(data = roc_comparison %&amp;gt;% filter(.threshold != Inf) %&amp;gt;% filter(.threshold != -Inf),
        aes(xintercept = .threshold, group = .threshold)) +
  theme_bw() +
  scale_fill_viridis_d() +
  transition_reveal(.threshold) +
  facet_wrap(~ model)



b &amp;lt;- roc_comparison %&amp;gt;%
 ggplot(aes(x = (1 - specificity), y = sensitivity, group = model)) +
  geom_line(aes(color = model)) +
  geom_point(colour = &amp;#39;red&amp;#39;, size = 3) +
  transition_reveal(sensitivity) +
  geom_abline(linetype = 3) +
  theme_bw()

# Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition
a_gif &amp;lt;- animate(a, width = 440, height = 440)
b_gif &amp;lt;- animate(b, width = 440, height = 440)

a_mgif &amp;lt;- image_read(a_gif)
b_mgif &amp;lt;- image_read(b_gif)

new_gif &amp;lt;- image_append(c(a_mgif[1], b_mgif[1]))

for(i in 2:100){
 combined &amp;lt;- image_append(c(a_mgif[i], b_mgif[i]))
 new_gif &amp;lt;- c(new_gif, combined)
}

new_gif&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-06-12-roc-curves-an-animated-example_files/figure-html/unnamed-chunk-11-1.gif&#34; /&gt;&lt;!-- --&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[1] The package &lt;code&gt;yardstick&lt;/code&gt; as many other packages use the first level of the response variable factor as the “event”. Therefore, the probability output determines how likely an observation belongs to the &lt;em&gt;first level of the factor&lt;/em&gt; of the response variable. This behavior can be changed in the &lt;code&gt;yardstick&lt;/code&gt; package global options.&lt;/p&gt;
&lt;p&gt;[2] The default output has a logit scale and needs to be transformed first to a probability value. This can be done automatically if we specify in the function &lt;code&gt;predict&lt;/code&gt; the argument &lt;strong&gt;type&lt;/strong&gt; as &lt;em&gt;prob&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;[3] Apart from tree decisions, linear models can be also used in gradient boosting.&lt;/p&gt;
&lt;p&gt;[4] Because of this reason, ROC curves might not be appropriate to evaluate the performance of models on small test sets.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An introduction to uncertainty with Bayesian models</title>
      <link>/post/introduction-uncertainty-bayesian-models/</link>
      <pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate>
      <guid>/post/introduction-uncertainty-bayesian-models/</guid>
      <description>


&lt;div id=&#34;overview&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Overview&lt;/h1&gt;
&lt;p&gt;In this post, we will get a first approximation to the “uncertainty” concept. First, we will train two models: logistic regression and its “Bayesian version” and compare their performance. Furthermore, we will explore the advantage of using a Bayesian model when we want to estimate how likely is our prediction. Finally, we will briefly discuss why there are some predicted values more probable than others.&lt;/p&gt;
&lt;div id=&#34;get-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Get the data&lt;/h2&gt;
&lt;p&gt;First, we download this data from &lt;a href=&#34;https://www.kaggle.com/gilsousa/habermans-survival-data-set&#34;&gt;Kaggle&lt;/a&gt;. This dataset includes 306 patients from a study of patients that had undergone a surgical operation on breast cancer. The table consists of three explanatory variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Age of patient during surgical operation (&lt;code&gt;age&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Year when the operation was made (&lt;code&gt;operation_year&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Number of positive axillary nodes detected (&lt;code&gt;nodes&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Furthermore, there is a column (&lt;code&gt;survival&lt;/code&gt;) that indicates whether the patient survived at least 5 years after the operation.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork) # merge plots
library(ggridges) # ridges plot
library(glue) # paste plot labels
library(yardstick) # helper roc curves and auc
library(rstanarm) # bayesian model
library(bayestestR) # helper for the bayesian model
library(broom) # make tidy

# Source: https://www.kaggle.com/gilsousa/habermans-survival-data-set
haberman &amp;lt;- read_csv(&amp;#39;data/haberman.csv&amp;#39;, col_names = c(&amp;#39;age&amp;#39;, 
                            &amp;#39;operation_year&amp;#39;, 
                            &amp;#39;nodes&amp;#39;, 
                            &amp;#39;survival&amp;#39;))

haberman &amp;lt;- haberman %&amp;gt;%
 mutate(survival = factor(if_else(survival == 1, &amp;#39;Yes&amp;#39;, &amp;#39;No&amp;#39;))) %&amp;gt;%
 mutate(operation_year = factor(operation_year)) %&amp;gt;%
 mutate(id = as.character(row_number())) %&amp;gt;% 
 select(id, everything())&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;exploratory-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploratory analysis&lt;/h2&gt;
&lt;p&gt;Since the dataset has 3 explanatory variables, let’s plot the distribution of each one of them with the response variable &lt;em&gt;survival&lt;/em&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p1 &amp;lt;- haberman %&amp;gt;%
 ggplot(aes(age)) +
 geom_density(aes(fill = survival), color = &amp;#39;black&amp;#39;, alpha = 0.4) +
 theme_bw()

p2 &amp;lt;- haberman %&amp;gt;%
 ggplot(aes(nodes)) +
 geom_density(aes(fill = survival), color = &amp;#39;black&amp;#39;, alpha = 0.4) +
 theme_bw()

p3 &amp;lt;- haberman %&amp;gt;%
 group_by(operation_year, survival) %&amp;gt;%
 summarise(n = n()) %&amp;gt;%
 mutate(perc = 100*(n / sum(n))) %&amp;gt;%
 ggplot(aes(operation_year, perc)) +
 geom_col(aes(fill = survival), color = &amp;#39;black&amp;#39;) +
 theme_bw() +
 labs(y = &amp;#39;Percentage (%)&amp;#39;)


p1 + p2 + p3 + patchwork::plot_layout(nrow = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Age and number of nodes seem to have a reasonable distribution but surprisingly, patient survival does not increase along the operation year. In theory, the patient survival of most cancer types has increased dramatically over the years. Therefore, it seems reasonable to find a similar pattern in this dataset. The interval of time (1958-1969) seems long enough and happened during a period of major progress in clinical therapies.&lt;/p&gt;
&lt;p&gt;A plausible explanation is an underlying effect of, at least, one remaining variable. Let’s observe the distribution of the variable age of patient over the years:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;haberman %&amp;gt;%
 ggplot(aes(age, operation_year)) +
 geom_density_ridges(aes(fill = operation_year), show.legend = FALSE) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;haberman %&amp;gt;%
 ggplot(aes(operation_year, age)) +
 geom_boxplot(aes(fill = operation_year), show.legend = FALSE) +
 theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There seem to be differences over the years. In this post, further analysis to control for this effect is out of scope, but a more exhaustive analysis of this dataset should be aware of it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;classification&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Classification&lt;/h1&gt;
&lt;p&gt;Once we have explored quickly the dataset, we are going to train a model to try to predict whether the patient survived 5 years after the operation.&lt;/p&gt;
&lt;p&gt;To do so, we are going to test two different approaches:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Logistic regression&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Bayesian generalized linear models&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this blog post, we will skip aspects such as cross-validation, feature engineering, precision-recall curve, or unbalanced labels (there is).&lt;/p&gt;
&lt;div id=&#34;split-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Split data&lt;/h2&gt;
&lt;p&gt;First, we will split the available dataset &lt;code&gt;haberman&lt;/code&gt; into two sets, a training (70%) and a test (30%).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(991)

training_ids &amp;lt;- haberman %&amp;gt;% sample_frac(0.7) %&amp;gt;% pull(id)

hab_training &amp;lt;- haberman %&amp;gt;% 
 filter(id %in% training_ids) %&amp;gt;% 
 mutate(operation_year = as.integer(operation_year))

hab_test &amp;lt;- haberman %&amp;gt;% 
 filter(!id %in% training_ids) %&amp;gt;% 
 mutate(operation_year = as.integer(operation_year))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We will train independently both models with the training set and predict the labels of the response variable (“survive”, “no survive”) in the test dataset. These predicted labels will be useful to compare both models in terms of performance and further aspects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;training---logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training - logistic regression&lt;/h2&gt;
&lt;p&gt;In R, we just need to use the &lt;code&gt;glm&lt;/code&gt; function and specify the argument &lt;code&gt;family = binomial&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_model &amp;lt;- glm(survival ~ age + nodes + operation_year, family = &amp;#39;binomial&amp;#39;, 
           data = hab_training)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;training---bayesian-logistic-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Training - Bayesian logistic regression&lt;/h2&gt;
&lt;p&gt;Thanks to the package &lt;code&gt;rstanarm&lt;/code&gt; that provides an elegant interface to &lt;a href=&#34;https://mc-stan.org/&#34;&gt;stan&lt;/a&gt;, we can keep almost the same syntax used before. In this case, we use the function &lt;code&gt;stan_glm&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayesian_model &amp;lt;- rstanarm::stan_glm(survival ~ age + nodes + operation_year, 
                   family = &amp;#39;binomial&amp;#39;,
                   data = hab_training,
                   prior = normal())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 0.137 seconds (Warm-up)
## Chain 1:                0.146 seconds (Sampling)
## Chain 1:                0.283 seconds (Total)
## Chain 1: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 2).
## Chain 2: 
## Chain 2: Gradient evaluation took 0 seconds
## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 2: Adjust your expectations accordingly!
## Chain 2: 
## Chain 2: 
## Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 2: 
## Chain 2:  Elapsed Time: 0.132 seconds (Warm-up)
## Chain 2:                0.284 seconds (Sampling)
## Chain 2:                0.416 seconds (Total)
## Chain 2: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 3).
## Chain 3: 
## Chain 3: Gradient evaluation took 0 seconds
## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 3: Adjust your expectations accordingly!
## Chain 3: 
## Chain 3: 
## Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 3: 
## Chain 3:  Elapsed Time: 0.153 seconds (Warm-up)
## Chain 3:                0.157 seconds (Sampling)
## Chain 3:                0.31 seconds (Total)
## Chain 3: 
## 
## SAMPLING FOR MODEL &amp;#39;bernoulli&amp;#39; NOW (CHAIN 4).
## Chain 4: 
## Chain 4: Gradient evaluation took 0 seconds
## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
## Chain 4: Adjust your expectations accordingly!
## Chain 4: 
## Chain 4: 
## Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
## Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
## Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
## Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
## Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
## Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
## Chain 4: 
## Chain 4:  Elapsed Time: 0.144 seconds (Warm-up)
## Chain 4:                0.172 seconds (Sampling)
## Chain 4:                0.316 seconds (Total)
## Chain 4:&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;performance&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Performance&lt;/h2&gt;
&lt;p&gt;Once we trained both models, we are going to compare their performance with the test set (split at the beginning of the post). To that end, we calculate the ROC curve and the Area Under the Curve (AUC) of each model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_logistic &amp;lt;- predict(logistic_model, hab_test, type = &amp;#39;response&amp;#39;)
pred_bayesian &amp;lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %&amp;gt;% 
 as_tibble() %&amp;gt;%
 map_dbl(~ map_estimate(.x) )

check_pred &amp;lt;- hab_test %&amp;gt;%
 select(id) %&amp;gt;%
 mutate(pred_surv_no_log = pred_logistic,
     pred_surv_no_bay = pred_bayesian) %&amp;gt;%
 left_join(haberman %&amp;gt;% select(id, survival), by = &amp;#39;id&amp;#39;)


roc_logistic &amp;lt;- check_pred %&amp;gt;% roc_curve(survival, pred_surv_no_log) %&amp;gt;% mutate(model = &amp;#39;logistic&amp;#39;)
roc_bayesian &amp;lt;- check_pred %&amp;gt;% roc_curve(survival, pred_surv_no_bay) %&amp;gt;% mutate(model = &amp;#39;bayesian&amp;#39;)

auc_logistic &amp;lt;- check_pred %&amp;gt;% roc_auc(survival, pred_surv_no_log) %&amp;gt;% pull(.estimate) %&amp;gt;% round(3)
auc_bayesian &amp;lt;- check_pred %&amp;gt;% roc_auc(survival, pred_surv_no_bay) %&amp;gt;% pull(.estimate) %&amp;gt;% round(3)

roc_both &amp;lt;- roc_logistic %&amp;gt;% bind_rows(roc_bayesian)


roc_both %&amp;gt;%
 ggplot(aes((1-specificity), sensitivity)) +
  geom_line(aes(color = model), size = 1) +
  theme_bw() +
 geom_abline(linetype = 3) +
  labs(title = &amp;#39;Comparison performance logistic and Bayesian model&amp;#39;,
     subtitle = glue(&amp;#39;AUC (logistic) = {auc_logistic} - AUC (Bayesian) = {auc_bayesian}&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both models demonstrate similar performance. If we would have to decide, at this step of the analysis, one of them (logistic or Bayesian), there would not be a reason to choose one or the other. Probably, the logistic one, since it may sounds more familiar. But this might change when the &lt;strong&gt;uncertainty idea&lt;/strong&gt; comes up!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;uncertainty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uncertainty&lt;/h2&gt;
&lt;p&gt;First, we are going to explore the outcomes of the test set provided by the &lt;em&gt;logistic model&lt;/em&gt;. These values represent the probability [2] of each instance of being labeled as “No survive” five years after the operation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_logistic &amp;lt;- predict(logistic_model, hab_test, type = &amp;#39;response&amp;#39;)

p1 &amp;lt;- pred_logistic %&amp;gt;%
 enframe() %&amp;gt;%
 ggplot(aes(value)) +
 geom_density(fill = &amp;#39;steelblue&amp;#39;, alpha = 0.5) +
 theme_bw() +
 labs(x = &amp;#39;Probability&amp;#39;, y = &amp;#39;Density&amp;#39;)


p2 &amp;lt;- pred_logistic %&amp;gt;%
 enframe() %&amp;gt;%
 ggplot(aes(value)) +
 geom_histogram(fill = &amp;#39;yellow&amp;#39;, alpha = 0.5, color = &amp;#39;black&amp;#39;, binwidth = 0.05) +
 theme_bw() +
 labs(x = &amp;#39;Probability&amp;#39;, y = &amp;#39;Density&amp;#39;)

p1 + p2 + plot_layout(ncol = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Each probability value represents a single observation. To convert the predicted probability to labels, the user needs to specify a threshold where every value above the threshold is defined as “No survive”, otherwise “survive”. Most of the cases, this creates problematic scenarios where two observations can be equally labeled in spite of having distinct probabilities (e.g. 0.6 and 0.95).&lt;/p&gt;
&lt;p&gt;By contrast, for each one of observations in the test set, the &lt;strong&gt;Bayesian model&lt;/strong&gt; does not provide a single probability value but a posterior distribution. We can represent the posterior distributions from the 92 observations (test set) with a boxplot, for instance:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_uncertainty &amp;lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %&amp;gt;% 
 as_tibble() %&amp;gt;%
 pivot_longer(everything(), names_to = &amp;#39;rank_obs&amp;#39;, values_to = &amp;#39;pred_surv_n_bay&amp;#39;)

plot_uncertainty %&amp;gt;%
 ggplot(aes(rank_obs, pred_surv_n_bay)) +
 geom_boxplot() +
 theme_bw() +
 labs(x = &amp;#39;Test set - Observation&amp;#39;, y = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Another way to check the dispersion of the predicted outcome is with a ridge plot, that is especially useful when the number of samples is low. So, let’s pick only two observations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_uncertainty %&amp;gt;%
 # if you want to reproduce this code, just change the character &amp;#39;1&amp;#39; or &amp;#39;5&amp;#39; for any other.
 filter(rank_obs %in% c(&amp;#39;1&amp;#39;, &amp;#39;5&amp;#39;)) %&amp;gt;%
 ggplot(aes(pred_surv_n_bay, rank_obs)) +
  geom_density_ridges(aes(fill = rank_obs), alpha = 0.6) +
  theme_bw() +
  labs(y = &amp;#39;Test set - Observation&amp;#39;, x = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Picking joint bandwidth of 0.0101&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we observe that both distributions have a “peak” (known as MAP [1]) above 0.5, therefore the predicted label would be, in both cases, ‘no survive’. But, are these two predictions equally certain? Well, we notice, at least, two things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Both MAPs have different values (sample 1 - 0.9, sample 5 - 7.2).&lt;/li&gt;
&lt;li&gt;Observation number 5 has a flatter curve in comparison with the number 1.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In both cases, observation 5 reflects a higher uncertainty regarding its predicted label in comparison with observation 1. Should we make the same clinical decisions in both cases? Would this information be valuable in a clinical environment…? Probably yes, but first, we should find a way to measure it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;measuring-uncertainty-3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Measuring uncertainty [3]&lt;/h2&gt;
&lt;p&gt;A handy option is to use the standard deviation (sd) of the distribution, so we can estimate one value for each observation. With this in mind, we can plot the distribution of the sd from the 92 observations of the test dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;std_dev_tbl &amp;lt;- plot_uncertainty %&amp;gt;%
 group_by(rank_obs) %&amp;gt;%
 summarise(std_dev = sd(pred_surv_n_bay)) %&amp;gt;%
 ungroup()


std_dev_tbl %&amp;gt;% 
 ggplot(aes(std_dev)) +
 geom_density(fill = &amp;#39;steelblue&amp;#39;, alpha = 0.5) +
 theme_bw() +
 labs(x = &amp;#39;Standard deviation (sd)&amp;#39;,
    y = &amp;#39;Density&amp;#39;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Most of observations have a standard deviation of around 0.04. There are a few extreme values in the interval 0.08-0.10. In short, this plot shows that there are some observations whose sd is twice as high as others.&lt;/p&gt;
&lt;p&gt;We can filter and select observations based on the dispersion of its posterior distribution. For instance, we can split the test set of 92 observations in percentiles using the sd and plot the 1st (lowest sd) and 10th percentile (highest sd). In this way, it allows us to compare those observations with the highest and lowest standard deviation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;top_sd &amp;lt;- std_dev_tbl %&amp;gt;% 
 mutate(tile = ntile(std_dev, 10)) %&amp;gt;%
 filter(tile == 1 | tile == 10)

plot_uncertainty %&amp;gt;%
 # left_join(std_dev, by = &amp;#39;rank_obs&amp;#39;) %&amp;gt;%
 filter(rank_obs %in% top_sd$rank_obs) %&amp;gt;%
 ggplot(aes(pred_surv_n_bay, rank_obs)) +
 geom_density_ridges(aes(fill = rank_obs), alpha = 0.6, show.legend = FALSE) +
 theme_bw() +
 labs(y = &amp;#39;Test set - Observation&amp;#39;, x = &amp;#39;Probability (survival == &amp;quot;No survive&amp;quot;)&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above plot, we easily identify to which group each observation belongs to. Independently of the predicted labels, should their predictions be considered equally likely? If the final user of the model just receives a categorical outcome, he/she is definitely skipping some valuable information since some predictions look more unlikely than others. As an alternative, predictions could be grouped into categories and neglect those with a high dispersion or make it clear than further support should be required.&lt;/p&gt;
&lt;p&gt;In this post, we have measured the uncertainty of observations and identifying those samples with high uncertainty. But, we have not talked yet about what is the origin of it.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;why-some-predictions-are-more-unlikely-than-others&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Why some predictions are more unlikely than others?&lt;/h2&gt;
&lt;p&gt;In other words, why our model has more doubts about a sample than others? I find two possible explanations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The sample is mislabeled.&lt;/li&gt;
&lt;li&gt;Group variability.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first one is difficult to address but we can explore the group variability. Since we have three continuous explanatory variable, we can easily do a PCA with the function &lt;code&gt;prcomp&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl &amp;lt;- hab_test %&amp;gt;%
 # take only numeric columns
 select_if(is.numeric) %&amp;gt;%
 # Important - we need to scale and center each variable before PCA
 prcomp(scale = TRUE, center = TRUE) %&amp;gt;%
 tidy() %&amp;gt;%
 mutate(row = as.character(row)) %&amp;gt;%
 pivot_wider(id_cols = row, values_from = value, names_from = PC, names_prefix = &amp;#39;PC&amp;#39;) %&amp;gt;%
 left_join(top_sd %&amp;gt;% select(-std_dev), by = c(&amp;#39;row&amp;#39; = &amp;#39;rank_obs&amp;#39;)) %&amp;gt;%
 mutate(tile = ifelse(is.na(tile), &amp;#39;ok&amp;#39;, tile)) %&amp;gt;%
 left_join(hab_test %&amp;gt;% select(survival) %&amp;gt;% mutate(row = as.character(row_number())), 
      by = &amp;#39;row&amp;#39;)


pca_tbl %&amp;gt;%
 ggplot(aes(PC1, PC2)) +
  geom_point() +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As we have observations in two categories, let’s split them into two plots:&lt;/p&gt;
&lt;p&gt;Since we have observations defined into two categories (survival = Yes, survival = No), let’s lay out the plot into two different:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl %&amp;gt;%
 ggplot(aes(PC1, PC2)) +
  geom_point() +
  theme_bw() +
  facet_grid(~ survival)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, we are going to highlight those observations that belong to the highest and lowest sd groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pca_tbl %&amp;gt;%
 mutate(tile = case_when(
  tile == &amp;#39;1&amp;#39; ~ &amp;#39;lowest sd&amp;#39;,
  tile == &amp;#39;10&amp;#39; ~ &amp;#39;highest sd&amp;#39;,
  tile == &amp;#39;ok&amp;#39; ~ &amp;#39;ok&amp;#39;
 )) %&amp;gt;%
 ggplot(aes(PC1, PC2)) +
  geom_point(aes(fill = tile), color = &amp;#39;black&amp;#39;, shape = 21, size = 2) +
  theme_bw() +
  facet_grid(~ survival) +
  labs(fill = &amp;#39;Category&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-05-29-uncertainty-bayesian-models_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;On the one hand, “lowest sd” group observations are centrally located in the plot. This reflects a tendency of these samples to have similar features values with observations belonging to their own label. On the other hand, “highest sd” group points tend to be dispersed from the rest, all over the components. It makes sense since the uncertainty to predict these points come from the fact that their own feature values are different from points on the same category.&lt;/p&gt;
&lt;p&gt;Surprisingly, there is a red point on the left panel whose location is centric respect to the rest of the values. This perhaps arises the disadvantage of reducing a probability distribution to a point-estimate (standard deviation). The dispersion estimation might have not be accurate enough and further ways of measuring might be needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As humans, we make decisions based on uncertainty, even though we are not aware of it. If the weather forecasters show 10% of raining on the weekend, we will probably make a plan to go to the mountain. With 90% we may rethink about it…When we are talking with someone about a delicate topic, we pick the words based on the uncertainty of his/her predicted response: words with a broad meaning and therefore ambiguous might not be chosen, due to the high uncertainty. Therefore, if we constantly map our reality and &lt;em&gt;act&lt;/em&gt; through the constant evaluation of uncertainty, &lt;em&gt;why should we believe in predictions from machines without a shadow of doubt?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;My personal view is that the measurement of uncertainty will end up being essential. Especially, for every decision process supported by a machine in a clinical environment. In that way, I find &lt;em&gt;Bayesian models&lt;/em&gt; a nice fit for many of the challenges of tomorrow.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[1] To calculate the roc curves of the Bayesian model’s predictions, a single probability value for each observation is required. There are multiple ways to estimate it, such as mean, median, and MAP (Highest Maximum A Posteriori). In this case, I chose the latest because it provided the highest performance.&lt;/p&gt;
&lt;p&gt;[2] The function &lt;code&gt;predict&lt;/code&gt; retrieves outcomes as probabilities because we specified &lt;code&gt;type = response&lt;/code&gt; as argument. Otherwise, the default output would be as logit.&lt;/p&gt;
&lt;p&gt;[3] I am interested to know more ways to estimate the “uncertainty” of a prediction. Please if you have any reference or idea, let me know! ;P&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Can we predict cases of dengue with climate variables?</title>
      <link>/post/can-we-predict-cases-of-dengue-with-climate-variables/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0000</pubDate>
      <guid>/post/can-we-predict-cases-of-dengue-with-climate-variables/</guid>
      <description>


&lt;p&gt;Recently, I discovered a new website about competitions that it is not called Kaggle! Its name is Drivendata.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DrivenData&lt;/strong&gt; offers different competitions related with multiple types of field, such as health (oh yes!), ecology, society… with a common element: to face the world’s biggest social challenges.&lt;/p&gt;
&lt;p&gt;I decided to join my first competition called &lt;em&gt;‘DengAI: Predicting Disease Spread‘&lt;/em&gt;. In this case, the user receives a set of weather information (temperatures, precipitations, vegetations) from two cities: &lt;strong&gt;San Juan&lt;/strong&gt; (Puerto Rico) and &lt;strong&gt;Iquitos&lt;/strong&gt; (Peru) with total cases of dengue by year and week of every year.&lt;/p&gt;
&lt;p&gt;The goal of the competition is to develop a prediction model that would be able to anticipate the cases of dengue in every city depending on a set of climate variables.&lt;/p&gt;
&lt;p&gt;The DrivenData’s blog wrote some days ago, a post about a fast approach with this dataset. It was written in Python. So, I decided to “translate” to R language.&lt;/p&gt;
&lt;p&gt;The next code is divided into three main points:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1.&lt;/strong&gt; Code with clean tasks (transform NA values, remove of columns…) and exploratory analyses.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2.&lt;/strong&gt; Function with every step during cleaning of data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3.&lt;/strong&gt; Development of model, prediction and comparison of predicted vs real total cases detected.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load of libraries

library(tidyverse)
library(zoo)
library(corrplot)
library(MASS)
library(reshape2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load data
train_features &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_features_train.csv&amp;#39;)
  
train_labels &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_labels_train.csv&amp;#39;)

test_features &amp;lt;- read.csv(&amp;#39;data/dengue_post/dengue_features_test.csv&amp;#39;)

submission_format &amp;lt;- read.csv(&amp;#39;data/dengue_post/submission_format.csv&amp;#39;)
  
# Filter of data by city  

sj_train_labels &amp;lt;- filter(train_labels, city == &amp;#39;sj&amp;#39;)
sj_train_features &amp;lt;- filter(train_features, city == &amp;#39;sj&amp;#39;)

iq_train_labels &amp;lt;- filter(train_labels, city == &amp;#39;iq&amp;#39;)
iq_train_features &amp;lt;- filter(train_features, city == &amp;#39;iq&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Is there NA values?

df_na_sj &amp;lt;- as.data.frame(apply(sj_train_features,2, function(x) any(is.na(x))))
colnames(df_na_sj) &amp;lt;- &amp;#39;is_there_NA&amp;#39;
df_na_sj$number_NA &amp;lt;- apply(sj_train_features,2, function(x) sum(is.na(x)))
df_na_sj$mean_NA &amp;lt;- apply(sj_train_features, 2, function(x) mean(is.na(x)))

df_na_iq &amp;lt;- as.data.frame(apply(iq_train_features, 2, function(x) any(is.na(x))))
colnames(df_na_iq) &amp;lt;- &amp;#39;is_there_NA&amp;#39;
df_na_iq$number_NA &amp;lt;- apply(iq_train_features, 2, function(x) sum(is.na(x)))
df_na_iq$mean_NA &amp;lt;- apply(iq_train_features, 2, function(x) mean(is.na(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Vegetation Index over Time Plot with NAs

ggplot(sj_train_features, aes(x = as.Date(week_start_date), y = ndvi_ne )) +
  ggtitle(&amp;#39;Vegetation Index over Time&amp;#39;) +
  theme_bw() +
  xlab(&amp;#39;Title&amp;#39;) +
  geom_line(na.rm = FALSE, color = &amp;#39;blue&amp;#39;) +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Remove &amp;#39;weekofyear&amp;#39; column

sj_train_features &amp;lt;- dplyr::select(sj_train_features, -week_start_date)

iq_train_features &amp;lt;- dplyr::select(iq_train_features, -week_start_date)

# Fill the NA values with the previous value

sj_train_features &amp;lt;- sj_train_features %&amp;gt;%
            do(na.locf(.))

iq_train_features &amp;lt;- iq_train_features %&amp;gt;%
            do(na.locf(.))

# Distribution of labels

# print(mean(sj_train_labels$total_cases))
# print(var(sj_train_labels$total_cases))
# 
# print(mean(iq_train_labels$total_cases))
# print(var(iq_train_labels$total_cases))


ggplot(sj_train_labels, aes(x = total_cases)) +
  theme_bw() +
  ggtitle(&amp;#39;Cases of dengue in San Juan&amp;#39;) +
  geom_histogram() +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(iq_train_labels, aes(x = total_cases)) +
  theme_bw() +
  ggtitle(&amp;#39;Cases of dengue in Iquitos&amp;#39;) +
  geom_histogram() +
  theme(plot.title = element_text(hjust = 0.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Add total_cases column to *_train_features dataframes


# sj_train_features &amp;lt;- left_join(sj_train_features, sj_train_labels, by = c(&amp;#39;city&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;weekofyear&amp;#39;))
 sj_train_features$total_cases &amp;lt;- sj_train_labels$total_cases

# iq_train_features &amp;lt;- left_join(iq_train_features, iq_train_labels, by = c(&amp;#39;city&amp;#39;, &amp;#39;year&amp;#39;, &amp;#39;weekofyear&amp;#39;))
iq_train_features$total_cases &amp;lt;- iq_train_labels$total_cases

# Correlation matrix

m_sj_train_features &amp;lt;- data.matrix(sj_train_features)
m_sj_train_features &amp;lt;- cor(x = m_sj_train_features[,3:24], use = &amp;#39;complete.obs&amp;#39;, method = &amp;#39;pearson&amp;#39;)

m_iq_train_features &amp;lt;- data.matrix(iq_train_features)
m_iq_train_features &amp;lt;- cor(x = m_iq_train_features[,3:24], use = &amp;#39;everything&amp;#39;, method = &amp;#39;pearson&amp;#39;)

# Correlation Heatmap

corrplot(m_sj_train_features, type = &amp;#39;full&amp;#39;, tl.col = &amp;#39;black&amp;#39;, method=&amp;quot;shade&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-3.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrplot(m_iq_train_features, type = &amp;#39;full&amp;#39;, tl.col = &amp;#39;black&amp;#39;, method = &amp;#39;shade&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-4.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Correlation Bar plot

df_sj_train_features &amp;lt;- data.frame(m_sj_train_features)[2:21,] 
df_sj_train_features &amp;lt;- dplyr::select(df_sj_train_features, total_cases) 
                                    
df_iq_train_features &amp;lt;- data.frame(m_iq_train_features)[2:21,]
df_iq_train_features &amp;lt;- dplyr::select(df_iq_train_features, total_cases) 

ggplot(df_sj_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;) +
  theme_bw() +
  ggtitle(&amp;#39;Correlation of variables in San Juan&amp;#39;) +
  ylab(&amp;#39;Correlation&amp;#39;) +
  xlab(&amp;#39;Variables&amp;#39;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-5.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(df_iq_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) +
  geom_bar(stat = &amp;#39;identity&amp;#39;) +
  theme_bw() +
  ggtitle(&amp;#39;Correlation of variables in Iquitos&amp;#39;) +
  ylab(&amp;#39;Correlation&amp;#39;) +
  xlab(&amp;#39;Variables&amp;#39;) +
  coord_flip()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-4-6.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function data cleaning

data_clean &amp;lt;- function(df_dengue_features, df_dengue_labels = NULL, add_cases = TRUE) {
  
  # Filter by city
  sj_df_dengue_features &amp;lt;- filter(df_dengue_features, city == &amp;#39;sj&amp;#39;)
  iq_df_dengue_features &amp;lt;- filter(df_dengue_features, city == &amp;#39;iq&amp;#39;)
  
  if (add_cases == TRUE) {
  sj_df_dengue_labels &amp;lt;- filter(df_dengue_labels, city == &amp;#39;sj&amp;#39;)
  iq_df_dengue_labels &amp;lt;- filter(df_dengue_labels, city == &amp;#39;iq&amp;#39;)
  }
  # Removing week_start_date column
  sj_df_dengue_features &amp;lt;- dplyr::select(sj_df_dengue_features, -week_start_date)
  iq_df_dengue_features &amp;lt;- dplyr::select(iq_df_dengue_features, -week_start_date)

  # Fill of NA values with the previous value
  sj_df_dengue_features &amp;lt;- sj_df_dengue_features %&amp;gt;%
    do(na.locf(.))
  
  iq_df_dengue_features &amp;lt;- iq_df_dengue_features %&amp;gt;%
    do(na.locf(.))
  
  # Add total_cases to dataframe with features
  if (add_cases == TRUE) {
  sj_df_dengue_features$total_cases &amp;lt;- sj_df_dengue_labels$total_cases
  iq_df_dengue_features$total_cases &amp;lt;- iq_df_dengue_labels$total_cases
  }
  
  # Converting character columns into numbers
  sj_df_dengue_features &amp;lt;- as.data.frame(apply(sj_df_dengue_features,2,as.numeric))
  sj_df_dengue_features$city &amp;lt;- rep(&amp;#39;sj&amp;#39;, nrow(sj_df_dengue_features))
  iq_df_dengue_features &amp;lt;- as.data.frame(apply(iq_df_dengue_features,2,as.numeric))
  iq_df_dengue_features$city &amp;lt;- rep(&amp;#39;iq&amp;#39;, nrow(iq_df_dengue_features))
  
  result &amp;lt;- list(sj_df_dengue_features, iq_df_dengue_features )
  
  return(result)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Getting data_training clean

data_train &amp;lt;- data_clean(train_features, train_labels, TRUE)

# Getting negative binomials models by city

training_sj &amp;lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +
                     reanalysis_dew_point_temp_k +
                     station_min_temp_c +
                     station_avg_temp_c, data = data_train[[1]])

training_iq &amp;lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg +
                        reanalysis_dew_point_temp_k +
                        station_min_temp_c +
                        station_avg_temp_c, data = data_train[[2]])

# Getting data_test clean

data_test &amp;lt;- data_clean(test_features, add_cases = FALSE)


# Testing model with training data

prediction_train_sj &amp;lt;-  predict(training_sj, data_train[[1]], type = &amp;#39;response&amp;#39;)
prediction_train_iq &amp;lt;-  predict(training_iq, data_train[[2]], type = &amp;#39;response&amp;#39;)

df_prediction_train_sj &amp;lt;- data.frame(&amp;#39;prediction&amp;#39; = prediction_train_sj, &amp;#39;actual&amp;#39; = data_train[[1]]$total_cases,
                                     &amp;#39;time&amp;#39; = as.Date(train_features$week_start_date[1:936]))
df_prediction_train_sj &amp;lt;- melt(df_prediction_train_sj, id.vars = &amp;#39;time&amp;#39;)
ggplot(df_prediction_train_sj, aes(x = time, y = value, color = variable)) +
  geom_line() +
  ggtitle(&amp;#39;Dengue predicted Cases vs. Actual Cases (City-San Juan) &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df_prediction_train_iq &amp;lt;- data.frame(&amp;#39;prediction&amp;#39; = prediction_train_iq, &amp;#39;actual&amp;#39; = data_train[[2]]$total_cases,
                                     &amp;#39;time&amp;#39; = as.Date(train_features$week_start_date[937:1456]))
df_prediction_train_iq &amp;lt;- melt(df_prediction_train_iq, id.vars = &amp;#39;time&amp;#39;)
ggplot(df_prediction_train_iq, aes(x = time, y = value, color = variable)) +
  geom_line() +
  ggtitle(&amp;#39;Dengue predicted Cases vs. Actual Cases (City-Iquitos) &amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-12-09-can-we-predict-cases-of-dengue-with-climate-variables_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Prediction of total_cases in the data set

prediction_sj &amp;lt;-  predict(training_sj, data_test[[1]], type = &amp;#39;response&amp;#39;)
prediction_iq &amp;lt;-  predict(training_iq, data_test[[2]], type = &amp;#39;response&amp;#39;)
 
data_prediction_sj &amp;lt;- data.frame(&amp;#39;city&amp;#39; = rep(&amp;#39;sj&amp;#39;, length(prediction_sj) ), 
                                 &amp;#39;total_cases&amp;#39; = prediction_sj, 
                                 &amp;#39;weekofyear&amp;#39; = data_test[[1]]$weekofyear,
                                 &amp;#39;year&amp;#39; = data_test[[1]]$year )

data_prediction_iq &amp;lt;- data.frame(&amp;#39;city&amp;#39; = rep(&amp;#39;iq&amp;#39;, length(prediction_iq) ), 
                                 &amp;#39;total_cases&amp;#39; = prediction_iq,
                                 &amp;#39;weekofyear&amp;#39; = data_test[[2]]$weekofyear,
                                 &amp;#39;year&amp;#39; = data_test[[2]]$year)


  
submission_format$total_cases &amp;lt;- as.numeric(c(data_prediction_sj$total_cases, 
                                                   data_prediction_iq$total_cases))

submission_format$total_cases &amp;lt;- round(submission_format$total_cases, 0)
  
write.csv(submission_format,
          file = &amp;#39;submission_format_submit.csv&amp;#39;, row.names = F)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
