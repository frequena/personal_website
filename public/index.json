[{"authors":["admin"],"categories":null,"content":"I did my PhD at the Clinical Bioinformatics lab in the Imagine Institute (Paris). My current work focuses on the development of computational methods, including machine-learning, for the clinical interpretation of variants in rare disease patients.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://franciscorequena.com/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I did my PhD at the Clinical Bioinformatics lab in the Imagine Institute (Paris). My current work focuses on the development of computational methods, including machine-learning, for the clinical interpretation of variants in rare disease patients.","tags":null,"title":"Francisco Requena","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://franciscorequena.com/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://franciscorequena.com/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://franciscorequena.com/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":["drug-discovery"],"content":"For children with a rare disease, an accurate diagnosis is crucial to provide advice, possible therapies and assess the potential risk for family members in future generations. Public initiatives such as the International Rare Diseases Research Consortium (IRDiRC) set the goal for 2017-2027 to \u0026ldquo;enable all people living with a rare disease to receive an accurate diagnosis, care, and available therapy soon after seeking medical care\u0026rdquo; (1).\nDespite significant efforts in the field of drug development, the reality is discouraging. Drug candidates that move from phase 1 clinical trials to approval and launch remain at around 10%. This rate is even lower in some specific fields, such as oncology (3.4%).\nAt the same time, large-scale biomedical databases, such as gnomAD and the UK Biobank, offer the possibility to assess the effect of variants in humans and their association with multiple conditions. In addition, the recruitment of population-based cohorts considered disease-agnostic increases the number of hypotheses that can be tested. In particular, the high scientific value of the UK Biobank is worth mentioning, as it not only releases genomic data but also rich phenotype characterization alongside other data sources (273).\nBoth mutations and drugs share the same mechanism: they disrupt the normal functioning of the human body. How they do so may differ, but it seems plausible to hypothesize that, in some cases, the phenotypic consequences of a loss-of-function SNVs or deletions are similar to the pharmacological effect of an inhibitor drug.\nThere is no doubt that this view poses some problems. As mentioned above, the heterogeneity and particularities in genetics are enormous even for monogenic diseases. Elements such as the genotype of the variant or discrepancies between the predicted effect and the actual consequences can make the use of this approach challenging. For instance, it is already known that LoF mutations may not decrease protein or even mRNA levels.\nIn spite of these drawbacks, human genetics seems to be a great tool for the identification of drugs with therapeutic effects.\nThere is evidence to support this assumption. For instance, gastrointestinal adverse events observed in clinical trials of DGAT1 inhibitors could have been predicted based on the causal relationship between rare and highly penetrant variants of DGAT1 and congenital diarrheal disorder (3).\nAnother well-known example is the association of heterozygous gain-of-function mutations in the PCSK9 gene and familial hypercholesterolemia, which leads to heart attacks or strokes relatively early in life. Strikingly, LoF variants in the PCSK9 gene have been causally associated with low levels of low-density lipoprotein cholesterol (4).\nThis human genetic evidence contributed to the technical and regulatory success of PCSK9 inhibitors, leading to the launch of evolocumab (Amgen) and alirocumab (Regeneron), and has also shown value in patient stratification in clinical trials (5).\nOther drug candidates with strong genetic evidence between disease phenotypes and functional genetic variants have been identified, such as HSD17B13 for chronic liver disease (6), TYK2 for multiple autoimmune disorders (7), NRXN1 for neuropsychiatric disease (8), ASGR1 for cardiovascular disease (9),\nThese are not isolated examples, but a general trend. Nelson et al. found that pairing genetic target indication with genetic evidence almost doubles the success rate in clinical development. These analyses were re-evaluated three years later by other researchers with data after the original publication and controlling for potential confounding factors and corroborated the same claims (10).\nHarnessing human genomic data can improve the drug discovery process, from target selection to reducing failures due to lack of efficacy or adverse effects.\nReferences  .Sanders AD, Falconer E, Hills M, Spierings DCJ, Lansdorp PM. Single-cell template strand sequencing by Strand-seq enables the characterization of individual homologs. Nat Protoc. 2017;12: 1151\u0026ndash;1176. Szustakowski JD, Balasubramanian S, Kvikstad E, Khalid S, Bronson PG, Sasson A, et al. Advancing human genetics research and drug discovery through exome sequencing of the UK Biobank. Nat Genet. 2021;53. doi:10.1038/s41588-021-00885-0 Haas JT, Winter HS, Lim E, Kirby A, Blumenstiel B, DeFelice M, et al. DGAT1 mutation is linked to a congenital diarrheal disorder. J Clin Invest. 2012;122: 4680\u0026ndash;4684. Cohen JC, Boerwinkle E, Mosley TH Jr, Hobbs HH. Sequence variations in PCSK9, low LDL, and protection against coronary heart disease. N Engl J Med. 2006;354: 1264\u0026ndash;1272. Sabatine MS, Giugliano RP, Keech AC, Honarpour N, Wiviott SD, Murphy SA, et al. Evolocumab and Clinical Outcomes in Patients with Cardiovascular Disease. N Engl J Med. 2017;376: 1713\u0026ndash;1722. Abul-Husn NS, Cheng X, Li AH, Xin Y, Schurmann C, Stevis P, et al. A Protein-Truncating HSD17B13 Variant and Protection from Chronic Liver Disease. N Engl J Med. 2018;378: 1096\u0026ndash;1106. Dendrou CA, Cortes A, Shipman L, Evans HG, Attfield KE, Jostins L, et al. Resolving TYK2 locus genotype-to-phenotype differences in autoimmunity. Sci Transl Med. 2016;8: 363ra149. Noh HJ, Tang R, Flannick J, O\u0026rsquo;Dushlaine C, Swofford R, Howrigan D, et al. Integrating evolutionary and regulatory information with a multispecies approach implicates genes and pathways in obsessive-compulsive disorder. Nat Commun. 2017;8: 774. Nioi P, Sigurdsson A, Thorleifsson G, Helgason H, Agustsdottir AB, Norddahl GL, et al. Variant ASGR1 Associated with a Reduced Risk of Coronary Artery Disease. N Engl J Med. 2016;374: 2131\u0026ndash;2141. King EA, Davis JW, Degner JF. Are drug targets with genetic support twice as likely to be approved? Revised estimates of the impact of genetic support for drug mechanisms on the probability of drug approval. PLoS Genet. 2019;15: e1008489.  ","date":1654905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657537660,"objectID":"a3fd1e17c839bb6e12c5c3ade1855c49","permalink":"https://franciscorequena.com/post/2022-07-11-human-genetics-as-a-tool-for-drug-discovery/","publishdate":"2022-06-11T00:00:00Z","relpermalink":"/post/2022-07-11-human-genetics-as-a-tool-for-drug-discovery/","section":"post","summary":"For children with a rare disease, an accurate diagnosis is crucial to provide advice, possible therapies and assess the potential risk for family members in future generations. Public initiatives such as the International Rare Diseases Research Consortium (IRDiRC) set the goal for 2017-2027 to \u0026ldquo;enable all people living with a rare disease to receive an accurate diagnosis, care, and available therapy soon after seeking medical care\u0026rdquo; (1).","tags":[],"title":"Human genetics as a tool for drug discovery","type":"post"},{"authors":[],"categories":null,"content":"  In the biomedical literature, it is common to find sentences like:\n“Besides, the gene [gene symbol] has been associated with [type of cancer(s)] [References]”\nThe structure of these sentences can change from article to article, but the underlying idea and goal are the same. I will try to summarise it in the following sentence:\n“Hello reader/editor/reviewer, I was studying [any field], and I found this gene. I think it is a relevant/remarkable finding because it has been associated with cancer [references]. Therefore, it supports my hypothesis about the biological relevance of the gene in my field. Please, publish it.”\nThis approach is valid and logical as long as the association gene \u0026lt;-\u0026gt; cancer has been well-described and validated by different experiments and research teams. Unfortunately, some of these associations will be just spurious and no well-supported.\nTo explore this problem, we will count the number of articles in PubMed associating cancer with each one of the 19,205 protein-coding genes in the human genome.\nTo do so, we will write a simple code in R that will make a query for each gene to PubMed using the fantastic rentrez package.\nThe script has two simple steps:\n Download the official list of protein-coding gene symbols from HUGO. For each gene, make the following query “gene_symbol[Title/Abstract] AND cancer[Title/Abstract]” in PubMed (1-2).  You can find the code below:\nlibrary(tidyverse) library(scales) library(rentrez) gene_symbols \u0026lt;- read_tsv(\u0026#39;http://ftp.ebi.ac.uk/pub/databases/genenames/hgnc/tsv/locus_types/gene_with_protein_product.txt\u0026#39;) %\u0026gt;% pull(symbol) # Careful: it takes long to make all the queries query_pubmed \u0026lt;- function(input_gene) { print(input_gene) Sys.sleep(0.3) query_tmp \u0026lt;- entrez_search(db =\u0026quot;pubmed\u0026quot;, term = paste(paste0(input_gene, \u0026#39;[Title/Abstract]\u0026#39;),\u0026#39; AND \u0026#39;, \u0026#39;cancer[Title/Abstract]\u0026#39;), retmax = 600) tibble(\u0026#39;gene\u0026#39; = input_gene, \u0026#39;n_hits\u0026#39; = length(query_tmp[[\u0026#39;ids\u0026#39;]])) } result_genes \u0026lt;- gene_symbols %\u0026gt;% map_dfr(~ query_pubmed(.x)) result_genes %\u0026gt;% ggplot(aes(n_hits)) + geom_histogram(binwidth = 5) + theme_minimal() + labs(x = \u0026#39;Nº articles\u0026#39;, y = \u0026#39;Nº genes\u0026#39;) result_genes %\u0026gt;% mutate(category = case_when( n_hits == 0 ~ \u0026#39;0 articles\u0026#39;, n_hits \u0026gt;= 1 \u0026amp; n_hits \u0026lt;= 5 ~ \u0026#39;1-5 articles\u0026#39;, TRUE ~ \u0026#39;\u0026gt;5 articles\u0026#39; )) %\u0026gt;% count(category) %\u0026gt;% mutate(perc = n / sum(n)) %\u0026gt;% ggplot(aes(reorder(category,perc), perc)) + geom_col(aes(fill = category), color = \u0026#39;black\u0026#39;) + scale_y_continuous(label = percent, limits = c(0, 1)) + geom_label(aes(label = paste0(round(perc, 2)*100, \u0026#39;%\u0026#39;))) + labs(fill = \u0026#39;Category\u0026#39;, x = \u0026#39;Category\u0026#39;, y = \u0026#39;Percentage\u0026#39;) + theme_minimal() As you can see in the plot, 41% of the genes have been associated with cancer in more than five articles, 36% in 1-5 articles, and only 23% of the genes with no publications.\nIf I choose a random protein-coding from the human genome and do a query in PubMed, it is more likely (77%) to find at least one article than none.\nThis data reflects how easy it is to find articles associating cancer with most of the genes. Therefore, when a reader finds this kind of argument [my gene is important -\u0026gt; gene + cancer + references] should take it with a grain of salt.\nAn interesting point is the reasons behind these numbers. From a biological perspective, it is difficult to assume the relevance in cancer of most of the human genome even though cancer is a group including many different kinds of diseases with their subgroups.\nIn the following points, I describe some of the reasons that might explain these numbers:\n In most cases, a tumor is produced by the disruption of multiple genes simultaneously. Structural variants or chromosomal aberrations can map a considerable portion of the genome and disrupt many genes. This scenario makes it difficult to identify the driver mutations from those mutations (passengers) with no relevance and, therefore, finding the causal gene(s). Researchers have an important incentive: publish. Therefore, it makes sense that some researchers “orientate” their studies and results to the cancer field because it is a way to give more “weight” to their research even though, in some cases, the evidence for it is scarce.\n The overproduction of scientific studies in specific areas of knowledge has already been described. For instance, in a comment published in 2021, the authors find that only 22% of gene-related publications were related to 1% of genes. Also, they find “new yearly publications focusing on a given gene is linearly proportional to the size of previous literature on it”.\n  It is reasonable to think that a similar scenario happens with many research published trying to link their analysis with any aspect of cancer though the evidence is limit.\nTo clarify, this is by no means a way to discredit researchers with work related to cancer. It is a way to make people aware of the problematic aspect of finding articles in PubMed describing the gene A associated with cancer and using them as evidence without further analysis.\nSome ideas for a future version\n There have been multiple naming conventions to identify genes. A paper describing a gene with a synonymous symbol instead of the official gene will not be reported in our script. Luckily, HUGO reports the list of synonymous symbols along with the official one. Therefore, it would be easy to adapt our script with new queries and merge the number of hits.\n Some gene symbols can be confounded with composited names. For instance, the query “A1BG[Title/Abstract] AND cancer[Title/Abstract]” retrieved 22 hits, and one of them was describing the A1BG-AS1 lncRNA. An interesting idea would be to reanalyze the data but change cancer by each of the cancer types.\n  Notes\n We can run the queries in parallel, adding the tag “future_” to the function “map_dfr” thanks to the package furrr. Note the API has a limit of simultaneous queries per IP.\n I set a limit of 600 articles retrieved to avoid the exceeded limit error of the API.\n  ","date":1621123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621181538,"objectID":"9e62e47f1f88873ca485de4c9599f49b","permalink":"https://franciscorequena.com/post/genes-associated-cancer_pubmed/","publishdate":"2021-05-16T00:00:00Z","relpermalink":"/post/genes-associated-cancer_pubmed/","section":"post","summary":"In the biomedical literature, it is common to find sentences like:\n“Besides, the gene [gene symbol] has been associated with [type of cancer(s)] [References]”\nThe structure of these sentences can change from article to article, but the underlying idea and goal are the same.","tags":[],"title":"How many genes have been associated with cancer in PubMed?","type":"post"},{"authors":[],"categories":[],"content":"  The Genomics England PanelApp provides panels of genes related to human disorders manually curated by healthcare experts. From a clinical and research perspective, this is a remarkable resource. At the time of writing this post, over 320 panels have been published.\nUnfortunately, you can only download the panels manually one at a time or through an API that retrieves the information as a JSON file.\nAlternatively, below you can find a script in R to extract all the panels from the website and merge them into a single dataset. Please note the following points before using the script:\n I only consider genes labeled as “Expert Review Green” defined as “gene-disease association with a high level of evidence” and exclude STRs and CNVs entities. More information on the criteria used can be found on the main page (heading: Understanding gene classifications in a version 1+ gene panel. The script selects only a subset of columns from the total available. Be careful, the script will download more than 320 files automatically (on my laptop, the execution process is ~7 min). The script is ready to run on the Linux system.  As the script is based on the current website structure, any changes could break the code. Please let me know if this happens. I will try to code an updated version of the code.\nlibrary(rvest) library(purrr) library(tidyverse) website \u0026lt;- \u0026quot;https://panelapp.genomicsengland.co.uk/panels/\u0026quot; page \u0026lt;- read_html(website) c_ref \u0026lt;- page %\u0026gt;% html_nodes(\u0026quot;a\u0026quot;) %\u0026gt;% # find all links html_attr(\u0026quot;href\u0026quot;) df_ref \u0026lt;- tibble(ref = c_ref, id = NA) %\u0026gt;% filter(str_detect(ref, \u0026#39;download\u0026#39;)) %\u0026gt;% mutate(ref = str_remove(ref, \u0026#39;/panels/\u0026#39;)) %\u0026gt;% mutate(id = ref) %\u0026gt;% mutate(id = str_remove(id, \u0026#39;/download/01234/\u0026#39;)) # Linux command - if you are using Windows, please make sure that you create a new folder with the name \u0026#39;gene_panel\u0026#39; # and remove the \u0026quot;system(\u0026#39;mkdir gene_panel\u0026#39;)\u0026quot; line system(\u0026#39;mkdir gene_panel\u0026#39;) setwd(\u0026#39;gene_panel\u0026#39;) walk2(df_ref$ref, df_ref$id, function(a, b) download.file(url = paste0(website, a), destfile = paste0(\u0026#39;gene_panel_\u0026#39;, b)) ) files_panel \u0026lt;- list.files() panel_total \u0026lt;- files_panel %\u0026gt;% map_dfr(~ read_tsv(.x) %\u0026gt;% select(`Entity Name`, `Entity type`, `Gene Symbol`, `Sources(; separated)`, Level4, Phenotypes) %\u0026gt;% mutate(source = .x) ) # Filtering out genes with a evidence level (red - amber) panel_total \u0026lt;- panel_total %\u0026gt;% rename(entity_name = `Entity Name`, entity_type = `Entity type`, gene = `Gene Symbol`, sources = `Sources(; separated)`) %\u0026gt;% filter(entity_type == \u0026#39;gene\u0026#39;) %\u0026gt;% # optional - we can include regions in our analysis filter(str_detect(sources, \u0026#39;Expert Review Green\u0026#39;)) %\u0026gt;% select(gene, Level4, -sources, source, Phenotypes) setwd(\u0026#39;..\u0026#39;) write_tsv(panel_total, \u0026#39;panel_genes.tsv\u0026#39;) ","date":1616198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616255070,"objectID":"4b1261d25afb535172a0c1be504a5350","permalink":"https://franciscorequena.com/post/2021-03-20-extracting-gene-panels-from-the-genomics-england-panelapp/","publishdate":"2021-03-20T00:00:00Z","relpermalink":"/post/2021-03-20-extracting-gene-panels-from-the-genomics-england-panelapp/","section":"post","summary":"The Genomics England PanelApp provides panels of genes related to human disorders manually curated by healthcare experts. From a clinical and research perspective, this is a remarkable resource. At the time of writing this post, over 320 panels have been published.","tags":[],"title":"Extracting gene panels from the Genomics England Panelapp","type":"post"},{"authors":[],"categories":["machine-learning"],"content":" Overview Receiver operating characteristic (ROC) curves is one of the concepts I have struggled most. As a personal view, I do not find it intuitive or clear at first glance. Possibly, because we are used to interpreting information as single values, such as mean, median, accuracy…ROC curves are different because it represents a group of values conforming a curve. Besides, it is the most popular way to represent a model performance for a particular dataset where the task is a binary classification.\nBefore explaining where the ROC curves come from, let’s focus on what is the outcome of most of the classification models. To illustrate this point, let’s train a few logistic regression models with a toy dataset and use the package parsnip which provides a common interface to train models from many other packages.\n Data For this post, we are going to use a dataset that includes 310 patients and six explanatory variables related to biomechanical features of the vertebral column. Besides, it contains a response variable abnormality that defines if the patient has been diagnosed with a medical condition in the vertebral column (yes and no).\nlibrary(tidyverse) library(gganimate) # animated plots library(magick) # combine two gif library(yardstick) # roc_curve helper library(parsnip) # train logistic regression models # Source: https://archive.ics.uci.edu/ml/machine-learning-databases/00212/ verterbral \u0026lt;- read.table(\u0026#39;data/column_2C.dat\u0026#39;, header = FALSE, sep = \u0026#39; \u0026#39;) colnames(verterbral) \u0026lt;- c(\u0026#39;pelvic_incidence\u0026#39;, \u0026#39;pelvic_tilt\u0026#39;, \u0026#39;lumbar_lordosis_angle\u0026#39;, \u0026#39;sacral_slope\u0026#39;, \u0026#39;pelvic_radius\u0026#39;, \u0026#39;degree_spondylolisthesis\u0026#39;, \u0026#39;abnormality\u0026#39;) verterbral \u0026lt;- verterbral %\u0026gt;% select(abnormality, everything()) %\u0026gt;% mutate(id = row_number()) %\u0026gt;% mutate(abnormality = factor(if_else(abnormality == \u0026#39;AB\u0026#39;, \u0026#39;yes\u0026#39;, \u0026#39;no\u0026#39;)))  Split data …and split it in training (70%) and test set (30%).\nset.seed(992) training_ids \u0026lt;- verterbral %\u0026gt;% sample_frac(0.7) %\u0026gt;% pull(id) vert_training \u0026lt;- verterbral %\u0026gt;% filter(id %in% training_ids) vert_test \u0026lt;- verterbral %\u0026gt;% filter(!id %in% training_ids)  Train models logistic_model_one \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% fit(abnormality ~ pelvic_incidence, data = vert_training) logistic_model_two \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% fit(abnormality ~ pelvic_incidence + pelvic_tilt, data = vert_training) logistic_model_three \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope, data = vert_training) logistic_model_four \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius, data = vert_training) logistic_model_five \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% fit(abnormality ~ pelvic_incidence + pelvic_tilt + sacral_slope + pelvic_radius + lumbar_lordosis_angle, data = vert_training) logistic_model_all \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% set_mode(\u0026quot;classification\u0026quot;) %\u0026gt;% fit(abnormality ~ ., data = vert_training[,-ncol(vert_training)]) check_pred \u0026lt;- vert_test %\u0026gt;% select(id) %\u0026gt;% mutate( pred_logistic_one = predict(logistic_model_one, vert_test, type = \u0026#39;prob\u0026#39;)$.pred_yes, pred_logistic_two = predict(logistic_model_two, vert_test, type = \u0026#39;prob\u0026#39;)$.pred_yes, pred_logistic_three = predict(logistic_model_three, vert_test, type = \u0026#39;prob\u0026#39;)$.pred_yes, pred_logistic_four = predict(logistic_model_four, vert_test, type = \u0026#39;prob\u0026#39;)$.pred_yes, pred_logistic_five = predict(logistic_model_five, vert_test, type = \u0026#39;prob\u0026#39;)$.pred_yes, pred_logistic_all = predict(logistic_model_all, vert_test, type = \u0026#39;prob\u0026#39;)$.pred_yes ) %\u0026gt;% left_join(verterbral %\u0026gt;% select(id, abnormality), by = \u0026#39;id\u0026#39;)  Plot raw outcome check_pred %\u0026gt;% glimpse() ## Observations: 93 ## Variables: 8 ## $ id \u0026lt;int\u0026gt; 1, 3, 6, 9, 10, 11, 12, 13, 20, 22, 27, 43, 44,... ## $ pred_logistic_one \u0026lt;dbl\u0026gt; 0.7499979, 0.8028267, 0.4747471, 0.5213208, 0.4... ## $ pred_logistic_two \u0026lt;dbl\u0026gt; 0.7940143, 0.8233949, 0.5251698, 0.5516503, 0.3... ## $ pred_logistic_three \u0026lt;dbl\u0026gt; 0.7939271, 0.8233653, 0.5240107, 0.5505372, 0.3... ## $ pred_logistic_four \u0026lt;dbl\u0026gt; 0.9301525, 0.9029361, 0.4239115, 0.5079236, 0.8... ## $ pred_logistic_five \u0026lt;dbl\u0026gt; 0.9035941, 0.8837432, 0.3236233, 0.5683134, 0.9... ## $ pred_logistic_all \u0026lt;dbl\u0026gt; 0.7531384, 0.2486521, 0.4043606, 0.8549908, 0.9... ## $ abnormality \u0026lt;fct\u0026gt; yes, yes, yes, yes, yes, yes, yes, yes, yes, ye... For each observation of the test set, the models retrieve a probability. This value represents how likely that observation belongs to the label abnormality == yes[1].\nProbability is not a particular output format of logistic regressions models [2], but a standard way of many models. For instance, models based on tree decisions, such as gradient boosting [3] or random forest, retrieve probabilities as output.\nTo make it simple, for now, we will use only the predicted values (pred_logistic_all) from the trained model that used all the explanatory variables.\nSince we have all the probabilities values retrieved by the model in the variable pred_logistic_all, we can explore the distribution of the model’s outcome. To do this, there are two common ways: boxplot and density plots. For the scope of this post, we are going to use the latter. Besides, since our observations are defined by two label options (survival == ‘yes’, survival = ‘no’), we are going to plot two different distributions, one for each label:\ncheck_pred %\u0026gt;% ggplot(aes(pred_logistic_all)) + geom_density(aes(fill = abnormality), alpha = 0.4) + theme_bw() + scale_fill_viridis_d() We can extract some ideas from the above plot:\n Since this value represents the probability of an observation to belong to abnormality = 'yes', it makes sense to find observations whose real label is ‘yes’ with high probability. On the other way around, we expect to find observations whose real label is abnormality = 'no' with low probability. Though this is what we expect, this is not always the case, since we find also observations whose probability of belonging to abnormality = 'yes' is quite low, even though, its real label is yes.\n There is a twilight zone, where we have observations from both labels levels that have “inaccurate” probabilities.\n We can somehow see how well a model performed based on the overlapping of these two distributions.\n A perfect model would retrieve both distributions with no overlapping.\n  Since these models do not retrieve directly the label of the response variable. A threshold to discretize a continuous probability is required to transform the probability into a label. This is a difficult part, because no matter where you define the threshold, we face a trade-off between the percentage of False Positives (FP) and False Negatives (FN). Besides, there is not a clear rule for it, and the results can be pretty arbitrary.\nAnother problem arises: if the selection of the threshold is arbitrary, how do we compare different models? Here it is where the ROC curves come out!\nROC curves try to overcome this issue, taking into account all the possible scenarios given multiple thresholds. This allows us to estimate the performance of our model independently of the threshold you take.\n How to create a ROC curve? To create a ROC curve, the starting point is precisely the same information we used to display the density plot: a column with predicted probabilities and another with the real labels. Each row is an observation of the test set.\nOnce we have this information, we define as many thresholds [4] as observations found in the test set (plus Inf and -Inf). These values are defined by the probability of each observation.\nFurthermore, for each threshold value, all the probabilities above it will be identified as abnormality = yes and we count the number of True Positive (TP), True Negative (TN), but also, those observations predicted as abnormality = yes but actually are no (False Positive (FP)) and those predicted as no but actually are yes (False Negative (FN)).\nFinally, we need this information to calculate the values that will make up the ROC curve axis:\n Sensitivity (also known as True positive rate). This metric reflects the number of positives in the test dataset that are correctly identified. Specificity (also known as True negative rate). This metric measures the number of negatives in the test dataset that are correctly identified.  In both cases, a result of 1 is considered perfect.\nTo facilitate this, there are multiple packages in R to calculate the ROC curve. For this case, I am going to use the function roc_curve from the package yardstick which I recommend.\nCheck the output of the function roc_curve:\n# we just need to specify the column with the labels (abnormality) and the predicted probabilities (pred_logistic_all) roc_logistic \u0026lt;- check_pred %\u0026gt;% roc_curve(abnormality, pred_logistic_all) roc_logistic %\u0026gt;% head() ## # A tibble: 6 x 3 ## .threshold specificity sensitivity ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 -Inf 1 0 ## 2 0.0179 1 0.0294 ## 3 0.0292 1 0.0588 ## 4 0.0369 1 0.0882 ## 5 0.0372 1 0.118 ## 6 0.0493 1 0.147 Finally, for the visualization, we only need to modify the specificity variable as 1 - specificity:\nroc_logistic %\u0026gt;% ggplot(aes(x = (1 - specificity), y = sensitivity)) + geom_line() + geom_abline(linetype = 3) + theme_bw()  Animated ROC curve To build some intuition, we can see how to build the ROC curve while we define thresholds values in the density plot:\na \u0026lt;- check_pred %\u0026gt;% ggplot() + geom_density(aes(x = pred_logistic_all, fill = abnormality), alpha = 0.5) + geom_vline(data = roc_logistic %\u0026gt;% filter( .threshold != Inf) %\u0026gt;% filter(.threshold != -Inf), aes(xintercept = .threshold, group = .threshold)) + transition_reveal(.threshold) + theme_bw() b \u0026lt;- roc_logistic %\u0026gt;% ggplot(aes(x = (1 - specificity), y = sensitivity)) + geom_line() + geom_point(colour = \u0026#39;red\u0026#39;, size = 3) + transition_reveal(sensitivity) + geom_abline(linetype = 3) + theme_bw() # Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition a_gif \u0026lt;- animate(a, width = 440, height = 440) b_gif \u0026lt;- animate(b, width = 440, height = 440) a_mgif \u0026lt;- image_read(a_gif) b_mgif \u0026lt;- image_read(b_gif) new_gif \u0026lt;- image_append(c(a_mgif[1], b_mgif[1])) for(i in 2:100){ combined \u0026lt;- image_append(c(a_mgif[i], b_mgif[i])) new_gif \u0026lt;- c(new_gif, combined) } new_gif  Comparing six models At the beginning of this post, we trained five models, each one with a different number of explanatory variables: one, two, three, four, five, and six.\nWe can easily display their probability distribution:\ncomparison_six \u0026lt;- check_pred %\u0026gt;% pivot_longer(starts_with(\u0026#39;pred\u0026#39;), names_to = \u0026#39;model\u0026#39;, values_to = \u0026#39;prob\u0026#39;) %\u0026gt;% mutate(model = fct_inorder(as.factor(model))) comparison_six %\u0026gt;% ggplot(aes(prob)) + geom_density(aes(fill = abnormality), alpha = 0.4) + theme_bw() + scale_fill_viridis_d() + facet_wrap(~ model) In the example above, we observe that the overlapping between both distributions decreases as we increase the number of explanatory variables. In other words, since we increase the amount of useful information to discriminate between the two labels (yes, no), the predictive power of the model improves.\nBesides, we can plot their ROC curves:\ncomparison_six %\u0026gt;% group_by(model) %\u0026gt;% roc_curve(abnormality, prob) %\u0026gt;% ggplot(aes(x = (1 - specificity), y = sensitivity)) + geom_line(aes(color = model)) + geom_abline(linetype = 3) + theme_bw()  Animated comparison Finally, we can replicate the previous code and compare the six models:\nroc_comparison \u0026lt;- comparison_six %\u0026gt;% group_by(model) %\u0026gt;% roc_curve(abnormality, prob) %\u0026gt;% ungroup() a \u0026lt;- comparison_six %\u0026gt;% ggplot(aes(prob)) + geom_density(aes(fill = abnormality), alpha = 0.4) + geom_vline(data = roc_comparison %\u0026gt;% filter(.threshold != Inf) %\u0026gt;% filter(.threshold != -Inf), aes(xintercept = .threshold, group = .threshold)) + theme_bw() + scale_fill_viridis_d() + transition_reveal(.threshold) + facet_wrap(~ model) b \u0026lt;- roc_comparison %\u0026gt;% ggplot(aes(x = (1 - specificity), y = sensitivity, group = model)) + geom_line(aes(color = model)) + geom_point(colour = \u0026#39;red\u0026#39;, size = 3) + transition_reveal(sensitivity) + geom_abline(linetype = 3) + theme_bw() # Code below from https://github.com/thomasp85/gganimate/wiki/Animation-Composition a_gif \u0026lt;- animate(a, width = 440, height = 440) b_gif \u0026lt;- animate(b, width = 440, height = 440) a_mgif \u0026lt;- image_read(a_gif) b_mgif \u0026lt;- image_read(b_gif) new_gif \u0026lt;- image_append(c(a_mgif[1], b_mgif[1])) for(i in 2:100){ combined \u0026lt;- image_append(c(a_mgif[i], b_mgif[i])) new_gif \u0026lt;- c(new_gif, combined) } new_gif  Notes [1] The package yardstick as many other packages use the first level of the response variable factor as the “event”. Therefore, the probability output determines how likely an observation belongs to the first level of the factor of the response variable. This behavior can be changed in the yardstick package global options.\n[2] The default output has a logit scale and needs to be transformed first to a probability value. This can be done automatically if we specify in the function predict the argument type as prob.\n[3] Apart from tree decisions, linear models can be also used in gradient boosting.\n[4] Because of this reason, ROC curves might not be appropriate to evaluate the performance of models on small test sets.\n ","date":1591920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591951861,"objectID":"f37b06061a42f6b9871c20d705d96f2c","permalink":"https://franciscorequena.com/post/roc-curves-an-animated-example/","publishdate":"2020-06-12T00:00:00Z","relpermalink":"/post/roc-curves-an-animated-example/","section":"post","summary":"Overview Receiver operating characteristic (ROC) curves is one of the concepts I have struggled most. As a personal view, I do not find it intuitive or clear at first glance.","tags":[],"title":"An introduction to ROC curves with animated examples","type":"post"},{"authors":[],"categories":["machine-learning"],"content":" Overview In this post, we will get a first approximation to the “uncertainty” concept. First, we will train two models: logistic regression and its “Bayesian version” and compare their performance. Furthermore, we will explore the advantage of using a Bayesian model when we want to estimate how likely is our prediction. Finally, we will briefly discuss why there are some predicted values more probable than others.\nGet the data First, we download this data from Kaggle. This dataset includes 306 patients from a study of patients that had undergone a surgical operation on breast cancer. The table consists of three explanatory variables:\n Age of patient during surgical operation (age) Year when the operation was made (operation_year) Number of positive axillary nodes detected (nodes)  Furthermore, there is a column (survival) that indicates whether the patient survived at least 5 years after the operation.\nlibrary(tidyverse) library(patchwork) # merge plots library(ggridges) # ridges plot library(glue) # paste plot labels library(yardstick) # helper roc curves and auc library(rstanarm) # bayesian model library(bayestestR) # helper for the bayesian model library(broom) # make tidy # Source: https://www.kaggle.com/gilsousa/habermans-survival-data-set haberman \u0026lt;- read_csv(\u0026#39;data/haberman.csv\u0026#39;, col_names = c(\u0026#39;age\u0026#39;, \u0026#39;operation_year\u0026#39;, \u0026#39;nodes\u0026#39;, \u0026#39;survival\u0026#39;)) haberman \u0026lt;- haberman %\u0026gt;% mutate(survival = factor(if_else(survival == 1, \u0026#39;Yes\u0026#39;, \u0026#39;No\u0026#39;))) %\u0026gt;% mutate(operation_year = factor(operation_year)) %\u0026gt;% mutate(id = as.character(row_number())) %\u0026gt;% select(id, everything())  Exploratory analysis Since the dataset has 3 explanatory variables, let’s plot the distribution of each one of them with the response variable survival:\np1 \u0026lt;- haberman %\u0026gt;% ggplot(aes(age)) + geom_density(aes(fill = survival), color = \u0026#39;black\u0026#39;, alpha = 0.4) + theme_bw() p2 \u0026lt;- haberman %\u0026gt;% ggplot(aes(nodes)) + geom_density(aes(fill = survival), color = \u0026#39;black\u0026#39;, alpha = 0.4) + theme_bw() p3 \u0026lt;- haberman %\u0026gt;% group_by(operation_year, survival) %\u0026gt;% summarise(n = n()) %\u0026gt;% mutate(perc = 100*(n / sum(n))) %\u0026gt;% ggplot(aes(operation_year, perc)) + geom_col(aes(fill = survival), color = \u0026#39;black\u0026#39;) + theme_bw() + labs(y = \u0026#39;Percentage (%)\u0026#39;) p1 + p2 + p3 + patchwork::plot_layout(nrow = 3) Age and number of nodes seem to have a reasonable distribution but surprisingly, patient survival does not increase along the operation year. In theory, the patient survival of most cancer types has increased dramatically over the years. Therefore, it seems reasonable to find a similar pattern in this dataset. The interval of time (1958-1969) seems long enough and happened during a period of major progress in clinical therapies.\nA plausible explanation is an underlying effect of, at least, one remaining variable. Let’s observe the distribution of the variable age of patient over the years:\nhaberman %\u0026gt;% ggplot(aes(age, operation_year)) + geom_density_ridges(aes(fill = operation_year), show.legend = FALSE) + theme_bw() haberman %\u0026gt;% ggplot(aes(operation_year, age)) + geom_boxplot(aes(fill = operation_year), show.legend = FALSE) + theme_bw() There seem to be differences over the years. In this post, further analysis to control for this effect is out of scope, but a more exhaustive analysis of this dataset should be aware of it.\n  Classification Once we have explored quickly the dataset, we are going to train a model to try to predict whether the patient survived 5 years after the operation.\nTo do so, we are going to test two different approaches:\n Logistic regression Bayesian generalized linear models  In this blog post, we will skip aspects such as cross-validation, feature engineering, precision-recall curve, or unbalanced labels (there is).\nSplit data First, we will split the available dataset haberman into two sets, a training (70%) and a test (30%).\nset.seed(991) training_ids \u0026lt;- haberman %\u0026gt;% sample_frac(0.7) %\u0026gt;% pull(id) hab_training \u0026lt;- haberman %\u0026gt;% filter(id %in% training_ids) %\u0026gt;% mutate(operation_year = as.integer(operation_year)) hab_test \u0026lt;- haberman %\u0026gt;% filter(!id %in% training_ids) %\u0026gt;% mutate(operation_year = as.integer(operation_year)) We will train independently both models with the training set and predict the labels of the response variable (“survive”, “no survive”) in the test dataset. These predicted labels will be useful to compare both models in terms of performance and further aspects.\n Training - logistic regression In R, we just need to use the glm function and specify the argument family = binomial:\nlogistic_model \u0026lt;- glm(survival ~ age + nodes + operation_year, family = \u0026#39;binomial\u0026#39;, data = hab_training)  Training - Bayesian logistic regression Thanks to the package rstanarm that provides an elegant interface to stan, we can keep almost the same syntax used before. In this case, we use the function stan_glm:\nbayesian_model \u0026lt;- rstanarm::stan_glm(survival ~ age + nodes + operation_year, family = \u0026#39;binomial\u0026#39;, data = hab_training, prior = normal()) ## ## SAMPLING FOR MODEL \u0026#39;bernoulli\u0026#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.137 seconds (Warm-up) ## Chain 1: 0.146 seconds (Sampling) ## Chain 1: 0.283 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL \u0026#39;bernoulli\u0026#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 0 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 2: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 2: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 2: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 2: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 2: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 2: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 2: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 2: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 2: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 2: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 2: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.132 seconds (Warm-up) ## Chain 2: 0.284 seconds (Sampling) ## Chain 2: 0.416 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL \u0026#39;bernoulli\u0026#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 0 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 3: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 3: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 3: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 3: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 3: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 3: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 3: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 3: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 3: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 3: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 3: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.153 seconds (Warm-up) ## Chain 3: 0.157 seconds (Sampling) ## Chain 3: 0.31 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL \u0026#39;bernoulli\u0026#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 0 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 4: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 4: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 4: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 4: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 4: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 4: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 4: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 4: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 4: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 4: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 4: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.144 seconds (Warm-up) ## Chain 4: 0.172 seconds (Sampling) ## Chain 4: 0.316 seconds (Total) ## Chain 4:  Performance Once we trained both models, we are going to compare their performance with the test set (split at the beginning of the post). To that end, we calculate the ROC curve and the Area Under the Curve (AUC) of each model:\npred_logistic \u0026lt;- predict(logistic_model, hab_test, type = \u0026#39;response\u0026#39;) pred_bayesian \u0026lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %\u0026gt;% as_tibble() %\u0026gt;% map_dbl(~ map_estimate(.x) ) check_pred \u0026lt;- hab_test %\u0026gt;% select(id) %\u0026gt;% mutate(pred_surv_no_log = pred_logistic, pred_surv_no_bay = pred_bayesian) %\u0026gt;% left_join(haberman %\u0026gt;% select(id, survival), by = \u0026#39;id\u0026#39;) roc_logistic \u0026lt;- check_pred %\u0026gt;% roc_curve(survival, pred_surv_no_log) %\u0026gt;% mutate(model = \u0026#39;logistic\u0026#39;) roc_bayesian \u0026lt;- check_pred %\u0026gt;% roc_curve(survival, pred_surv_no_bay) %\u0026gt;% mutate(model = \u0026#39;bayesian\u0026#39;) auc_logistic \u0026lt;- check_pred %\u0026gt;% roc_auc(survival, pred_surv_no_log) %\u0026gt;% pull(.estimate) %\u0026gt;% round(3) auc_bayesian \u0026lt;- check_pred %\u0026gt;% roc_auc(survival, pred_surv_no_bay) %\u0026gt;% pull(.estimate) %\u0026gt;% round(3) roc_both \u0026lt;- roc_logistic %\u0026gt;% bind_rows(roc_bayesian) roc_both %\u0026gt;% ggplot(aes((1-specificity), sensitivity)) + geom_line(aes(color = model), size = 1) + theme_bw() + geom_abline(linetype = 3) + labs(title = \u0026#39;Comparison performance logistic and Bayesian model\u0026#39;, subtitle = glue(\u0026#39;AUC (logistic) = {auc_logistic} - AUC (Bayesian) = {auc_bayesian}\u0026#39;)) Both models demonstrate similar performance. If we would have to decide, at this step of the analysis, one of them (logistic or Bayesian), there would not be a reason to choose one or the other. Probably, the logistic one, since it may sounds more familiar. But this might change when the uncertainty idea comes up!\n Uncertainty First, we are going to explore the outcomes of the test set provided by the logistic model. These values represent the probability [2] of each instance of being labeled as “No survive” five years after the operation:\npred_logistic \u0026lt;- predict(logistic_model, hab_test, type = \u0026#39;response\u0026#39;) p1 \u0026lt;- pred_logistic %\u0026gt;% enframe() %\u0026gt;% ggplot(aes(value)) + geom_density(fill = \u0026#39;steelblue\u0026#39;, alpha = 0.5) + theme_bw() + labs(x = \u0026#39;Probability\u0026#39;, y = \u0026#39;Density\u0026#39;) p2 \u0026lt;- pred_logistic %\u0026gt;% enframe() %\u0026gt;% ggplot(aes(value)) + geom_histogram(fill = \u0026#39;yellow\u0026#39;, alpha = 0.5, color = \u0026#39;black\u0026#39;, binwidth = 0.05) + theme_bw() + labs(x = \u0026#39;Probability\u0026#39;, y = \u0026#39;Density\u0026#39;) p1 + p2 + plot_layout(ncol = 2) Each probability value represents a single observation. To convert the predicted probability to labels, the user needs to specify a threshold where every value above the threshold is defined as “No survive”, otherwise “survive”. Most of the cases, this creates problematic scenarios where two observations can be equally labeled in spite of having distinct probabilities (e.g. 0.6 and 0.95).\nBy contrast, for each one of observations in the test set, the Bayesian model does not provide a single probability value but a posterior distribution. We can represent the posterior distributions from the 92 observations (test set) with a boxplot, for instance:\nplot_uncertainty \u0026lt;- posterior_linpred(bayesian_model, newdata = hab_test, transform = TRUE) %\u0026gt;% as_tibble() %\u0026gt;% pivot_longer(everything(), names_to = \u0026#39;rank_obs\u0026#39;, values_to = \u0026#39;pred_surv_n_bay\u0026#39;) plot_uncertainty %\u0026gt;% ggplot(aes(rank_obs, pred_surv_n_bay)) + geom_boxplot() + theme_bw() + labs(x = \u0026#39;Test set - Observation\u0026#39;, y = \u0026#39;Probability (survival == \u0026quot;No survive\u0026quot;)\u0026#39;) Another way to check the dispersion of the predicted outcome is with a ridge plot, that is especially useful when the number of samples is low. So, let’s pick only two observations:\nplot_uncertainty %\u0026gt;% # if you want to reproduce this code, just change the character \u0026#39;1\u0026#39; or \u0026#39;5\u0026#39; for any other. filter(rank_obs %in% c(\u0026#39;1\u0026#39;, \u0026#39;5\u0026#39;)) %\u0026gt;% ggplot(aes(pred_surv_n_bay, rank_obs)) + geom_density_ridges(aes(fill = rank_obs), alpha = 0.6) + theme_bw() + labs(y = \u0026#39;Test set - Observation\u0026#39;, x = \u0026#39;Probability (survival == \u0026quot;No survive\u0026quot;)\u0026#39;) ## Picking joint bandwidth of 0.0101 In the above plot, we observe that both distributions have a “peak” (known as MAP [1]) above 0.5, therefore the predicted label would be, in both cases, ‘no survive’. But, are these two predictions equally certain? Well, we notice, at least, two things:\n Both MAPs have different values (sample 1 - 0.9, sample 5 - 7.2). Observation number 5 has a flatter curve in comparison with the number 1.  In both cases, observation 5 reflects a higher uncertainty regarding its predicted label in comparison with observation 1. Should we make the same clinical decisions in both cases? Would this information be valuable in a clinical environment…? Probably yes, but first, we should find a way to measure it.\n Measuring uncertainty [3] A handy option is to use the standard deviation (sd) of the distribution, so we can estimate one value for each observation. With this in mind, we can plot the distribution of the sd from the 92 observations of the test dataset:\nstd_dev_tbl \u0026lt;- plot_uncertainty %\u0026gt;% group_by(rank_obs) %\u0026gt;% summarise(std_dev = sd(pred_surv_n_bay)) %\u0026gt;% ungroup() std_dev_tbl %\u0026gt;% ggplot(aes(std_dev)) + geom_density(fill = \u0026#39;steelblue\u0026#39;, alpha = 0.5) + theme_bw() + labs(x = \u0026#39;Standard deviation (sd)\u0026#39;, y = \u0026#39;Density\u0026#39;)  Most of observations have a standard deviation of around 0.04. There are a few extreme values in the interval 0.08-0.10. In short, this plot shows that there are some observations whose sd is twice as high as others.\nWe can filter and select observations based on the dispersion of its posterior distribution. For instance, we can split the test set of 92 observations in percentiles using the sd and plot the 1st (lowest sd) and 10th percentile (highest sd). In this way, it allows us to compare those observations with the highest and lowest standard deviation:\ntop_sd \u0026lt;- std_dev_tbl %\u0026gt;% mutate(tile = ntile(std_dev, 10)) %\u0026gt;% filter(tile == 1 | tile == 10) plot_uncertainty %\u0026gt;% # left_join(std_dev, by = \u0026#39;rank_obs\u0026#39;) %\u0026gt;% filter(rank_obs %in% top_sd$rank_obs) %\u0026gt;% ggplot(aes(pred_surv_n_bay, rank_obs)) + geom_density_ridges(aes(fill = rank_obs), alpha = 0.6, show.legend = FALSE) + theme_bw() + labs(y = \u0026#39;Test set - Observation\u0026#39;, x = \u0026#39;Probability (survival == \u0026quot;No survive\u0026quot;)\u0026#39;) In the above plot, we easily identify to which group each observation belongs to. Independently of the predicted labels, should their predictions be considered equally likely? If the final user of the model just receives a categorical outcome, he/she is definitely skipping some valuable information since some predictions look more unlikely than others. As an alternative, predictions could be grouped into categories and neglect those with a high dispersion or make it clear than further support should be required.\nIn this post, we have measured the uncertainty of observations and identifying those samples with high uncertainty. But, we have not talked yet about what is the origin of it.\n Why some predictions are more unlikely than others? In other words, why our model has more doubts about a sample than others? I find two possible explanations:\n The sample is mislabeled. Group variability.  The first one is difficult to address but we can explore the group variability. Since we have three continuous explanatory variable, we can easily do a PCA with the function prcomp:\npca_tbl \u0026lt;- hab_test %\u0026gt;% # take only numeric columns select_if(is.numeric) %\u0026gt;% # Important - we need to scale and center each variable before PCA prcomp(scale = TRUE, center = TRUE) %\u0026gt;% tidy() %\u0026gt;% mutate(row = as.character(row)) %\u0026gt;% pivot_wider(id_cols = row, values_from = value, names_from = PC, names_prefix = \u0026#39;PC\u0026#39;) %\u0026gt;% left_join(top_sd %\u0026gt;% select(-std_dev), by = c(\u0026#39;row\u0026#39; = \u0026#39;rank_obs\u0026#39;)) %\u0026gt;% mutate(tile = ifelse(is.na(tile), \u0026#39;ok\u0026#39;, tile)) %\u0026gt;% left_join(hab_test %\u0026gt;% select(survival) %\u0026gt;% mutate(row = as.character(row_number())), by = \u0026#39;row\u0026#39;) pca_tbl %\u0026gt;% ggplot(aes(PC1, PC2)) + geom_point() + theme_bw() As we have observations in two categories, let’s split them into two plots:\nSince we have observations defined into two categories (survival = Yes, survival = No), let’s lay out the plot into two different:\npca_tbl %\u0026gt;% ggplot(aes(PC1, PC2)) + geom_point() + theme_bw() + facet_grid(~ survival) Furthermore, we are going to highlight those observations that belong to the highest and lowest sd groups:\npca_tbl %\u0026gt;% mutate(tile = case_when( tile == \u0026#39;1\u0026#39; ~ \u0026#39;lowest sd\u0026#39;, tile == \u0026#39;10\u0026#39; ~ \u0026#39;highest sd\u0026#39;, tile == \u0026#39;ok\u0026#39; ~ \u0026#39;ok\u0026#39; )) %\u0026gt;% ggplot(aes(PC1, PC2)) + geom_point(aes(fill = tile), color = \u0026#39;black\u0026#39;, shape = 21, size = 2) + theme_bw() + facet_grid(~ survival) + labs(fill = \u0026#39;Category\u0026#39;) On the one hand, “lowest sd” group observations are centrally located in the plot. This reflects a tendency of these samples to have similar features values with observations belonging to their own label. On the other hand, “highest sd” group points tend to be dispersed from the rest, all over the components. It makes sense since the uncertainty to predict these points come from the fact that their own feature values are different from points on the same category.\nSurprisingly, there is a red point on the left panel whose location is centric respect to the rest of the values. This perhaps arises the disadvantage of reducing a probability distribution to a point-estimate (standard deviation). The dispersion estimation might have not be accurate enough and further ways of measuring might be needed.\n Conclusion As humans, we make decisions based on uncertainty, even though we are not aware of it. If the weather forecasters show 10% of raining on the weekend, we will probably make a plan to go to the mountain. With 90% we may rethink about it…When we are talking with someone about a delicate topic, we pick the words based on the uncertainty of his/her predicted response: words with a broad meaning and therefore ambiguous might not be chosen, due to the high uncertainty. Therefore, if we constantly map our reality and act through the constant evaluation of uncertainty, why should we believe in predictions from machines without a shadow of doubt?\nMy personal view is that the measurement of uncertainty will end up being essential. Especially, for every decision process supported by a machine in a clinical environment. In that way, I find Bayesian models a nice fit for many of the challenges of tomorrow.\n Notes [1] To calculate the roc curves of the Bayesian model’s predictions, a single probability value for each observation is required. There are multiple ways to estimate it, such as mean, median, and MAP (Highest Maximum A Posteriori). In this case, I chose the latest because it provided the highest performance.\n[2] The function predict retrieves outcomes as probabilities because we specified type = response as argument. Otherwise, the default output would be as logit.\n[3] I am interested to know more ways to estimate the “uncertainty” of a prediction. Please if you have any reference or idea, let me know! ;P\n  ","date":1590710400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590751665,"objectID":"a1fc8fcb88c1e5db24a84be8e8a013be","permalink":"https://franciscorequena.com/post/introduction-uncertainty-bayesian-models/","publishdate":"2020-05-29T00:00:00Z","relpermalink":"/post/introduction-uncertainty-bayesian-models/","section":"post","summary":"Overview In this post, we will get a first approximation to the “uncertainty” concept. First, we will train two models: logistic regression and its “Bayesian version” and compare their performance.","tags":[],"title":"An introduction to uncertainty with Bayesian models","type":"post"},{"authors":[],"categories":[],"content":" Overview In this post, I will discuss briefly what is the Poisson distribution and describe two examples extracted from research articles in the genomics field. One of them based on the distribution of structural variants across the genome and other about de novo variants in a patient cohort.\n Poisson distribution In genomics, many of the events we observe correspond to countable values. For instance, the number of mutations found in a specific type of genomic regions or a patient cohort, sequence reads…The Poisson distribution is a discrete probability model that takes countable numbers as the mentioned before and will be defined as events.\nTo calculate a Poisson distribution, we need to specify a single parameter which is called lambda (\\[\\lambda\\]). This value is known as the rate parameter and defines the mean number of events in a given interval. In other words, if we know the total number of events of our system, we just need to divide it by the number of intervals. We will see further some examples of this.\nOnce we know lambda, we can calculate the probability of seeing \\[/x\\] number of events on a given interval, following this formula:\n\\[P\\left( x \\right) = \\frac{{e^{ - \\lambda } \\lambda ^x }}{{x!}}\\]\nIn the next example, we are going to generate a Poisson distribution of 100 samples whose lambda value is equal to 2 with the rpois function:\n# Load libraries library(dplyr) library(ggplot2) library(gganimate) library(tidyr) tibble(x = rpois(n = 100, lambda = 2)) %\u0026gt;% ggplot(aes(x)) + geom_histogram(binwidth = 1, fill = \u0026#39;steelblue\u0026#39;, color = \u0026#39;black\u0026#39;) + theme_bw() + labs(title = \u0026#39;Poisson distribution\u0026#39;, x = \u0026#39;Events\u0026#39;, y = \u0026#39;Count\u0026#39;) An interesting aspect about the Poisson distribution: the mean and variance of the distribution are equal to the value of lambda. Therefore, the probability of finding an interval with 3 events (for instance) is higher as long as we increase the value of lambda.\nConfusing? Check out this example:\npoisson_tbl \u0026lt;- tibble(lambda = seq(1.2, 7, 0.2)) %\u0026gt;% rowwise() %\u0026gt;% mutate(value = paste(rpois(1000, lambda), collapse = \u0026#39;,\u0026#39;)) %\u0026gt;% separate_rows(value, sep = \u0026#39;,\u0026#39;) %\u0026gt;% mutate(value = as.integer(value)) %\u0026gt;% mutate(prob_7 = round(dpois(7, lambda), 5)) poisson_tbl %\u0026gt;% ggplot(aes(value)) + geom_histogram(binwidth = 1, fill = \u0026#39;steelblue\u0026#39;, color = \u0026#39;black\u0026#39;) + transition_states(lambda) + geom_vline(xintercept = 7, color = \u0026#39;red\u0026#39;, linetype = 4 ) + labs(title = \u0026#39;Poisson distribution\u0026#39;, subtitle = \u0026#39;lambda: {closest_state}, P(X = 7) : {poisson_tbl[poisson_tbl$lambda == {closest_state},] %\u0026gt;% pull(prob_7) %\u0026gt;% unique()}\u0026#39;, x = \u0026#39;Events\u0026#39;, y = \u0026#39;Count\u0026#39;) + theme_bw() In the example above, we are generating 30 times a set of 1000 random values following a Poisson distribution, increasing each time the value of lambda (from 1.2 to 7). The probability of finding an interval with 7 events (red line) is higher as long as we increase lambda.\nAs we said before, we only need lambda to generate a Poisson distribution. Generally, we calculate this value if we know beforehand the number of events and intervals (as we will see in the second example, this is not always the case).\nLet’s put some examples of what an interval or event can be:\n Intervals. We can define intervals as fixed time units, such as days, months, years…or also delimited areas of a geographical region (see this nice post of cancer clusters or this one about the distribution of impacts of V-1 and V-2 missiles during WWII).\n Events. The amount of clicks on a banner or the number of homicides or blackouts every year…An important condition is that each event is independent of each other (events occur independently).\n  Every time, we perform a Poisson distribution, we always ask ourselves the same question: are these events distributed randomly across the intervals?\n Poisson distribution in genomics In genomics, as many other fields, there are different ways to define intervals and events. In the next examples, we will explore two completely different approaches:\nExample 1 : Structural variants in the human genome Structural Variants (SVs) are mutations of more than 50bp and include deletions, duplications, inversion, translocations…These types of variants are important causes of multiple disorders, such as autism, schizoprenia, autoimmune diseases or developmental disorders.\nIn this article Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots [1], the authors explore whether the distribution of this type of mutations is random or follow any pattern across the genome.\nTo do this, the researchers defined each structural variant as an event. Next, they divided the human genome into 100 kb intervals and after discarding incomplete intervals (the intervals need to be fixed), they got a total of 28,103 intervals.\nThe number of structural variants is 42,758 SVs. Therefore, to calculate lambda, they just had to divide this number by the total number of intervals. Finally, they generated a Poisson distribution and defined as “hotspot regions” all the intervals that exceeded the 99th percentile (6 SVs per 100 kb interval) concluding that these intervals had more SVs than expected by chance. Furthermore, they were able to identify “desert regions” as those intervals with a lower nº of SVs as compared with the number expected by chance.\n Example 2 : De novo variants in neurodevelopmental disorders In this article De novo mutations in regulatory elements in neurodevelopmental disorders [2], the researchers explore the impact of de novo variants (those present on children but not their parents) on regulatory regions of the genome in a cohort of patients with neurodevelopmental disorders. The majority of patients in this cohort did not present any de novo mutations (DNMs) in protein-coding genes. Therefore, a plausible hypothesis is to find some of these DNMs in those regions of the DNA yet unexplored: regulatory regions.\nMost of the human genome (98%) do not encode for protein regions. Therefore, the researchers decided to narrow down the search and focus only on those regulatory regions based on two features: regions highly conserved or experimentally validated.\nFinally, they found DNMs mapping this set of regulatory regions, which is great since it allows us to identify the causal mutation and find a diagnos….but wait a minute: Each person’s genome harbors many variants and most of the time, these variants are not harmful. So, we expect to find variants randomly in these regulatory regions just by chance. Yes, as you can guess…here it comes the Poisson distribution.\nThe researchers knew this fact, therefore, to validate their results, they performed (surprise…) a Poisson distribution. First, they calculated the lambda parameter following the next approach:\nThey focused on 6,239 individuals and counted the number of mutations found in regulatory regions. Furthermore, for each region, they calculated the expected number of mutations given the nucleotide context.\nOnce they got the expected number of mutations for each regulatory region, they summed the values and multiplied by the total number of individuals (6,239) to obtain lambda. This value represents the expected number of mutations. Finally, they generated a Poisson distribution with lambda and compared it with the number of observed mutations. This allowed them to demonstrate, first, there were some subgroups of regulatory regions with an excess of the novo variants and second, this excess could be considered as statistically significative. These significant regions were mostly featured by fetal brain DNase signal.\nContrary to what we saw at the beginning of the post, the approach to calculate lambda has been completely different in the second example. Precisely, this versatility makes the Poisson distribution one of the most popular ways to model counted data.\n  Notes  We discussed here about two different scenarios whose events were defined as mutations. But the Poisson distribution can help us to modelate other kind of events, for instance, sequence data. One of the most used techniques for the identification of peaks in Chip-seq analysis is called Model-based Analysis of ChIP-Seq data ( MACS). This program generates a Poisson distribution to identify regions with a higher number of reads than just by chance.\n When we use the genome to produce fixed size intervals to generate a Poisson distribution, an important aspect, it is the genome size. In principle, we already know this value: ~3,100 milions b.p (hg19) and ~ 3,200 millions b.p (hg38). Unfortunately, there are many inaccesible regions (gap regions) represented by Ns. Therefore, the use of the total size would artificially decrease the value of lambda and increase the number of false findings. As a consequence, we need to provide a effective genome size.\n   References [1] Lin, Yen-Lung, and Omer Gokcumen. “Fine-scale characterization of genomic structural variation in the human genome reveals adaptive and biomedically relevant hotspots” Genome biology and evolution 11.4 (2019): 1136-1151.\n[2] Short, Patrick J., et al. “De novo mutations in regulatory elements in neurodevelopmental disorders.” Nature 555.7698 (2018): 611-616.\n ","date":1589414400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589472209,"objectID":"c3404987b092571a22cbdc3300e1fe13","permalink":"https://franciscorequena.com/post/poisson-distribution-applied-in-genomics/","publishdate":"2020-05-14T00:00:00Z","relpermalink":"/post/poisson-distribution-applied-in-genomics/","section":"post","summary":"Overview In this post, I will discuss briefly what is the Poisson distribution and describe two examples extracted from research articles in the genomics field. One of them based on the distribution of structural variants across the genome and other about de novo variants in a patient cohort.","tags":["statistics"],"title":"Poisson distribution applied in genomics","type":"post"},{"authors":[],"categories":[],"content":" # Load of libraries library(tidyverse) library(sp) library(gganimate) n_simulations \u0026lt;- 3000 df \u0026lt;- tibble( values_x = runif(n_simulations,0,1), values_y = runif(n_simulations,0,1) ) circleFun \u0026lt;- function(center=c(0,0), diameter=1, npoints=100, start=0, end=2) { tt \u0026lt;- seq(start*pi, end*pi, length.out=npoints) data.frame(x = center[1] + diameter / 2 * cos(tt), y = center[2] + diameter / 2 * sin(tt)) } dat \u0026lt;- circleFun(c(0,0), 2, start=1.5, end=2.5) df \u0026lt;- df %\u0026gt;% rowwise() %\u0026gt;% mutate(label = point.in.polygon(values_x, values_y, dat$x, dat$y, mode.checked=FALSE)) %\u0026gt;% ungroup() %\u0026gt;% mutate(count_in = cumsum(label), id = row_number(), pi_value = 4*(count_in / id)) ggplot(df) + geom_rect(aes(xmin = -1, xmax = 1, ymin = -1, ymax = 1), colour = \u0026quot;black\u0026quot;, show.legend = FALSE) + geom_polygon(aes(x, y), data = dat, alpha = 0.4) + geom_point(aes(x = values_x, y = values_y, fill = factor(label), group=id), color = \u0026#39;black\u0026#39;, shape = 21, show.legend = FALSE) + theme_minimal() + coord_cartesian(ylim=c(0, 1), xlim = c(0,1)) + transition_reveal(id) + labs(title = \u0026#39;Nº of observations: {frame_along} of {n_simulations}\u0026#39;, subtitle = \u0026#39;Estimated value of pi: {df$pi_value[as.integer(frame_along)]}\u0026#39;) ggplot(df, aes(id, pi_value)) + geom_line() + geom_point(colour = \u0026#39;red\u0026#39;, size = 3) + transition_reveal(id) + geom_hline(yintercept = pi, color = \u0026#39;red\u0026#39;, linetype= \u0026#39;dashed\u0026#39;) + theme_bw() + labs(title = \u0026#39;Nº of observations: {frame_along} of {n_simulations}\u0026#39;, subtitle = \u0026#39;Estimated value of pi: {df$pi_value[as.integer(frame_along)]}\u0026#39;, x = \u0026#39;Sample size\u0026#39;, y = \u0026#39;Pi value\u0026#39;) + coord_cartesian(ylim=c(0, 4))  ","date":1588464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588526769,"objectID":"75a9a0a56d775842d13ad3c4c99527ac","permalink":"https://franciscorequena.com/post/estimating-pi-value-with-monte-carlo-simulation/","publishdate":"2020-05-03T00:00:00Z","relpermalink":"/post/estimating-pi-value-with-monte-carlo-simulation/","section":"post","summary":"# Load of libraries library(tidyverse) library(sp) library(gganimate) n_simulations \u0026lt;- 3000 df \u0026lt;- tibble( values_x = runif(n_simulations,0,1), values_y = runif(n_simulations,0,1) ) circleFun \u0026lt;- function(center=c(0,0), diameter=1, npoints=100, start=0, end=2) { tt \u0026lt;- seq(start*pi, end*pi, length.","tags":[],"title":"Estimating pi value with Monte Carlo simulation","type":"post"},{"authors":null,"categories":["R","open data","networks"],"content":" Introduction Recently, I started to read this free accessible book written by Albert-László Barabási. In the Chapter 4 of his book, it depicts the USA airport networks to represent scale-free networks. I was wondering if we can get a world picture, creating the same network but including the global routes using open data from internet.\n 1. What is a scale-free network? Scale-free networks are characterized by a large number of nodes with low degree (number of links) and very few hubs with a high degree. If we represent the distribution of degrees of these nodes, it follows a power-law distribution. To illustrate this idea, let’s create a quick example:\n# Load libraries library(tidygraph) library(ggraph) library(igraph) library(stringr) library(tidyverse) library(patchwork) library(ggthemes) # 1. Example showing a scale-free network scale_free_net \u0026lt;- play_barabasi_albert(n = 1000, power = 1) # 1.1 Scale-free network p1 \u0026lt;- ggraph(scale_free_net, layout = \u0026#39;kk\u0026#39;) + geom_edge_link(alpha = 0.3) + geom_node_point(fill = \u0026#39;steelblue\u0026#39;, color = \u0026#39;black\u0026#39;, shape = 21) + ggtitle(\u0026#39;Scale-free network\u0026#39;) + theme_graph() vector_values \u0026lt;- degree_distribution(scale_free_net)[-1] # Eliminate first element, it represents zero degree vertices df \u0026lt;- data.frame(frequency = vector_values, degrees = seq(1, length(vector_values),1)) # 1.2 Degree distribution p2 \u0026lt;- ggplot(df, aes(degrees, frequency)) + geom_col(fill = \u0026#39;steelblue\u0026#39;, color = \u0026#39;black\u0026#39;) + ggtitle(\u0026#39;Degree Distribution of a scale-free network\u0026#39;) + ylab(\u0026#39;Relative frequency\u0026#39;) + xlab(\u0026#39;Number of links\u0026#39;) + theme_bw() p1 + p2 Many real networks share this feature. For instance, if we take a look how internet is organized and calculate the number of links that every site has, we find that the most of websites (nodes) have a low number of links (edges) and very few will have a large number of links (e.g. Google, Facebook…). Other examples are social, co-authorship or protein-protein network.\nWe hope to see the same pattern through our airport’s network: very few airports have a large number of routes while the most will have few routes.\n 2. Data (airports and routes) At the beginning, we talked about creating our own network of airlines routes. To achieve this, we download our data from Openflights whose have a lot of information about flights. We will just download data about airports (selecting: code, longitude and latitude) and routes (selecting: name, code source, code destination and continent location). Besides, we will clean those observations with NA’s values or wrong strings.\nImportant: Aiports will be the nodes of our network and the routes will conform the edges between the nodes.\n# Data with routes # https://openflights.org/data.html#route df \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\u0026#39;,header = FALSE, stringsAsFactors = FALSE, col.names = c(\u0026#39;airline\u0026#39;, \u0026#39;airline_id\u0026#39;, \u0026#39;src\u0026#39;, \u0026#39;src_id\u0026#39;, \u0026#39;dest\u0026#39;, \u0026#39;dest_id\u0026#39;, \u0026#39;codeshare\u0026#39;,\u0026#39;stops\u0026#39;, \u0026#39;equip\u0026#39;))[,c(3,5)] # Data with airport information # https://openflights.org/data.html#airport df2 \u0026lt;- read.csv(\u0026#39;https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\u0026#39;,header = FALSE, stringsAsFactors = FALSE)[,c(2,5,7,8,12)] colnames(df2) \u0026lt;- c(\u0026#39;name\u0026#39;,\u0026#39;code\u0026#39;, \u0026#39;lat\u0026#39;, \u0026#39;long\u0026#39;, \u0026#39;location\u0026#39;) # Clean data df_airport \u0026lt;- df2 %\u0026gt;% filter(!str_detect(code, fixed(\u0026quot;\\\\N\u0026quot;))) %\u0026gt;% filter(!str_detect(location, fixed(\u0026quot;\\\\N\u0026quot;))) %\u0026gt;% as_tibble() tmp_loc \u0026lt;- str_split(df_airport$location, \u0026#39;/\u0026#39;) df_airport$location \u0026lt;- map_chr(tmp_loc, function(x) x[[1]]) df_airport \u0026lt;- df_airport %\u0026gt;% mutate(location = as.factor(location)) df_routes \u0026lt;- df %\u0026gt;% filter(!str_detect(src, fixed(\u0026quot;\\\\N\u0026quot;)) \u0026amp; !str_detect(dest, fixed(\u0026quot;\\\\N\u0026quot;))) %\u0026gt;% filter(!src == dest) %\u0026gt;% group_by(src, dest) %\u0026gt;% count() %\u0026gt;% arrange(desc(n)) %\u0026gt;% ungroup() %\u0026gt;% as_tibble()  3. Airport Network visualization To make possible the downstream analysis, we have to transform the observations of our dataframe into nodes and edges (tbl_graph object). We can do this thanks to the package ggraph. Once we do this, we will be able to visualise the network applying different algorithms layers and calculate topological parameters of the nodes that otherwise would not be possible.\nFor instance, we can choose the ‘mds’ layout (you can find many other layouts described here). This algorithm layout measures the shortest path between each node and display together those nodes which are closer in the network. Besides, we are going to calculate some scores per node and to make faster the algorithm, I will eliminate those airports whose number of routes are low.\n# Convert dataframe (df_routes) to tbl_graph object (df_graph) df_graph \u0026lt;- as_tbl_graph(df_routes,directed = FALSE) %\u0026gt;% activate(edges) %\u0026gt;% filter(!edge_is_multiple()) %\u0026gt;% activate(nodes) %\u0026gt;% mutate(n_degree = centrality_degree(), betweenness = centrality_betweenness(), community = group_walktrap(), n_triangles = local_triangles(), clust = local_transitivity()) %\u0026gt;% left_join(df_airport, by = c(\u0026#39;name\u0026#39; = \u0026#39;code\u0026#39;)) %\u0026gt;% filter(!is.na(lat) \u0026amp; !is.na(long)) # ggraph(df_graph %\u0026gt;% activate(nodes) %\u0026gt;% filter(n_degree \u0026gt;= 10), layout = \u0026quot;mds\u0026quot;) + # geom_edge_link(aes(edge_width = n), alpha = 0.1, edge_colour = \u0026#39;gray\u0026#39;) + # geom_node_point(aes(size = n_degree, fill = location), shape = 21) + # scale_fill_brewer(palette = \u0026#39;Set1\u0026#39;) + # scale_size(range = c(0, 14)) + # theme_graph() + # guides(size=FALSE, edge_width = FALSE, fill = guide_legend(override.aes = list(size = 7))) + # ggtitle(\u0026#39;Airports network\u0026#39;) Besides, we plot the degree distribution of our network using ggplot2. For that, we convert our tbl_graph to a dataframe (the reverse step we did before) applying the function activation(nodes) and then as_tibble().\n# Degree distribution df_nodes \u0026lt;- df_graph %\u0026gt;% activate(nodes) %\u0026gt;% as_tibble() ggplot(df_nodes, aes(n_degree)) + geom_histogram(fill = \u0026#39;steelblue\u0026#39;, color = \u0026#39;black\u0026#39;, binwidth = 1) + ggtitle(\u0026#39;Degree Distribution of airports network\u0026#39;) + ylab(\u0026#39;Frequency\u0026#39;) + xlab(\u0026#39;Number of links\u0026#39;) + theme_bw() As we saw at the beginning, both networks follow a power-law distribution.\n 4. Where is my airport? At first glance, let’s take a look at the distribution of the airports around the world based on their region:\nworldmap \u0026lt;- borders(\u0026quot;world\u0026quot;, colour=\u0026quot;#efede1\u0026quot;, fill=\u0026quot;#efede1\u0026quot;) # Get airports by degree ggplot(df_airport, aes(long, lat)) + worldmap + geom_point(aes(fill = location), color = \u0026#39;black\u0026#39;, shape = 21) + theme_void() + guides(fill = guide_legend(override.aes = list(size = 7))) + ggtitle(\u0026#39; Aiports across the world by region\u0026#39;) We can see the biggest hubs are influenced by the economical situation and the population density of the region.\n What are the best connected airports? p1 \u0026lt;- ggplot(df_nodes %\u0026gt;% filter(n_degree \u0026gt;= 50), aes(long, lat)) + worldmap + geom_point(aes(size = n_degree, fill = n_degree), pch = 21) + scale_fill_viridis_c() + theme_void() + scale_size_continuous(range = c(1,10)) p2 \u0026lt;- ggplot(df_nodes %\u0026gt;% top_n(20, n_degree), aes(reorder(name, -n_degree), n_degree)) + geom_col(aes(fill = n_degree), color = \u0026#39;black\u0026#39;) + scale_fill_viridis() + ggtitle(\u0026#39;Top 20 airport by number of routes\u0026#39;) + ylab(\u0026#39;Nº of routes\u0026#39;) + xlab(\u0026#39;Code Airport\u0026#39;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=20)) p3 \u0026lt;- ggplot(df_nodes %\u0026gt;% group_by(location) %\u0026gt;% top_n(10, n_degree), aes(reorder(name, -n_degree), n_degree)) + geom_col(aes(fill = n_degree), color = \u0026#39;black\u0026#39;) + scale_fill_viridis() + ggtitle(\u0026#39;Top 10 airport by number of routes and region\u0026#39;) + ylab(\u0026#39;Nº of routes\u0026#39;) + xlab(\u0026#39;Code Airport\u0026#39;) + facet_wrap(~ location, scales = \u0026#39;free_x\u0026#39;) + theme_bw() + guides(fill = FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=20)) p1 + p2 + plot_layout(ncol = 1, heights = c(3, 1)) p3 What is the longest path possible? Can you guess how many steps would be required to travel the longest path possible between two airports? This number is called diameter and can be calculated easily:\ndf_graph %\u0026gt;% activate(nodes) %\u0026gt;% mutate(diam = graph_diameter()) %\u0026gt;% distinct(diam) %\u0026gt;% as_tibble() ## # A tibble: 1 x 1 ## diam ## \u0026lt;dbl\u0026gt; ## 1 12 The longest path is 12 steps. Not so long if we take into account the remote distance of some of the airports (Siberia, Greenland, Pacific regions…).\n What is the shortest path between two airports? We can select an airport and calculate the shortest path needed to reach another one. For instance, the Charles de Gaulle Airport (Paris) is one step from Adolfo Suárez Madrid–Barajas (Madrid), but what is the number of steps needed to reach the Hawai’s airport from Paris? Let’s calculate it:\nshortest_paths(df_graph, \u0026#39;CDG\u0026#39;, \u0026#39;HNL\u0026#39;)$vpath[[1]] ## + 3/3209 vertices, named, from 9a73413: ## [1] CDG ORD HNL The shortest path from Paris to Honolulu is: Paris -\u0026gt; Chicago -\u0026gt; Honolulu.\nNow, imagine that we calculate all the shortest paths between Paris and the rest of airports and we repeat it with every airport and calculate the average. This value is called: average shortest path and is average number of minimum connections required from any airport to any other airport.\ndf_graph %\u0026gt;% activate(nodes) %\u0026gt;% mutate(dist = graph_mean_dist()) %\u0026gt;% distinct(dist) %\u0026gt;% as_tibble() ## # A tibble: 1 x 1 ## dist ## \u0026lt;dbl\u0026gt; ## 1 3.97 The average shortesth path is 3.94, almost 4 steps on average to go from an airport to any other.\n What is the longest distance possible from a specific airport? We are in Paris again, and we want to go to the most distant airport possible (in steps). This value is called eccentricity and is specific for each airport. Let’s take a look at three of the most connected airports:\ndf_graph_eccen \u0026lt;- df_graph %\u0026gt;% activate(nodes) %\u0026gt;% mutate(eccentricity = node_eccentricity()) %\u0026gt;% as_tibble() df_graph_eccen %\u0026gt;% filter(name == \u0026#39;ATL\u0026#39; | name == \u0026#39;CDG\u0026#39; | name == \u0026#39;AMS\u0026#39;) %\u0026gt;% select(name.y, eccentricity ) ## # A tibble: 3 x 2 ## name.y eccentricity ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Hartsfield Jackson Atlanta International Airport 7 ## 2 Charles de Gaulle International Airport 7 ## 3 Amsterdam Airport Schiphol 7 We would need 7 steps to go from Paris to the most distant airport, the same value obtained with Atlanta and Amsterdam airports. This make sense as we have selected nodes with the highest nº of routes. But the value 7 is the lowest that we can get?\nLet’s see the distribution:\n# The filter(eccentricity \u0026gt; 2) eliminate those airports that are disconnected from the main network and have a eccentricity from 0 to 2 ggplot(df_graph_eccen %\u0026gt;% filter(eccentricity \u0026gt; 2), aes(eccentricity)) + geom_histogram(fill = \u0026#39;steelblue\u0026#39;, color = \u0026#39;black\u0026#39;) + ylab(\u0026#39;Nº of airports\u0026#39;) + theme(text = element_text(size=20)) As we see above, most of the airports are located between 8 and 9. Those airports with the highest number of routes have a value of 7. But there is an airport whose value is 6.\n df_graph_eccen %\u0026gt;% filter(eccentricity == 6) ## # A tibble: 1 x 11 ## name n_degree betweenness community n_triangles clust name.y lat long ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 YYZ 147 249941. 1 2061 0.192 Leste~ 43.7 -79.6 ## # ... with 2 more variables: location \u0026lt;fct\u0026gt;, eccentricity \u0026lt;dbl\u0026gt; Well, it is interesting that the airport with the lowest eccentricity is Lester B. Pearson International Airport located at Toronto. Its number of routes (n_degree) is not very high but has an important particularity. If we see the map, Canada is a country with a large number of airports sparse along the territory. While the majority of airports have to “spend” steps to reach those distant airport (mainly at the north of the territory), this airport is very close to them and at the same time is close to the rest of airports across the world (USA, Europe, China…)\n Where are the hubs? We can detect also the most relevant hubs (densely connected subgraphs) and display those airports that belongs to one of the top 10 hubs:\nggplot(df_nodes %\u0026gt;% filter(community \u0026lt;= 10), aes(long, lat)) + worldmap + geom_point(aes(fill = as.factor(community)), color = \u0026#39;black\u0026#39;, shape = 21) + theme_void() + scale_fill_brewer(palette = \u0026#39;Paired\u0026#39;) + guides(fill = guide_legend(override.aes = list(size = 12))) + ggtitle(\u0026#39; Aiports across the world by region\u0026#39;) + labs(fill=\u0026quot;List of Hubs\u0026quot;) + theme_map() We have applied a walktrap community finding algorithm that uses random walks between the nodes and group those airports that are connected by short random walks.\nIf you take a look at the map, these hubs represent not only a group of airports densely connected but also political and economical hubs. For instance, a hub includes Ex-soviets states, another Europe, Canary Islands and some cities from Magreb.\nIn addition, we can classify the airports in 3 categories:\n Core: Those aiports whose have the highest number of triangles (subgraph of 3 nodes and 3 edges). If an airport is located in many triangles, we consider it as a well connected airport. Peryphery: Airports that are located in distant regions with few routes. Bridge: Those airports that allow the communication between the airports that form the core and the periphery.  df_nodes \u0026lt;- df_nodes %\u0026gt;% mutate(category = \u0026#39;Bridge\u0026#39;) df_nodes$category \u0026lt;- ifelse(df_nodes$n_triangles \u0026gt; 400, \u0026#39;Core\u0026#39;, df_nodes$category) df_nodes$category \u0026lt;- ifelse(df_nodes$clust == 0, \u0026#39;Periphery\u0026#39;, df_nodes$category) ggplot(df_nodes, aes(long, lat)) + worldmap + geom_point(aes(fill = category), color = \u0026#39;black\u0026#39;, shape = 21) + facet_grid(category ~.) + theme_map() + theme(strip.text = element_text(size=25)) + guides(fill = guide_legend(override.aes = list(size = 20)))   5. Which are the best connected airports? There are different ways to measure the connectivity of a node in a network. One of the most used is the betweenness centrality which is the sum of the shortest paths that pass through a node:\np1 \u0026lt;- ggplot(df_nodes %\u0026gt;% filter(n_degree \u0026gt;= 20), aes(long, lat)) + worldmap + geom_point(aes(size = betweenness, fill = betweenness), pch = 21) + scale_fill_viridis_c() + theme_void() + scale_size_continuous(range = c(1,10)) p2 \u0026lt;- ggplot(df_nodes %\u0026gt;% top_n(20, betweenness), aes(reorder(name, -betweenness), betweenness)) + geom_col(aes(fill = betweenness), color = \u0026#39;black\u0026#39;) + scale_fill_viridis() + ggtitle(\u0026#39;Top 20 airport by number of betweenness\u0026#39;) + ylab(\u0026#39;Frequency\u0026#39;) + xlab(\u0026#39;Code Airport\u0026#39;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=20)) p3 \u0026lt;- ggplot(df_nodes %\u0026gt;% group_by(location) %\u0026gt;% top_n(10, betweenness), aes(reorder(name, -betweenness), betweenness)) + geom_col(aes(fill = betweenness), color = \u0026#39;black\u0026#39;) + scale_fill_viridis() + ggtitle(\u0026#39;Top 10 airport by number of betweenness and region\u0026#39;) + ylab(\u0026#39;Frequency\u0026#39;) + xlab(\u0026#39;Code Airport\u0026#39;) + facet_wrap(~ location, scales = \u0026#39;free_x\u0026#39;) + theme_bw() + guides(fill = FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=20)) p1 + p2 + plot_layout(ncol = 1, heights = c(3, 1)) p3 As we see above, airports with with a high number of routes usually have a high betweenness. But we find an exception: the Ted Stevens Anchorage International Airport (ANL). Honestly, I did not expect this airport with the highest betweenness but if we take a look at the organization of the Alaska’s airports:\ndf_routes_def \u0026lt;- df_routes %\u0026gt;% left_join(df_airport, by = c(\u0026#39;src\u0026#39; = \u0026#39;code\u0026#39;)) %\u0026gt;% rename(long_src = long, lat_src = lat) %\u0026gt;% left_join(df_airport, by = c(\u0026#39;dest\u0026#39; = \u0026#39;code\u0026#39;)) %\u0026gt;% rename(long_dest = long, lat_dest = lat) %\u0026gt;% left_join(df_nodes, by = c(\u0026#39;src\u0026#39; = \u0026#39;name\u0026#39;)) %\u0026gt;% select(-lat, -long) df_routes_anc \u0026lt;- df_routes_def %\u0026gt;% filter( dest == \u0026#39;ANC\u0026#39;) ggplot(df_routes_anc, aes(long_src, lat_src)) + worldmap + coord_map(xlim=c(-180,180)) + geom_segment(aes(x = long_src, y = lat_src, xend = long_dest, yend = lat_dest), alpha = 0.7, color = \u0026#39;steelblue\u0026#39;) + scale_fill_viridis_c() + theme_map() + ggtitle(\u0026#39;Ted Stevens Anchorage International Airport\u0026#39;)  Crossing this airport is required to reach the rest of airports in Alaska. Therefore, this create a bottleneck where most of nodes have to cross this airport before reach the rest.\n7. Routes by number of airlines We can take a look at those routes whose have the largest number of airlines:\nggplot(df_routes %\u0026gt;% top_n(20, n), aes(reorder(paste(src, dest, sep =\u0026#39; - \u0026#39;), -n), n)) + geom_col(aes(fill = n), color = \u0026#39;black\u0026#39;) + scale_fill_viridis() + ggtitle(\u0026#39;Top 20 routes by number of airlines\u0026#39;) + ylab(\u0026#39;Frequency\u0026#39;) + xlab(\u0026#39;Route\u0026#39;) + theme_bw() + guides(fill = FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=20))   8. Connections between Madrid and Dubai We can display all the connections between Madrid and Dubai.\ndf_routes_dubai \u0026lt;- df_routes_def %\u0026gt;% filter( src == \u0026#39;DXB\u0026#39; | dest == \u0026#39;DXB\u0026#39;) p1 \u0026lt;- ggplot(df_routes_dubai, aes(long_src, lat_src)) + worldmap + coord_map(\u0026quot;gilbert\u0026quot;, xlim=c(-180,180)) + geom_segment(aes(x = long_src, y = lat_src, xend = long_dest, yend = lat_dest), alpha = 0.3, color = \u0026#39;steelblue\u0026#39;) + scale_fill_viridis_c() + theme_map() + ggtitle(\u0026#39;Dubai International Airport connections\u0026#39;) p2 \u0026lt;- ggplot(df_routes_dubai %\u0026gt;% filter(src == \u0026#39;DXB\u0026#39;) %\u0026gt;% top_n(10, n) , aes(reorder(paste(src, dest, sep =\u0026#39; - \u0026#39;), -n), n)) + geom_col(aes(fill = n), color = \u0026#39;black\u0026#39;) + scale_fill_viridis() + ggtitle(\u0026#39;Top 10 routes by number of airlines\u0026#39;) + ylab(\u0026#39;Frequency\u0026#39;) + xlab(\u0026#39;Route\u0026#39;) + theme_bw() + guides(fill = FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=20)) p1 FALSE Warning: Removed 10 rows containing missing values (geom_segment). p2 …or the routes between Madrid and the rest of the world:\ndf_routes_madrid \u0026lt;- df_routes_def %\u0026gt;% filter( src == \u0026#39;MAD\u0026#39; | dest == \u0026#39;MAD\u0026#39;) p1 \u0026lt;- ggplot(df_routes_madrid, aes(long_src, lat_src)) + worldmap + coord_map(\u0026quot;gilbert\u0026quot;, xlim=c(-180,180)) + geom_segment(aes(x = long_src, y = lat_src, xend = long_dest, yend = lat_dest), alpha = 0.3, color = \u0026#39;orange\u0026#39;) + scale_fill_viridis_c() + theme_map() + ggtitle(\u0026#39;Madrid Barajas International Airport connections\u0026#39;) p2 \u0026lt;- ggplot(df_routes_madrid %\u0026gt;% filter(src == \u0026#39;MAD\u0026#39;) %\u0026gt;% top_n(10, n) , aes(reorder(paste(src, dest, sep =\u0026#39; - \u0026#39;), -n), n)) + geom_col(aes(fill = n), color = \u0026#39;black\u0026#39;) + scale_fill_viridis() + ggtitle(\u0026#39;Top 10 routes by number of airlines\u0026#39;) + ylab(\u0026#39;Frequency\u0026#39;) + xlab(\u0026#39;Route\u0026#39;) + theme_bw() + guides(fill = FALSE) + theme(axis.text.x = element_text(angle = 90, hjust = 1), text = element_text(size=20)) p1 FALSE Warning: Removed 4 rows containing missing values (geom_segment). p2 Conclusion The relevance of an airport in the network can be assessed through different metrics: nº of routes, nº of triangles, clustering, betweenness, eccentricity or shortest path. At the same time, the identification of groups of airports, we have clustered airports by continent, random walks algorithm, or using a blend of centrality measures filtering the nodes in three groups (core, bridge, peripherial).\nIn conclusion, network science allows us to improve our knowledge about data that can be converted into a network, through the use of multiple approaches.\n Final notes  To simplify this post, I have not included the direction of the edges neither the real distance between airports.\n A very interesting point is the analysis of the resilence: what would happen if we delete a specific airport from the network? Would the impact be equal across the aiports?\n    ","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"58be5bb2af776d957c30c60672ac015b","permalink":"https://franciscorequena.com/post/exploring-world-airline-network/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/post/exploring-world-airline-network/","section":"post","summary":"Introduction Recently, I started to read this free accessible book written by Albert-László Barabási. In the Chapter 4 of his book, it depicts the USA airport networks to represent scale-free networks.","tags":null,"title":"Exploring world flights using a network approach","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://franciscorequena.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["R","machine learning"],"content":" Recently, I discovered a new website about competitions that it is not called Kaggle! Its name is Drivendata.\nDrivenData offers different competitions related with multiple types of field, such as health (oh yes!), ecology, society… with a common element: to face the world’s biggest social challenges.\nI decided to join my first competition called ‘DengAI: Predicting Disease Spread‘. In this case, the user receives a set of weather information (temperatures, precipitations, vegetations) from two cities: San Juan (Puerto Rico) and Iquitos (Peru) with total cases of dengue by year and week of every year.\nThe goal of the competition is to develop a prediction model that would be able to anticipate the cases of dengue in every city depending on a set of climate variables.\nThe DrivenData’s blog wrote some days ago, a post about a fast approach with this dataset. It was written in Python. So, I decided to “translate” to R language.\nThe next code is divided into three main points:\n1. Code with clean tasks (transform NA values, remove of columns…) and exploratory analyses.\n2. Function with every step during cleaning of data.\n3. Development of model, prediction and comparison of predicted vs real total cases detected.\n# Load of libraries library(tidyverse) library(zoo) library(corrplot) library(MASS) library(reshape2) # Load data train_features \u0026lt;- read.csv(\u0026#39;data/dengue_post/dengue_features_train.csv\u0026#39;) train_labels \u0026lt;- read.csv(\u0026#39;data/dengue_post/dengue_labels_train.csv\u0026#39;) test_features \u0026lt;- read.csv(\u0026#39;data/dengue_post/dengue_features_test.csv\u0026#39;) submission_format \u0026lt;- read.csv(\u0026#39;data/dengue_post/submission_format.csv\u0026#39;) # Filter of data by city sj_train_labels \u0026lt;- filter(train_labels, city == \u0026#39;sj\u0026#39;) sj_train_features \u0026lt;- filter(train_features, city == \u0026#39;sj\u0026#39;) iq_train_labels \u0026lt;- filter(train_labels, city == \u0026#39;iq\u0026#39;) iq_train_features \u0026lt;- filter(train_features, city == \u0026#39;iq\u0026#39;) # Is there NA values? df_na_sj \u0026lt;- as.data.frame(apply(sj_train_features,2, function(x) any(is.na(x)))) colnames(df_na_sj) \u0026lt;- \u0026#39;is_there_NA\u0026#39; df_na_sj$number_NA \u0026lt;- apply(sj_train_features,2, function(x) sum(is.na(x))) df_na_sj$mean_NA \u0026lt;- apply(sj_train_features, 2, function(x) mean(is.na(x))) df_na_iq \u0026lt;- as.data.frame(apply(iq_train_features, 2, function(x) any(is.na(x)))) colnames(df_na_iq) \u0026lt;- \u0026#39;is_there_NA\u0026#39; df_na_iq$number_NA \u0026lt;- apply(iq_train_features, 2, function(x) sum(is.na(x))) df_na_iq$mean_NA \u0026lt;- apply(iq_train_features, 2, function(x) mean(is.na(x))) # Vegetation Index over Time Plot with NAs ggplot(sj_train_features, aes(x = as.Date(week_start_date), y = ndvi_ne )) + ggtitle(\u0026#39;Vegetation Index over Time\u0026#39;) + theme_bw() + xlab(\u0026#39;Title\u0026#39;) + geom_line(na.rm = FALSE, color = \u0026#39;blue\u0026#39;) + theme(plot.title = element_text(hjust = 0.5)) # Remove \u0026#39;weekofyear\u0026#39; column sj_train_features \u0026lt;- dplyr::select(sj_train_features, -week_start_date) iq_train_features \u0026lt;- dplyr::select(iq_train_features, -week_start_date) # Fill the NA values with the previous value sj_train_features \u0026lt;- sj_train_features %\u0026gt;% do(na.locf(.)) iq_train_features \u0026lt;- iq_train_features %\u0026gt;% do(na.locf(.)) # Distribution of labels # print(mean(sj_train_labels$total_cases)) # print(var(sj_train_labels$total_cases)) # # print(mean(iq_train_labels$total_cases)) # print(var(iq_train_labels$total_cases)) ggplot(sj_train_labels, aes(x = total_cases)) + theme_bw() + ggtitle(\u0026#39;Cases of dengue in San Juan\u0026#39;) + geom_histogram() + theme(plot.title = element_text(hjust = 0.5)) ggplot(iq_train_labels, aes(x = total_cases)) + theme_bw() + ggtitle(\u0026#39;Cases of dengue in Iquitos\u0026#39;) + geom_histogram() + theme(plot.title = element_text(hjust = 0.5)) # Add total_cases column to *_train_features dataframes # sj_train_features \u0026lt;- left_join(sj_train_features, sj_train_labels, by = c(\u0026#39;city\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;weekofyear\u0026#39;)) sj_train_features$total_cases \u0026lt;- sj_train_labels$total_cases # iq_train_features \u0026lt;- left_join(iq_train_features, iq_train_labels, by = c(\u0026#39;city\u0026#39;, \u0026#39;year\u0026#39;, \u0026#39;weekofyear\u0026#39;)) iq_train_features$total_cases \u0026lt;- iq_train_labels$total_cases # Correlation matrix m_sj_train_features \u0026lt;- data.matrix(sj_train_features) m_sj_train_features \u0026lt;- cor(x = m_sj_train_features[,3:24], use = \u0026#39;complete.obs\u0026#39;, method = \u0026#39;pearson\u0026#39;) m_iq_train_features \u0026lt;- data.matrix(iq_train_features) m_iq_train_features \u0026lt;- cor(x = m_iq_train_features[,3:24], use = \u0026#39;everything\u0026#39;, method = \u0026#39;pearson\u0026#39;) # Correlation Heatmap corrplot(m_sj_train_features, type = \u0026#39;full\u0026#39;, tl.col = \u0026#39;black\u0026#39;, method=\u0026quot;shade\u0026quot;) corrplot(m_iq_train_features, type = \u0026#39;full\u0026#39;, tl.col = \u0026#39;black\u0026#39;, method = \u0026#39;shade\u0026#39;) # Correlation Bar plot df_sj_train_features \u0026lt;- data.frame(m_sj_train_features)[2:21,] df_sj_train_features \u0026lt;- dplyr::select(df_sj_train_features, total_cases) df_iq_train_features \u0026lt;- data.frame(m_iq_train_features)[2:21,] df_iq_train_features \u0026lt;- dplyr::select(df_iq_train_features, total_cases) ggplot(df_sj_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) + geom_bar(stat = \u0026#39;identity\u0026#39;) + theme_bw() + ggtitle(\u0026#39;Correlation of variables in San Juan\u0026#39;) + ylab(\u0026#39;Correlation\u0026#39;) + xlab(\u0026#39;Variables\u0026#39;) + coord_flip() ggplot(df_iq_train_features, aes(x= reorder(rownames(df_sj_train_features), -total_cases), y = total_cases)) + geom_bar(stat = \u0026#39;identity\u0026#39;) + theme_bw() + ggtitle(\u0026#39;Correlation of variables in Iquitos\u0026#39;) + ylab(\u0026#39;Correlation\u0026#39;) + xlab(\u0026#39;Variables\u0026#39;) + coord_flip() # Function data cleaning data_clean \u0026lt;- function(df_dengue_features, df_dengue_labels = NULL, add_cases = TRUE) { # Filter by city sj_df_dengue_features \u0026lt;- filter(df_dengue_features, city == \u0026#39;sj\u0026#39;) iq_df_dengue_features \u0026lt;- filter(df_dengue_features, city == \u0026#39;iq\u0026#39;) if (add_cases == TRUE) { sj_df_dengue_labels \u0026lt;- filter(df_dengue_labels, city == \u0026#39;sj\u0026#39;) iq_df_dengue_labels \u0026lt;- filter(df_dengue_labels, city == \u0026#39;iq\u0026#39;) } # Removing week_start_date column sj_df_dengue_features \u0026lt;- dplyr::select(sj_df_dengue_features, -week_start_date) iq_df_dengue_features \u0026lt;- dplyr::select(iq_df_dengue_features, -week_start_date) # Fill of NA values with the previous value sj_df_dengue_features \u0026lt;- sj_df_dengue_features %\u0026gt;% do(na.locf(.)) iq_df_dengue_features \u0026lt;- iq_df_dengue_features %\u0026gt;% do(na.locf(.)) # Add total_cases to dataframe with features if (add_cases == TRUE) { sj_df_dengue_features$total_cases \u0026lt;- sj_df_dengue_labels$total_cases iq_df_dengue_features$total_cases \u0026lt;- iq_df_dengue_labels$total_cases } # Converting character columns into numbers sj_df_dengue_features \u0026lt;- as.data.frame(apply(sj_df_dengue_features,2,as.numeric)) sj_df_dengue_features$city \u0026lt;- rep(\u0026#39;sj\u0026#39;, nrow(sj_df_dengue_features)) iq_df_dengue_features \u0026lt;- as.data.frame(apply(iq_df_dengue_features,2,as.numeric)) iq_df_dengue_features$city \u0026lt;- rep(\u0026#39;iq\u0026#39;, nrow(iq_df_dengue_features)) result \u0026lt;- list(sj_df_dengue_features, iq_df_dengue_features ) return(result) } # Getting data_training clean data_train \u0026lt;- data_clean(train_features, train_labels, TRUE) # Getting negative binomials models by city training_sj \u0026lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg + reanalysis_dew_point_temp_k + station_min_temp_c + station_avg_temp_c, data = data_train[[1]]) training_iq \u0026lt;- glm.nb(formula = total_cases ~ reanalysis_specific_humidity_g_per_kg + reanalysis_dew_point_temp_k + station_min_temp_c + station_avg_temp_c, data = data_train[[2]]) # Getting data_test clean data_test \u0026lt;- data_clean(test_features, add_cases = FALSE) # Testing model with training data prediction_train_sj \u0026lt;- predict(training_sj, data_train[[1]], type = \u0026#39;response\u0026#39;) prediction_train_iq \u0026lt;- predict(training_iq, data_train[[2]], type = \u0026#39;response\u0026#39;) df_prediction_train_sj \u0026lt;- data.frame(\u0026#39;prediction\u0026#39; = prediction_train_sj, \u0026#39;actual\u0026#39; = data_train[[1]]$total_cases, \u0026#39;time\u0026#39; = as.Date(train_features$week_start_date[1:936])) df_prediction_train_sj \u0026lt;- melt(df_prediction_train_sj, id.vars = \u0026#39;time\u0026#39;) ggplot(df_prediction_train_sj, aes(x = time, y = value, color = variable)) + geom_line() + ggtitle(\u0026#39;Dengue predicted Cases vs. Actual Cases (City-San Juan) \u0026#39;) df_prediction_train_iq \u0026lt;- data.frame(\u0026#39;prediction\u0026#39; = prediction_train_iq, \u0026#39;actual\u0026#39; = data_train[[2]]$total_cases, \u0026#39;time\u0026#39; = as.Date(train_features$week_start_date[937:1456])) df_prediction_train_iq \u0026lt;- melt(df_prediction_train_iq, id.vars = \u0026#39;time\u0026#39;) ggplot(df_prediction_train_iq, aes(x = time, y = value, color = variable)) + geom_line() + ggtitle(\u0026#39;Dengue predicted Cases vs. Actual Cases (City-Iquitos) \u0026#39;) # Prediction of total_cases in the data set prediction_sj \u0026lt;- predict(training_sj, data_test[[1]], type = \u0026#39;response\u0026#39;) prediction_iq \u0026lt;- predict(training_iq, data_test[[2]], type = \u0026#39;response\u0026#39;) data_prediction_sj \u0026lt;- data.frame(\u0026#39;city\u0026#39; = rep(\u0026#39;sj\u0026#39;, length(prediction_sj) ), \u0026#39;total_cases\u0026#39; = prediction_sj, \u0026#39;weekofyear\u0026#39; = data_test[[1]]$weekofyear, \u0026#39;year\u0026#39; = data_test[[1]]$year ) data_prediction_iq \u0026lt;- data.frame(\u0026#39;city\u0026#39; = rep(\u0026#39;iq\u0026#39;, length(prediction_iq) ), \u0026#39;total_cases\u0026#39; = prediction_iq, \u0026#39;weekofyear\u0026#39; = data_test[[2]]$weekofyear, \u0026#39;year\u0026#39; = data_test[[2]]$year) submission_format$total_cases \u0026lt;- as.numeric(c(data_prediction_sj$total_cases, data_prediction_iq$total_cases)) submission_format$total_cases \u0026lt;- round(submission_format$total_cases, 0) write.csv(submission_format, file = \u0026#39;submission_format_submit.csv\u0026#39;, row.names = F) ","date":1512777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512777600,"objectID":"bc9a72a1bd22065d7459b586981c9f1f","permalink":"https://franciscorequena.com/post/can-we-predict-cases-of-dengue-with-climate-variables/","publishdate":"2017-12-09T00:00:00Z","relpermalink":"/post/can-we-predict-cases-of-dengue-with-climate-variables/","section":"post","summary":"Recently, I discovered a new website about competitions that it is not called Kaggle! Its name is Drivendata.\nDrivenData offers different competitions related with multiple types of field, such as health (oh yes!","tags":[],"title":"Can we predict cases of dengue with climate variables?","type":"post"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1f29febbc615242dc40f5d0944052f46","permalink":"https://franciscorequena.com/project/drugsplot/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/drugsplot/","section":"project","summary":"Web application which analyzes data from the European MonitoringC entre for Drugs and Drug Addiction (EMCDDA) with more than 500 variables throughdata visualization such as interactive boxplots, shapefile maps and automated reports. Developed with R and Shiny.","tags":["Dashboard"],"title":"Drugsplot","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"c79ceba036d1c4f0610e75b12b8d88e3","permalink":"https://franciscorequena.com/project/healthplot/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/healthplot/","section":"project","summary":"Dashboard of 40 individual datasets and more than 50 graphics divided into 13 categories (health, religion, politics, genre, security, ancestry, immigration, demography, economic, logistic, languages and population) that reflect some aspects of the North American public health..","tags":["Dashboard"],"title":"HealthPlot","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d13ed60d515a3141459ea2701b1b057a","permalink":"https://franciscorequena.com/project/rsciencexplorer/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/rsciencexplorer/","section":"project","summary":"This application analyzes more than 12.000 articles and 22.000 tweets obtained through relevant scientific journals (and their twitter accounts). This app was built with R and Shiny.","tags":["Dashboard"],"title":"Rsciencexplorer","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"df68cfcbce1f8727c33153dcb212717b","permalink":"https://franciscorequena.com/project/sciencenet/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/sciencenet/","section":"project","summary":"Query a PMID publication and retrieve information such as cites network, centrality measure by article...Project using API to the NCBI database.","tags":["Dashboard"],"title":"ScienceNet","type":"project"},{"authors":["__Francisco Requena__, Helena G Asenjo et al."],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    #  Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.    Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). --  ","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://franciscorequena.com/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"NOMePlot - analysis of DNA methylation and nucleosome occupancy at the single molecule","type":"publication"}]